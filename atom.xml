<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>拾荒志</title>
  
  <subtitle>虚怀若谷，大智若愚</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://murphypei.github.io/"/>
  <updated>2025-09-16T03:40:54.595Z</updated>
  <id>https://murphypei.github.io/</id>
  
  <author>
    <name>AngryBirds</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>让 LLM 输出规范 JSON 的方法</title>
    <link href="https://murphypei.github.io//blog/2025/09/llm-output-json.html"/>
    <id>https://murphypei.github.io//blog/2025/09/llm-output-json.html</id>
    <published>2025-09-15T06:20:00.000Z</published>
    <updated>2025-09-16T03:40:54.595Z</updated>
    
    <content type="html"><![CDATA[<p>在现代 AI 应用开发中，让大语言模型（LLM）生成结构化的 JSON 数据是一个关键需求。无论是构建 API 服务、数据处理流水线，还是与现有系统集成，结构化输出都是必不可少的。本文将深入探讨多种让 LLM 生成规范 JSON 的方法，从基础技巧到高级工程实践。</p><a id="more"></a><h3 id="为什么需要结构化输出？"><a href="#为什么需要结构化输出？" class="headerlink" title="为什么需要结构化输出？"></a>为什么需要结构化输出？</h3><p>在实际应用中，我们经常需要 LLM 的输出能够被程序直接解析和使用，而不是仅仅作为文本供人类阅读。结构化的 JSON 输出具有以下优势：</p><ul><li><strong>可解析性</strong>：程序可以直接解析和处理 JSON 数据</li><li><strong>类型安全</strong>：明确的字段类型和结构规范</li><li><strong>可验证性</strong>：可以通过 Schema 验证数据完整性</li><li><strong>易集成</strong>：与现有系统和 API 无缝集成</li></ul><h3 id="方法一：提示词-JSON-format"><a href="#方法一：提示词-JSON-format" class="headerlink" title="方法一：提示词 + JSON format"></a>方法一：提示词 + JSON format</h3><p>这是最常用的让模型输出 JSON 格式的方法了。首先就是 prompt 中声明要输出 JSON，配合一些样例 few shot，然后呢，类似 Azure、Gemini 这类的大模型调用接口，都有类似 response_format 可以指定输出 JSON 格式。</p><p>模式如下：<br>JSON 提示词描述 + JSON 样例（Few shot）+ JSON response_format 来约束大模型的 JSON 输出。</p><p>如何用比较好地用提示词描述 JSON 字段呢？网络上比较好的实践是用 Typescript 或者 Yaml 格式描述（LLM生成Json结构化数据的几种方案，个人目前认为最好的方式依然是`TypeScript约束Prompt + Yaml格），当然简单地就直接用列表描述就好。可以参考这篇文章：<a href="https://juejin.cn/post/7325429835387404307" target="_blank" rel="noopener">https://juejin.cn/post/7325429835387404307</a></p><p><strong>需要注意的点</strong>：</p><ul><li><ol><li>这个方法不能百分百保证。笔者在 gemini 2.5 pro，指定了 response_format 为 JSON，prompt 给了 few shot，在大型的 JSON 生成的时候，一样会失败。但是 gemini 2.5 pro 指定 json schema，生成成功率大幅度提高。</li></ol></li><li><ol><li>对于没有 json schema 参数的接口，few show 中的 json 样例非常重要。</li></ol></li></ul><p>再说一下这些 API 提供的 JSON response format，本质是一个 Constrained Decoding，即在预测下一字符时，把不符合 JSON 格式的丢掉，基本在高级模型可以非常稳定地输出 JSON 格式，所以还是会有一些极端 case 导致解码失败，或者解码后不是完整的 JSON 格式。</p><h3 id="方法二：Function-call"><a href="#方法二：Function-call" class="headerlink" title="方法二：Function call"></a>方法二：Function call</h3><p>本质和方法一其实是一样的…</p><h3 id="方法三：后处理"><a href="#方法三：后处理" class="headerlink" title="方法三：后处理"></a>方法三：后处理</h3><p>三个臭皮匠顶个诸葛亮，大模型输出的 JSON 不完美，那就做善后处理。笔者最开始用 GPT-4 生成 JSON 的时候，写了很多后处理的函数，包括：去掉 ```、修复转移错误、轻微语法问题等等。这些案例现在网上有很多，可以自行参考。</p><h3 id="方法四：大模型修复"><a href="#方法四：大模型修复" class="headerlink" title="方法四：大模型修复"></a>方法四：大模型修复</h3><p>在方法三的基础上，让大模型自己纠正输出的 JSON，其实比单独 JSON 解码要简单的多，大模型自己也能做好。但是这个方法的前提是，第一个模型输出的 JSON 内容是基本正确的，如果内容是错误的，后面的模型也很难纠正。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>总结下来就是，一套基本可用的链路就是：prompt + few shot / json schema + response formt + 后处理，这是 API 接口调用常规用法。笔者日常项目体验，目前比较强的模型基本这套链路能处理的生成 JSON 的规模已经很大了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在现代 AI 应用开发中，让大语言模型（LLM）生成结构化的 JSON 数据是一个关键需求。无论是构建 API 服务、数据处理流水线，还是与现有系统集成，结构化输出都是必不可少的。本文将深入探讨多种让 LLM 生成规范 JSON 的方法，从基础技巧到高级工程实践。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="JSON" scheme="https://murphypei.github.io/tags/JSON/"/>
    
      <category term="结构化输出" scheme="https://murphypei.github.io/tags/%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA/"/>
    
      <category term="API" scheme="https://murphypei.github.io/tags/API/"/>
    
      <category term="schema" scheme="https://murphypei.github.io/tags/schema/"/>
    
  </entry>
  
  <entry>
    <title>翻译《如何构建多智能体研究系统：Anthropic 的工程实践》</title>
    <link href="https://murphypei.github.io//blog/2025/08/how-to-build-multi-agents.html"/>
    <id>https://murphypei.github.io//blog/2025/08/how-to-build-multi-agents.html</id>
    <published>2025-08-31T22:17:48.000Z</published>
    <updated>2025-09-01T07:07:33.482Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看打造 Agent 相关的研究，发现 Anthropic 他们的一篇文章写的特别好，有很多工程实践经验值得参考。虽然没有披露更多细节，但是也指出了很多方向。以下基本是原文翻译。</p><a id="more"></a><p>Claude 现在具备了研究功能，可以在网络、Google Workspace 以及任何集成系统中搜索，完成复杂任务。</p><p>这个多智能体系统从原型到生产的历程，让我们学到了关于系统架构、工具设计和提示工程的重要经验。多智能体系统由多个智能体（大语言模型在循环中自主使用工具）协同工作组成。我们的研究功能涉及一个智能体，它根据用户查询规划研究过程，然后使用工具创建并行智能体，同时搜索信息。具有多个智能体的系统在智能体协调、评估和可靠性方面引入了新的挑战。</p><p>本文分解了对我们有效的原则——我们希望您在构建自己的多智能体系统时会发现这些原则有用。</p><h2 id="多智能体系统的好处"><a href="#多智能体系统的好处" class="headerlink" title="多智能体系统的好处"></a>多智能体系统的好处</h2><p>研究工作涉及开放性问题，很难提前预测所需的步骤。你无法为探索复杂主题硬编码固定路径，因为这个过程本质上是动态的和路径依赖的。当人们进行研究时，他们倾向于根据发现持续更新方法，跟进调查过程中出现的线索。</p><p>这种不可预测性使 AI 智能体特别适合研究任务。研究需要灵活性，能够在调查展开时转向或探索切向连接。模型必须自主运行多轮，基于中间发现做出探索方向的决策。线性的一次性流水线无法处理这些任务。</p><p>搜索的本质是压缩：从庞大语料库中提取洞察。子智能体通过在各自的上下文窗口中并行操作来促进压缩，同时探索问题的不同方面，然后为主研究智能体压缩最重要的 token。每个子智能体还提供了关注点分离——不同的工具、提示和探索轨迹——这减少了路径依赖，实现了彻底、独立的调查。</p><p>一旦智能达到阈值，多智能体系统就成为扩展性能的重要方式。例如，虽然个体人类在过去 10 万年中变得更加智能，但在信息时代，人类社会因为我们的<strong>集体智能</strong>和协调能力而变得<strong>指数级</strong>更有能力。即使是通用智能体在单独操作时也面临限制；智能体群体可以完成更多任务。</p><p>我们的内部评估显示，多智能体研究系统在广度优先查询方面表现尤其出色，这些查询涉及同时追求多个独立方向。我们发现，以 Claude Opus 4 为主智能体、Claude Sonnet 4 为子智能体的多智能体系统在我们的内部研究评估中比单智能体 Claude Opus 4 表现好 90.2%。例如，当被要求识别信息技术 S&amp;P 500 中所有公司的董事会成员时，多智能体系统通过将此分解为子智能体任务找到了正确答案，而单智能体系统在缓慢的顺序搜索中失败了。</p><p>多智能体系统主要起作用，因为它们帮助花费足够的 token 来解决问题。在我们的分析中，三个因素解释了 BrowseComp 评估中 95% 的性能方差（该评估测试浏览智能体定位难以找到信息的能力）。我们发现，token 使用本身解释了 80% 的方差，工具调用次数和模型选择是另外两个解释因素。这一发现验证了我们的架构，该架构将工作分布在具有独立上下文窗口的智能体中，为并行推理增加更多容量。最新的 Claude 模型作为 token 使用的大效率倍增器，因为升级到 Claude Sonnet 4 比在 Claude Sonnet 3.7 上将 token 预算翻倍带来更大的性能提升。多智能体架构有效地扩展了超出单智能体限制的任务的 token 使用。</p><p>有一个缺点：在实践中，这些架构快速消耗 token。在我们的数据中，智能体通常使用比聊天交互多约 4 倍的 token，而多智能体系统使用比聊天多约 15 倍的 token。对于经济可行性，多智能体系统需要任务的价值足够高，能够支付增加的性能成本。此外，一些需要所有智能体共享相同上下文或涉及智能体之间许多依赖关系的领域，目前不太适合多智能体系统。例如，大多数编码任务涉及的真正可并行化任务比研究少，而且 LLM 智能体还不擅长实时协调和委托给其他智能体。我们发现多智能体系统在涉及大量并行化、超出单个上下文窗口的信息以及与众多复杂工具接口的有价值任务中表现出色。</p><h2 id="研究功能的架构概述"><a href="#研究功能的架构概述" class="headerlink" title="研究功能的架构概述"></a>研究功能的架构概述</h2><p>我们的研究系统使用编排器-工作者模式的多智能体架构，其中主智能体协调过程，同时委托给并行操作的专门子智能体。</p><p><img src="/images/posts/agent/multi-agent-architecture.webp" alt="多智能体架构"></p><p><em>多智能体架构实际应用：用户查询通过主智能体流动，主智能体创建专门的子智能体来并行搜索不同方面。</em></p><p>当用户提交查询时，主智能体分析查询，制定策略，并生成子智能体来同时探索不同方面。如上图所示，子智能体作为智能过滤器，迭代使用搜索工具收集信息，在本例中是关于 2025 年的 AI 智能体公司，然后向主智能体返回公司列表，以便编制最终答案。</p><p>使用检索增强生成（RAG）的传统方法使用静态检索。也就是说，它们获取与输入查询最相似的一些块集合，并使用这些块生成响应。相比之下，我们的架构使用多步搜索，动态找到相关信息，适应新发现，并分析结果以制定高质量答案。</p><p><img src="/images/posts/agent/multi-agent-process.webp" alt="多智能体处理流程"></p><p><em>流程图显示了我们多智能体研究系统的完整工作流程。当用户提交查询时，系统创建一个 LeadResearcher 智能体，进入迭代研究过程。LeadResearcher 首先思考方法并将其计划保存到内存中以持久化上下文，因为如果上下文窗口超过 200,000 个 token，它将被截断，保留计划很重要。然后它创建具有特定研究任务的专门子智能体（这里显示了两个，但可以是任意数量）。每个子智能体独立执行网络搜索，使用交错思考评估工具结果，并将发现返回给 LeadResearcher。LeadResearcher 综合这些结果并决定是否需要更多研究——如果需要，它可以创建额外的子智能体或完善其策略。一旦收集到足够的信息，系统退出研究循环并将所有发现传递给 CitationAgent，后者处理文档和研究报告以识别引用的特定位置。这确保所有声明都正确归属于其来源。最终的研究结果，连同引用，然后返回给用户。</em></p><h2 id="研究智能体的提示工程和评估"><a href="#研究智能体的提示工程和评估" class="headerlink" title="研究智能体的提示工程和评估"></a>研究智能体的提示工程和评估</h2><p>多智能体系统与单智能体系统有关键差异，包括协调复杂性的快速增长。早期智能体犯了诸如为简单查询生成 50 个子智能体、无休止地搜索不存在的来源、以及过度更新相互干扰等错误。由于每个智能体都由提示引导，提示工程是我们改善这些行为的主要杠杆。以下是我们学到的一些提示智能体的原则：</p><h3 id="1-像智能体一样思考"><a href="#1-像智能体一样思考" class="headerlink" title="1. 像智能体一样思考"></a>1. 像智能体一样思考</h3><p>要迭代提示，你必须理解它们的效果。为了帮助我们做到这一点，我们使用来自系统的确切提示和工具，通过我们的控制台构建了模拟，然后逐步观察智能体工作。这立即揭示了故障模式：智能体在已经有足够结果时继续工作，使用过于冗长的搜索查询，或选择错误的工具。有效的提示依赖于开发智能体的准确心理模型，这可以使最有影响力的变化变得明显。</p><h3 id="2-教编排器如何委托"><a href="#2-教编排器如何委托" class="headerlink" title="2. 教编排器如何委托"></a>2. 教编排器如何委托</h3><p>在我们的系统中，主智能体将查询分解为子任务并向子智能体描述它们。每个子智能体需要一个目标、输出格式、使用工具和来源的指导，以及明确的任务边界。没有详细的任务描述，智能体会重复工作、留下空白或无法找到必要信息。我们开始允许主智能体给出简单、简短的指令，如”研究半导体短缺”，但发现这些指令通常足够模糊，子智能体会误解任务或执行与其他智能体完全相同的搜索。例如，一个子智能体探索 2021 年汽车芯片危机，而另外两个重复工作调查当前 2025 年供应链，没有有效的分工。</p><h3 id="3-根据查询复杂性调整努力"><a href="#3-根据查询复杂性调整努力" class="headerlink" title="3. 根据查询复杂性调整努力"></a>3. 根据查询复杂性调整努力</h3><p>智能体很难判断不同任务的适当努力，所以我们在提示中嵌入了缩放规则。简单的事实查找只需要 1 个智能体进行 3-10 次工具调用，直接比较可能需要 2-4 个子智能体，每个进行 10-15 次调用，复杂研究可能使用超过 10 个子智能体，有明确划分的责任。这些明确的指导方针帮助主智能体有效分配资源，防止在简单查询上过度投资，这是我们早期版本中的常见故障模式。</p><h3 id="4-工具设计和选择至关重要"><a href="#4-工具设计和选择至关重要" class="headerlink" title="4. 工具设计和选择至关重要"></a>4. 工具设计和选择至关重要</h3><p>智能体-工具接口与人机接口一样重要。使用正确的工具是高效的——通常，这是严格必要的。例如，智能体在网络上搜索包含学术术语的内容可能需要 Google Scholar，而不是通用网络搜索。我们设计工具时明确定义了它们的用途、限制和最佳用例。模糊的工具描述导致误用，从而产生无关结果和浪费的努力。我们还发现，为智能体提供太多工具选择可能令人困惑——策划的工具集比大量通用工具更有效。</p><h3 id="5-建立明确的停止条件"><a href="#5-建立明确的停止条件" class="headerlink" title="5. 建立明确的停止条件"></a>5. 建立明确的停止条件</h3><p>智能体可能会陷入无限循环，不断搜索或细化已经足够的结果。我们设置了明确的完成标准：特定数量的来源、满足的关键信息要求，或达到的时间/调用限制。我们还教智能体识别何时找到了”足够好”的答案，而不是完美的答案，这在开放性研究中经常是现实的目标。</p><p>评估多智能体系统需要新方法。传统指标如 BLEU 分数或困惑度无法捕捉复杂的多步骤研究过程的质量。我们开发了测量准确性、完整性、效率和可用性的定制评估。准确性检查智能体是否找到正确信息，完整性评估是否回答了查询的所有部分，效率测量达到结果所需的时间和资源，可用性评估最终输出对用户的有用性。</p><p>我们还发现人工评估仍然至关重要。虽然自动化指标可以大规模捕捉基本质量，但人类判断对于评估细微差别、创造性和整体实用性是不可替代的。我们的提示帮助解决了这个问题。即使在自动化评估的世界中，手动测试仍然必不可少。</p><p>多智能体系统具有涌现行为，这些行为在没有特定编程的情况下出现。例如，对主智能体的小改变可能不可预测地改变子智能体的行为方式。成功需要理解交互模式，而不仅仅是个体智能体行为。因此，这些智能体的最佳提示不仅仅是严格的指令，而是定义分工、问题解决方法和努力预算的协作框架。正确做到这一点依赖于仔细的提示和工具设计、可靠的启发式方法、可观察性和紧密的反馈循环。</p><h2 id="生产可靠性和工程挑战"><a href="#生产可靠性和工程挑战" class="headerlink" title="生产可靠性和工程挑战"></a>生产可靠性和工程挑战</h2><p>在传统软件中，错误可能破坏功能、降低性能或导致中断。在智能体系统中，微小的变化会级联成大的行为变化，这使得为必须在长时间运行过程中维护状态的复杂智能体编写代码变得非常困难。</p><h3 id="智能体有状态且错误会复合"><a href="#智能体有状态且错误会复合" class="headerlink" title="智能体有状态且错误会复合"></a>智能体有状态且错误会复合</h3><p>智能体可以长时间运行，在许多工具调用中维护状态。这意味着我们需要持久执行代码并处理沿途的错误。没有有效的缓解措施，轻微的系统故障对智能体来说可能是灾难性的。当错误发生时，我们不能只是从头开始重新启动：重新启动对用户来说是昂贵且令人沮丧的。相反，我们构建了可以从智能体发生错误时的位置恢复的系统。我们还使用模型的智能来优雅地处理问题：例如，让智能体知道工具何时失败并让它适应，效果出奇地好。我们将建立在 Claude 基础上的 AI 智能体的适应性与重试逻辑和定期检查点等确定性保障措施相结合。</p><h3 id="调试受益于新方法"><a href="#调试受益于新方法" class="headerlink" title="调试受益于新方法"></a>调试受益于新方法</h3><p>智能体做出动态决策，在运行之间是非确定性的，即使使用相同的提示。这使调试变得更困难。例如，用户会报告智能体”没有找到明显信息”，但我们看不出原因。智能体是使用糟糕的搜索查询吗？选择差的来源吗？遇到工具故障吗？添加完整的生产跟踪让我们诊断智能体失败的原因并系统性地修复问题。除了标准可观察性，我们监控智能体决策模式和交互结构——所有这些都不监控个别对话的内容，以维护用户隐私。这种高级可观察性帮助我们诊断根本原因，发现意外行为，并修复常见故障。</p><h3 id="部署需要仔细协调"><a href="#部署需要仔细协调" class="headerlink" title="部署需要仔细协调"></a>部署需要仔细协调</h3><p>智能体系统是几乎连续运行的提示、工具和执行逻辑的高度有状态网络。这意味着每当我们部署更新时，智能体可能处于其过程中的任何位置。因此，我们需要防止我们善意的代码更改破坏现有智能体。我们不能同时将每个智能体更新到新版本。相反，我们使用彩虹部署来避免干扰运行中的智能体，通过逐渐将流量从旧版本转移到新版本，同时保持两者同时运行。</p><h3 id="同步执行创建瓶颈"><a href="#同步执行创建瓶颈" class="headerlink" title="同步执行创建瓶颈"></a>同步执行创建瓶颈</h3><p>目前，我们的主智能体同步执行子智能体，等待每组子智能体完成后再继续。这简化了协调，但在智能体之间的信息流中创建了瓶颈。例如，主智能体无法引导子智能体，子智能体无法协调，整个系统可能因等待单个子智能体完成搜索而被阻塞。异步执行将实现额外的并行性：智能体并发工作并在需要时创建新的子智能体。但这种异步性在结果协调、状态一致性和跨子智能体错误传播方面增加了挑战。随着模型能够处理更长更复杂的研究任务，我们预期性能增益将证明复杂性是合理的。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在构建 AI 智能体时，最后一英里通常成为大部分旅程。在开发机器上工作的代码库需要重要的工程才能成为可靠的生产系统。智能体系统中错误的复合性质意味着传统软件的小问题可能完全使智能体脱轨。一步失败可能导致智能体探索完全不同的轨迹，导致不可预测的结果。由于本文中描述的所有原因，原型和生产之间的差距通常比预期的更宽。</p><p>尽管存在这些挑战，多智能体系统已被证明对开放性研究任务有价值。用户表示 Claude 帮助他们找到了没有考虑过的商业机会，导航复杂的医疗保健选项，解决棘手的技术错误，并通过发现他们单独不会找到的研究连接节省了数天的工作。多智能体研究系统可以通过仔细的工程、全面的测试、细致的提示和工具设计、健壮的操作实践，以及对当前智能体能力有深刻理解的研究、产品和工程团队之间的紧密协作，在规模上可靠运行。我们已经看到这些系统正在改变人们解决复杂问题的方式。</p><p><em>人们今天使用研究功能的最常见方式。主要用例类别是：跨专业领域开发软件系统（10%），开发和优化专业和技术内容（8%），制定业务增长和收入生成策略（8%），协助学术研究和教育材料开发（7%），以及研究和验证关于人员、地点或组织的信息（5%）。</em></p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>以下是多智能体系统的一些额外杂项提示。</p><h3 id="改变状态的多轮智能体的终态评估"><a href="#改变状态的多轮智能体的终态评估" class="headerlink" title="改变状态的多轮智能体的终态评估"></a>改变状态的多轮智能体的终态评估</h3><p>评估在多轮对话中修改持久状态的智能体面临独特挑战。与只读研究任务不同，每个动作都可能改变后续步骤的环境，创建传统评估方法难以处理的依赖关系。我们发现专注于终态评估而不是逐轮分析是成功的。不是判断智能体是否遵循了特定过程，而是评估它是否达到了正确的最终状态。这种方法承认智能体可能找到实现相同目标的替代路径，同时仍确保它们提供预期结果。对于复杂工作流程，将评估分解为应该发生特定状态变化的离散检查点，而不是试图验证每个中间步骤。</p><h3 id="长期对话管理"><a href="#长期对话管理" class="headerlink" title="长期对话管理"></a>长期对话管理</h3><p>生产智能体经常参与跨越数百轮的对话，需要仔细的上下文管理策略。随着对话延长，标准上下文窗口变得不足，需要智能的压缩和内存机制。我们实施了智能体总结已完成工作阶段并在继续新任务之前将重要信息存储在外部内存中的模式。当上下文限制接近时，智能体可以通过仔细的交接生成具有干净上下文的新子智能体，同时保持连续性。此外，它们可以从内存中检索存储的上下文，如研究计划，而不是在达到上下文限制时丢失先前的工作。这种分布式方法防止上下文溢出，同时在扩展交互中保持对话连贯性。</p><h3 id="子智能体输出到文件系统以最小化”传话游戏”"><a href="#子智能体输出到文件系统以最小化”传话游戏”" class="headerlink" title="子智能体输出到文件系统以最小化”传话游戏”"></a>子智能体输出到文件系统以最小化”传话游戏”</h3><p>直接子智能体输出可以绕过主协调器处理某些类型的结果，提高保真度和性能。与其要求子智能体通过主智能体沟通一切，不如实施工件系统，专门智能体可以创建独立持久的输出。子智能体调用工具将其工作存储在外部系统中，然后将轻量级引用传回协调器。这防止在多阶段处理期间的信息丢失，并减少通过对话历史复制大输出的 token 开销。该模式特别适用于结构化输出，如代码、报告或数据可视化，其中子智能体的专门提示比通过通用协调器过滤产生更好的结果。</p><p><em>参考文献：</em></p><ul><li><a href="https://www.anthropic.com/engineering/multi-agent-research-system" target="_blank" rel="noopener">原文链接</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看打造 Agent 相关的研究，发现 Anthropic 他们的一篇文章写的特别好，有很多工程实践经验值得参考。虽然没有披露更多细节，但是也指出了很多方向。以下基本是原文翻译。&lt;/p&gt;
    
    </summary>
    
      <category term="Agent" scheme="https://murphypei.github.io/categories/Agent/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="AI" scheme="https://murphypei.github.io/tags/AI/"/>
    
      <category term="Agent" scheme="https://murphypei.github.io/tags/Agent/"/>
    
      <category term="Claude" scheme="https://murphypei.github.io/tags/Claude/"/>
    
      <category term="多智能体" scheme="https://murphypei.github.io/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
      <category term="工程实践" scheme="https://murphypei.github.io/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
  </entry>
  
  <entry>
    <title>RAG 中的检索核心问题</title>
    <link href="https://murphypei.github.io//blog/2025/08/rag-retrieval-quality.html"/>
    <id>https://murphypei.github.io//blog/2025/08/rag-retrieval-quality.html</id>
    <published>2025-08-29T05:35:25.000Z</published>
    <updated>2025-09-01T06:51:39.086Z</updated>
    
    <content type="html"><![CDATA[<p>RAG（检索增强生成）系统的核心在于能否准确、高效地检索到与用户查询最相关的文档片段。检索质量的好坏直接决定了最终生成结果的准确性和可靠性。本文将深入探讨 RAG 系统中检索优化的关键策略和最佳实践。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在当今的 AI 应用中，RAG 已经成为解决大语言模型知识局限性的重要技术方案。然而，构建一个高质量的 RAG 系统远不止是简单地将文档向量化后进行相似度搜索。检索环节的优化往往决定了整个系统的成败。</p><p>要保证 RAG 应用能准确检索到所需的文档，我们需要同时关注<strong>召回率（Recall）</strong>和<strong>准确率（Precision）</strong>：</p><ul><li><strong>召回率</strong>：所有相关文档中，被系统检索出来的比例</li><li><strong>准确率</strong>：被系统检索出来的文档中，真正相关的比例</li></ul><p>在 RAG 中，这两者往往是此消彼长的关系，需要找到一个最优的平衡点。</p><h2 id="检索质量评估框架"><a href="#检索质量评估框架" class="headerlink" title="检索质量评估框架"></a>检索质量评估框架</h2><h3 id="核心指标"><a href="#核心指标" class="headerlink" title="核心指标"></a>核心指标</h3><p>除了召回率和准确率，我们还需要关注以下指标：</p><ul><li><strong>NDCG（Normalized Discounted Cumulative Gain）</strong>：考虑排序位置的相关性指标</li><li><strong>MRR（Mean Reciprocal Rank）</strong>：平均倒数排名，衡量第一个相关结果的位置</li><li><strong>Hit Rate@K</strong>：前 K 个结果中包含相关文档的查询比例</li></ul><h3 id="评估数据集构建"><a href="#评估数据集构建" class="headerlink" title="评估数据集构建"></a>评估数据集构建</h3><p>建立高质量的评估数据集是优化的前提。例如，对于”如何优化深度学习模型的训练速度？”这样的查询，正相关文档应该是讨论 GPU 并行、批处理优化的内容，部分相关的可能涉及模型压缩、量化，而纯理论介绍或其他领域的文档则属于不相关。</p><h2 id="提升召回率的策略"><a href="#提升召回率的策略" class="headerlink" title="提升召回率的策略"></a>提升召回率的策略</h2><p>召回率关注的是”不漏”，即尽可能地找到所有相关的文档。</p><h3 id="1-优化分块（Chunking）策略"><a href="#1-优化分块（Chunking）策略" class="headerlink" title="1. 优化分块（Chunking）策略"></a>1. 优化分块（Chunking）策略</h3><p>分块是 RAG 的基础，而且非常重要。直接影响检索效果：</p><h4 id="分块大小优化"><a href="#分块大小优化" class="headerlink" title="分块大小优化"></a>分块大小优化</h4><ul><li><strong>固定大小分块</strong>：通常 512-1024 tokens 为宜。既不会太短，也不会太长。分块太小可能丢失上下文，分块太大则可能引入无关信息。</li><li><strong>基于语义的动态分块</strong>：尽量按照段落、章节等自然语义边界进行分块，而不是简单的固定字符数。例如，将完整的问答对或表格作为独立的块。</li><li><strong>重叠策略</strong>： 在相邻分块之间设置一定的重叠部分，有助于保留跨块的上下文信息，比如相邻块间保持 20-50 tokens 重叠。</li></ul><h4 id="语义边界保持"><a href="#语义边界保持" class="headerlink" title="语义边界保持"></a>语义边界保持</h4><p>实际分块操作可能是多种分块策略结合使用。比如先基于段落进行动态分块，然后在段落内部，基于固定大一或者句子来分块。基于语义的分块策略应该考虑句子边界和语义相似度，通过句子分割、动态调整块大小、保持语义完整性等方式，确保每个分块都包含完整的语义信息。</p><h4 id="结构化信息处理"><a href="#结构化信息处理" class="headerlink" title="结构化信息处理"></a>结构化信息处理</h4><ul><li><strong>表格数据</strong>：保持表格完整性，添加表头上下文。</li><li><strong>代码片段</strong>：包含函数/类的完整定义。</li><li><strong>列表项</strong>：保持列表的逻辑完整性。</li></ul><h3 id="2-查询增强（Query-Expansion）"><a href="#2-查询增强（Query-Expansion）" class="headerlink" title="2. 查询增强（Query Expansion）"></a>2. 查询增强（Query Expansion）</h3><p>通过扩展查询来增加召回相关文档的机会：</p><h4 id="多角度查询生成"><a href="#多角度查询生成" class="headerlink" title="多角度查询生成"></a>多角度查询生成</h4><p>使用 LLM 生成多个查询变体，可以通过以下方式实现：使用同义词替换、改变句式结构、添加相关术语、简化表达、详细描述等方法，为原始查询生成多个语义相似但表达方式不同的变体。</p><h4 id="层次化查询策略"><a href="#层次化查询策略" class="headerlink" title="层次化查询策略"></a>层次化查询策略</h4><ul><li><strong>粗粒度检索</strong>：使用概括性词汇扩大检索范围</li><li><strong>细粒度检索</strong>：使用具体术语提高精确度</li><li><strong>多层融合</strong>：综合不同粒度的检索结果</li></ul><h3 id="3-混合检索（Hybrid-Search）"><a href="#3-混合检索（Hybrid-Search）" class="headerlink" title="3. 混合检索（Hybrid Search）"></a>3. 混合检索（Hybrid Search）</h3><p>结合多种检索方法发挥各自优势：</p><h4 id="稀疏检索-密集检索"><a href="#稀疏检索-密集检索" class="headerlink" title="稀疏检索 + 密集检索"></a>稀疏检索 + 密集检索</h4><p>混合检索通过结合 BM25 关键词检索和向量语义检索来实现。首先分别进行两种检索，然后对检索分数进行归一化处理，最后通过加权融合（如向量检索权重 70%，BM25 权重 30%）得到最终排序结果。</p><h4 id="多模态检索"><a href="#多模态检索" class="headerlink" title="多模态检索"></a>多模态检索</h4><ul><li><strong>文本+图像</strong>：同时索引文档中的文字和图表信息</li><li><strong>结构化+非结构化</strong>：结合表格数据和自然语言描述</li><li><strong>元数据增强</strong>：利用时间、作者、类别等元信息</li></ul><h2 id="提升准确率的策略"><a href="#提升准确率的策略" class="headerlink" title="提升准确率的策略"></a>提升准确率的策略</h2><p>准确率关注的是”不瞎”，即召回的文档都是真正需要的。</p><h3 id="1-嵌入模型优化"><a href="#1-嵌入模型优化" class="headerlink" title="1. 嵌入模型优化"></a>1. 嵌入模型优化</h3><p>嵌入模型质量直接决定向量检索的准确性：</p><h4 id="领域适配"><a href="#领域适配" class="headerlink" title="领域适配"></a>领域适配</h4><ul><li><strong>预训练模型选择</strong>：如科学文献使用 SciBERT，法律文档使用 LegalBERT</li><li><strong>微调策略</strong>：在特定领域数据上进行对比学习微调</li><li><strong>多语言支持</strong>：针对中英文混合文档的特殊处理</li></ul><h4 id="嵌入维度优化"><a href="#嵌入维度优化" class="headerlink" title="嵌入维度优化"></a>嵌入维度优化</h4><p>嵌入维度的选择需要在性能和质量之间找平衡：</p><ul><li>128 维：速度快、准确率低、内存占用小</li><li>384 维：速度中等、准确率中等、内存占用中等</li><li>768 维：速度慢、准确率高、内存占用大</li><li>1024 维：速度很慢、准确率很高、内存占用很大</li></ul><h3 id="2-重排序（Reranking）"><a href="#2-重排序（Reranking）" class="headerlink" title="2. 重排序（Reranking）"></a>2. 重排序（Reranking）</h3><p>对初始检索结果进行精细化排序：</p><h4 id="Cross-Encoder-重排序"><a href="#Cross-Encoder-重排序" class="headerlink" title="Cross-Encoder 重排序"></a>Cross-Encoder 重排序</h4><p>使用重排序模型对候选文档进行精细排序的过程包括：构建查询-文档对、使用重排序模型计算相关性分数、根据分数进行排序，最终返回排序后的前 K 个文档。</p><h4 id="多阶段重排序"><a href="#多阶段重排序" class="headerlink" title="多阶段重排序"></a>多阶段重排序</h4><ol><li><strong>粗排</strong>：使用轻量级模型快速筛选Top-100</li><li><strong>精排</strong>：使用复杂模型对Top-20进行精确排序</li><li><strong>多样性调整</strong>：避免结果过于集中在相似文档</li></ol><h3 id="3-智能过滤策略"><a href="#3-智能过滤策略" class="headerlink" title="3. 智能过滤策略"></a>3. 智能过滤策略</h3><h4 id="预过滤机制"><a href="#预过滤机制" class="headerlink" title="预过滤机制"></a>预过滤机制</h4><ul><li><strong>元数据过滤</strong>：根据文档类型、时间范围、权威性筛选</li><li><strong>关键词门槛</strong>：确保文档包含查询的核心术语</li><li><strong>质量评分</strong>：基于文档完整性、可读性的预评分</li></ul><h4 id="后过滤优化"><a href="#后过滤优化" class="headerlink" title="后过滤优化"></a>后过滤优化</h4><p>检索后的文档过滤包括多个层面：相关性阈值过滤（如最低相关性 0.3）、重复内容检测（如相似度阈值 0.8）、内容质量检查等，通过这些过滤机制确保最终返回的文档都符合质量要求。</p><h2 id="高级优化技术"><a href="#高级优化技术" class="headerlink" title="高级优化技术"></a>高级优化技术</h2><h3 id="1-自适应检索策略"><a href="#1-自适应检索策略" class="headerlink" title="1. 自适应检索策略"></a>1. 自适应检索策略</h3><p>根据查询特征动态调整检索策略。首先分析查询的复杂度特征，然后根据不同类型采用相应策略：简单查询重点使用关键词匹配，概念性查询重点进行语义理解，复杂查询则采用混合策略。</p><h3 id="2-查询意图理解"><a href="#2-查询意图理解" class="headerlink" title="2. 查询意图理解"></a>2. 查询意图理解</h3><h4 id="意图分类"><a href="#意图分类" class="headerlink" title="意图分类"></a>意图分类</h4><ul><li><strong>事实查询</strong>：寻找具体信息（who, what, when）</li><li><strong>程序查询</strong>：寻找操作步骤（how to）</li><li><strong>比较查询</strong>：对比不同选项（difference, comparison）</li><li><strong>分析查询</strong>：深入理解（why, analysis）</li></ul><h4 id="针对性优化"><a href="#针对性优化" class="headerlink" title="针对性优化"></a>针对性优化</h4><p>根据不同查询意图采用相应的优化策略：</p><ul><li><strong>事实查询</strong>：embedding 权重 30%，关键词权重 70%，使用事实聚焦的重排序模型</li><li><strong>程序查询</strong>：embedding 权重 60%，关键词权重 40%，使用步骤感知的重排序模型  </li><li><strong>分析查询</strong>：embedding 权重 80%，关键词权重 20%，使用上下文感知的重排序模型</li></ul><h3 id="3-动态索引优化"><a href="#3-动态索引优化" class="headerlink" title="3. 动态索引优化"></a>3. 动态索引优化</h3><h4 id="增量更新策略"><a href="#增量更新策略" class="headerlink" title="增量更新策略"></a>增量更新策略</h4><ul><li><strong>热点文档</strong>：高频访问文档的索引优化</li><li><strong>时效性文档</strong>：新增文档的快速索引</li><li><strong>过期清理</strong>：定期清理不再相关的文档</li></ul><h4 id="索引压缩技术"><a href="#索引压缩技术" class="headerlink" title="索引压缩技术"></a>索引压缩技术</h4><ul><li><strong>向量量化</strong>：使用PQ（Product Quantization）压缩</li><li><strong>稀疏化</strong>：去除低权重的向量维度</li><li><strong>分层索引</strong>：构建粗粒度到细粒度的多层索引</li></ul><h2 id="实践中的平衡之道"><a href="#实践中的平衡之道" class="headerlink" title="实践中的平衡之道"></a>实践中的平衡之道</h2><h3 id="召回优先策略"><a href="#召回优先策略" class="headerlink" title="召回优先策略"></a>召回优先策略</h3><p>在实际应用中，通常采用”召回优先，精度优化”的两阶段策略：</p><ol><li><p><strong>广泛召回阶段</strong></p><ul><li>使用宽松的相似度阈值</li><li>应用多种查询扩展技术</li><li>结合多种检索方法</li></ul></li><li><p><strong>精度优化阶段</strong></p><ul><li>应用重排序模型</li><li>执行多层过滤</li><li>进行结果去重和多样性优化</li></ul></li></ol><h3 id="性能与质量权衡"><a href="#性能与质量权衡" class="headerlink" title="性能与质量权衡"></a>性能与质量权衡</h3><p>不同应用场景需要不同的配置策略：</p><ul><li><strong>实时问答</strong>：延迟预算 &lt;200ms，采用轻量级 embedding+简单重排序，牺牲部分准确率换取响应速度</li><li><strong>深度分析</strong>：延迟预算 &lt;5s，采用高质量 embedding+复杂重排序，容忍较高延迟获得最佳质量</li><li><strong>批量处理</strong>：无延迟限制，采用多模型 ensemble+全面后处理，追求最高质量</li></ul><h3 id="持续优化机制"><a href="#持续优化机制" class="headerlink" title="持续优化机制"></a>持续优化机制</h3><h4 id="A-B-测试框架"><a href="#A-B-测试框架" class="headerlink" title="A/B 测试框架"></a>A/B 测试框架</h4><ul><li><strong>检索策略对比</strong>：不同算法的效果验证</li><li><strong>参数调优</strong>：阈值、权重等超参数优化</li><li><strong>用户体验监控</strong>：基于真实反馈的持续改进</li></ul><h4 id="监控指标体系"><a href="#监控指标体系" class="headerlink" title="监控指标体系"></a>监控指标体系</h4><p>建立全面的监控指标体系：</p><ul><li><strong>检索质量</strong>：NDCG@10、MRR、Hit Rate@5</li><li><strong>系统性能</strong>：平均延迟、P99 延迟、QPS</li><li><strong>用户满意度</strong>：点击率、停留时间、反馈评分</li><li><strong>业务指标</strong>：任务完成率、准确答案比例、用户留存</li></ul><h2 id="未来发展趋势"><a href="#未来发展趋势" class="headerlink" title="未来发展趋势"></a>未来发展趋势</h2><h3 id="1-多模态检索融合"><a href="#1-多模态检索融合" class="headerlink" title="1. 多模态检索融合"></a>1. 多模态检索融合</h3><p>随着多模态大模型的发展，RAG 系统将更好地处理文本、图像、音视频等多种模态的信息检索和融合。</p><h3 id="2-个性化检索优化"><a href="#2-个性化检索优化" class="headerlink" title="2. 个性化检索优化"></a>2. 个性化检索优化</h3><p>基于用户历史行为和偏好，构建个性化的检索模型，提供更精准的个人知识服务。</p><h3 id="3-实时学习能力"><a href="#3-实时学习能力" class="headerlink" title="3. 实时学习能力"></a>3. 实时学习能力</h3><p>检索系统将具备从用户反馈中实时学习的能力，持续优化检索质量。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>RAG 系统的检索优化是一个系统性工程，需要从分块策略、查询处理、检索算法、重排序等多个维度进行综合优化。关键在于：</p><ol><li><strong>建立完善的评估体系</strong>：确保优化方向正确</li><li><strong>平衡召回率与准确率</strong>：根据应用场景找到最优平衡点</li><li><strong>采用分层优化策略</strong>：粗排+精排的两阶段设计</li><li><strong>持续监控和迭代</strong>：基于真实数据不断优化</li></ol><p>只有通过系统性的优化和持续的迭代改进，才能构建出既能全面检索，又能精准定位的高质量 RAG 应用，为用户提供准确、及时、有价值的信息服务。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RAG（检索增强生成）系统的核心在于能否准确、高效地检索到与用户查询最相关的文档片段。检索质量的好坏直接决定了最终生成结果的准确性和可靠性。本文将深入探讨 RAG 系统中检索优化的关键策略和最佳实践。&lt;/p&gt;
    
    </summary>
    
      <category term="RAG" scheme="https://murphypei.github.io/categories/RAG/"/>
    
    
      <category term="chunk" scheme="https://murphypei.github.io/tags/chunk/"/>
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="RAG" scheme="https://murphypei.github.io/tags/RAG/"/>
    
      <category term="retrieval" scheme="https://murphypei.github.io/tags/retrieval/"/>
    
      <category term="index" scheme="https://murphypei.github.io/tags/index/"/>
    
      <category term="检索优化" scheme="https://murphypei.github.io/tags/%E6%A3%80%E7%B4%A2%E4%BC%98%E5%8C%96/"/>
    
      <category term="质量评估" scheme="https://murphypei.github.io/tags/%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>RAG 分块策略</title>
    <link href="https://murphypei.github.io//blog/2025/08/rag-chunking.html"/>
    <id>https://murphypei.github.io//blog/2025/08/rag-chunking.html</id>
    <published>2025-08-28T06:35:25.000Z</published>
    <updated>2025-09-01T06:51:42.298Z</updated>
    
    <content type="html"><![CDATA[<p>在构建一个高效的检索增强生成（RAG）系统时，一个常常被忽视但至关重要的环节是<strong>分块（Chunking）</strong>。这个过程是将大型文档切分成小块，以便 LLM 可以轻松地检索和理解。如果分块策略不当，即使拥有最先进的语言模型和向量数据库，你的 RAG 系统也可能表现不佳。</p><a id="more"></a><p>本文将深入探讨为什么分块如此重要，以及如何通过几种核心策略来优化它。</p><h2 id="什么是词块"><a href="#什么是词块" class="headerlink" title="什么是词块"></a>什么是词块</h2><p>将词块视为 RAG 系统中知识的原子单元 。每个词块应该：</p><ul><li><strong>语义完整</strong> ：包含单独来看有意义的连贯信息。</li><li>语境丰富 ：包含足够的周围环境，无需外部参考即可理解。</li><li>最佳尺寸 ：足够大以具有意义，足够小以便精确检索。</li><li>边界感知 ：尊重句子、段落和章节等自然语言结构。</li></ul><p>这些要求经常相互冲突。语义完整的块可能太大，无法进行精确检索。大小合适的块可能会在边界处丢失关键上下文。这时，不同的分块策略就派上用场了。每种方法都有各自的优缺点，最佳选择取决于您的具体用例、文档类型和检索需求。</p><h2 id="分块策略"><a href="#分块策略" class="headerlink" title="分块策略"></a>分块策略</h2><p>下面由浅入深，介绍不同的分块策略：</p><h3 id="固定大小分块（简单基线）"><a href="#固定大小分块（简单基线）" class="headerlink" title="固定大小分块（简单基线）"></a>固定大小分块（简单基线）</h3><p>固定大小分块将文档拆分为预定大小的块，通常以以下方式测量：</p><ul><li>字符数 （例如，每块 1000 个字符）</li><li>令牌数量 （例如，每块 256 个令牌）</li><li>字数统计 （例如，每段 200 个字）</li></ul><p>重叠参数在这里至关重要。如果没有重叠，您可能会丢失跨越块边界的信息。重叠率为 20% 时，可以确保相邻块之间有一定的连续性。</p><p>固定大小分块策略几乎不适用于任何场景，仅在有限计算资源的情况。固定大小分块适用的场景：</p><ul><li>有统一、简单的文档（博客、文章、小说）</li><li>处理速度至关重要</li><li>存储和计算资源严重有限</li></ul><h3 id="语义分块"><a href="#语义分块" class="headerlink" title="语义分块"></a>语义分块</h3><p>语义分块代表着思维的根本性转变。语义分块不再根据大小限制任意切割文本，而是根据语义和文档结构识别自然断点。语义分块需要更多的计算资源，但能带来更好的结果。</p><p>核心见解：<strong>文档具有固有的语义边界，可以指导分块决策</strong>。</p><p>语义分块的工作原理：</p><ol><li>句子分割 ：将文档分解成单个句子</li><li>嵌入生成 ：为每个句子创建向量表示</li><li>相似度分析 ：测量<strong>相邻句子</strong>之间的语义相似度</li><li>边界检测 ：在相似度低于阈值的地方创建块边界</li><li>组块 ：将连续相似的句子组合成连贯的组块</li></ol><p>高级语义技术：</p><ul><li>主题建模集成 ：使用基于 LDA 或 BERT 的主题模型来识别主题边界</li><li>层次聚类 ：在分块之前按语义相似度对句子进行分组</li><li>实体连续性 ：确保命名实体及其引用保持在同一块内</li><li>话语标记 ：使用语言线索（但是、此外、总之）来识别界限</li></ul><p>语义分块适用于很多场景：</p><ul><li>文档质量差异很大</li><li>上下文保存很重要</li><li>准确性比速度更重要</li></ul><h3 id="分层文档分块"><a href="#分层文档分块" class="headerlink" title="分层文档分块"></a>分层文档分块</h3><p>对于具有嵌套结构的复杂文档（例如技术手册、法律合同或学术论文），分层分块可以保留文档的组织逻辑。这种方法认识到文档不是平面文本，而是具有章节、小节和逻辑层次的结构化知识。</p><p>分层分块的主要优点：</p><ul><li>上下文继承 ：每个块都知道它在文档层次结构中的位置</li><li>元数据保存 ：维护章节标题、级别和关系</li><li>结构感知检索 ：您可以按不同的粒度进行检索（部分与小节）</li><li>交叉引用处理 ：对其他部分的引用仍然有意义</li></ul><p>对于 200 页的技术手册，分层分块可能会创建：</p><ol><li>15 个章节级块（高级概述）</li><li>127 个部分级块（详细解释）</li><li>340 个子节级块（具体程序）</li></ol><p>在检索过程中，系统可以将查询与适当的细节级别进行匹配，并提供分层上下文。</p><p>如果出现以下情况，则实施分层分块：</p><ul><li>文件结构清晰（手册、报告、学术论文）</li><li>用户需要不同程度的细节</li><li>交叉引用很常见</li><li>文档格式结构良好</li></ul><h3 id="滑动窗口分块（重叠优化器）"><a href="#滑动窗口分块（重叠优化器）" class="headerlink" title="滑动窗口分块（重叠优化器）"></a>滑动窗口分块（重叠优化器）</h3><p>传统的组块方法会创建离散的、不重叠的组块。但是，如果最重要的信息恰好落在组块边界上，该怎么办呢？滑动窗口分块通过创建重叠块解决了这个问题，确保没有信息被遗漏。</p><p>当 window_size=1000、stride=800 时，可获得 200 个字符的重叠（20%）。此重叠可确保：</p><ol><li>关键句子没有被拆分成多个部分</li><li>上下文在相邻块之间自然流动</li><li>检索有多次机会找到相关信息</li></ol><h3 id="自适应滑动窗口"><a href="#自适应滑动窗口" class="headerlink" title="自适应滑动窗口"></a>自适应滑动窗口</h3><p>高级实现根据内容密度调整窗口大小和步幅：</p><ol><li>常见问题解答文档 ：问题和答案受益于重叠的上下文</li><li>法律文件 ：精确的语言界限至关重要</li><li>技术程序 ：步骤参考先前的操作</li><li>叙事内容 ：故事连续性至关重要</li></ol><h3 id="大模型分块"><a href="#大模型分块" class="headerlink" title="大模型分块"></a>大模型分块</h3><p>使用大语言模型进行递归分块： 最新的进展是使用 LLM 本身进行分块决策。</p><h3 id="多模态分块"><a href="#多模态分块" class="headerlink" title="多模态分块"></a>多模态分块</h3><p>对于包含图像、表格和混合内容的文档。</p><h3 id="基于查询模式的动态分块"><a href="#基于查询模式的动态分块" class="headerlink" title="基于查询模式的动态分块"></a>基于查询模式的动态分块</h3><p>最先进的系统根据文档的实际查询方式来调整分块。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在构建一个高效的检索增强生成（RAG）系统时，一个常常被忽视但至关重要的环节是&lt;strong&gt;分块（Chunking）&lt;/strong&gt;。这个过程是将大型文档切分成小块，以便 LLM 可以轻松地检索和理解。如果分块策略不当，即使拥有最先进的语言模型和向量数据库，你的 RAG 系统也可能表现不佳。&lt;/p&gt;
    
    </summary>
    
      <category term="RAG" scheme="https://murphypei.github.io/categories/RAG/"/>
    
    
      <category term="chunk" scheme="https://murphypei.github.io/tags/chunk/"/>
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="RAG" scheme="https://murphypei.github.io/tags/RAG/"/>
    
      <category term="retrieval" scheme="https://murphypei.github.io/tags/retrieval/"/>
    
      <category term="index" scheme="https://murphypei.github.io/tags/index/"/>
    
      <category term="分块" scheme="https://murphypei.github.io/tags/%E5%88%86%E5%9D%97/"/>
    
      <category term="检索" scheme="https://murphypei.github.io/tags/%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>Agent 几个问题思考</title>
    <link href="https://murphypei.github.io//blog/2025/07/agent-fc-mcp.html"/>
    <id>https://murphypei.github.io//blog/2025/07/agent-fc-mcp.html</id>
    <published>2025-07-28T19:35:25.000Z</published>
    <updated>2025-09-02T04:02:26.729Z</updated>
    
    <content type="html"><![CDATA[<p>最近遇到了几个大模型模型算法应用的关键问题，作为记录。</p><a id="more"></a><h3 id="Agent-设计范式"><a href="#Agent-设计范式" class="headerlink" title="Agent 设计范式"></a>Agent 设计范式</h3><p>目前主流的 AI Agent（智能体）设计模式，通常不是单一的，而是基于一些核心思想和框架的组合。这些设计模式旨在赋予大语言模型<strong>自主思考、规划、行动和反思</strong>的能力，以完成更复杂的任务。</p><h4 id="1-ReAct-Reasoning-and-Acting"><a href="#1-ReAct-Reasoning-and-Acting" class="headerlink" title="1. ReAct (Reasoning and Acting)"></a>1. ReAct (Reasoning and Acting)</h4><p>这是目前最流行、最基础也是最核心的一种 Agent 设计模式。它的核心思想是让大语言模型在一个<strong>“思考-行动-观察”</strong>的循环中持续工作，直到任务完成。</p><ul><li><strong>思考（Thought）</strong>：LLM 会根据当前任务和过往观察，产生一段内部思考，比如“我需要找到XX信息，我应该使用什么工具？”</li><li><strong>行动（Action）</strong>：LLM 根据思考，生成一个调用外部工具的指令（即 Function Call）。</li><li><strong>观察（Observation）</strong>：LLM 接收到外部工具返回的结果。</li></ul><p>这个循环会反复进行，直到 LLM 判断任务已完成并生成最终回复。这种模式将”思考”过程显性化，使得模型的决策过程更加透明和可控。</p><h4 id="2-Plan-and-Execute（规划与执行）"><a href="#2-Plan-and-Execute（规划与执行）" class="headerlink" title="2. Plan-and-Execute（规划与执行）"></a>2. Plan-and-Execute（规划与执行）</h4><p>这种模式更侧重于对复杂任务的<strong>结构化分解</strong>。它将一个大任务分为两个阶段：</p><ul><li><strong>规划阶段（Planning）</strong>：LLM 首先会分析用户的请求，并生成一个详细的、分步骤的行动计划。这个计划是固定的，不会在执行过程中轻易改变。</li><li><strong>执行阶段（Execution）</strong>：LLM 严格按照规划好的步骤，一步一步地调用工具和执行任务。</li></ul><p>这种模式的优点是任务流程清晰、稳定，适合需要按部就班完成的复杂任务。但缺点是缺乏灵活性，如果计划中的某个步骤失败，Agent 可能无法自主调整。</p><h4 id="3-Self-Correction（自我纠正-反思）"><a href="#3-Self-Correction（自我纠正-反思）" class="headerlink" title="3. Self-Correction（自我纠正/反思）"></a>3. Self-Correction（自我纠正/反思）</h4><p>这种设计模式的核心是让 Agent 具备<strong>复盘和纠错</strong>的能力。它通常与其他模式结合使用，为 Agent 增加一个”反思”步骤。</p><ul><li><strong>反思（Reflection）</strong>：Agent 在完成一个任务或得到一个结果后，会重新评估这个结果是否正确或达到预期。</li><li><strong>纠正（Correction）</strong>：如果评估结果不理想，Agent 会根据反思结果，修改其原始的思考路径或行动计划，然后再次尝试，直到达到满意的结果。</li></ul><p>这种模式能显著提升 Agent 在复杂问题上的表现，因为它允许 Agent 从错误中学习，避免重复犯错。</p><p>这三种模式并不是相互独立的。一个强大的 Agent 通常会结合使用这些思想：例如，一个 Agent 可能先用 <strong>Plan-and-Execute</strong> 进行任务分解，然后在每个执行步骤中，使用 <strong>ReAct</strong> 循环来调用工具，并最终用 <strong>Self-Correction</strong> 机制来验证和修正结果。</p><h3 id="大模型如何”用”工具？Agent、Function-Call-与-MCP-的进化之路"><a href="#大模型如何”用”工具？Agent、Function-Call-与-MCP-的进化之路" class="headerlink" title="大模型如何”用”工具？Agent、Function Call 与 MCP 的进化之路"></a>大模型如何”用”工具？Agent、Function Call 与 MCP 的进化之路</h3><p>在构建基于大语言模型（LLM）的应用时，一个核心挑战是让 LLM 不只停留在”聊天”，而是真正具备”行动”能力，比如联网搜索、调用 API 或执行代码。这就是 <strong>Agent</strong> 的核心思想：让 LLM 像一个智能体一样，能够根据用户的指令，自主决定是直接回答，还是调用外部工具来获取信息或完成任务。</p><h4 id="1-Agent-的决策过程：LLM-如何知道何时调用工具？"><a href="#1-Agent-的决策过程：LLM-如何知道何时调用工具？" class="headerlink" title="1. Agent 的决策过程：LLM 如何知道何时调用工具？"></a>1. Agent 的决策过程：LLM 如何知道何时调用工具？</h4><p>Agent 的决策机制本质上是 <strong>Prompt Engineering</strong> 的一种高级应用。开发者会设计一个精巧的 <strong>Prompt</strong>，将可用的工具列表、它们的用途和描述一并告诉 LLM。</p><p>例如，当我们问一个 Agent：“今天北京的天气怎么样？”它的思考过程可能如下：</p><ol><li><strong>用户意图分析：</strong> 用户想知道北京的天气。</li><li><strong>工具匹配：</strong> 我有一个可以查询天气的工具（<code>get_weather(location)</code>）。</li><li><strong>决策与执行：</strong> 我需要调用这个工具，并把“北京”作为参数。</li></ol><p>这个思考过程并不神秘，而是通过精心设计的 <strong>Prompt</strong> 来引导 LLM 生成。LLM 会根据输入的指令和工具描述，在输出中”思考”并生成一个结构化的行动指令，然后由外部程序（Agent 框架）去实际执行。</p><h4 id="2-Function-Call-FC-：将决策能力内置到模型中"><a href="#2-Function-Call-FC-：将决策能力内置到模型中" class="headerlink" title="2. Function Call (FC)：将决策能力内置到模型中"></a>2. Function Call (FC)：将决策能力内置到模型中</h4><p><strong>Function Call (FC)</strong> 是对上述 Agent 决策机制的一种原生优化。它将“思考”和“生成调用指令”的能力直接通过模型训练内置进去。</p><p><strong>FC 的核心是：</strong> 模型能够根据上下文，直接以预先定义好的 <strong>JSON 格式</strong> 生成对外部工具的调用，而不是像传统 Agent 那样需要外部框架去解析 LLM 生成的文本。</p><p>这是一种巨大的进步，因为它使得工具调用更加稳定、高效，并减少了外部解析的复杂性。</p><p><strong>那么，为什么说 FC 存在“MxN”的问题？</strong></p><p>这里有一个常见的误解：很多人以为每增加一个工具，就需要重新训练模型。<strong>这是不正确的。</strong></p><p>FC 的”MxN”问题不在于模型本身，而在于 <strong>工具的描述格式</strong>。每个拥有 FC 能力的 LLM 平台（如 OpenAI, Google, Anthropic）都有自己独特的工具描述 Schema（函数签名）。一个搜索工具，为了能被不同的模型调用，开发者需要为它编写 <strong>M</strong> 份不同的描述文档。同样，一个 Agent 开发者如果想使用 <strong>N</strong> 个工具，并支持 <strong>M</strong> 个不同的 LLM，就需要处理 MxN 个兼容性问题。</p><p>简而言之，<strong>FC 解决了“让模型知道如何调用工具”的问题，但没有解决“工具描述格式不统一”的问题。</strong></p><h4 id="3-MCP：通过抽象层实现真正的解耦"><a href="#3-MCP：通过抽象层实现真正的解耦" class="headerlink" title="3. MCP：通过抽象层实现真正的解耦"></a>3. MCP：通过抽象层实现真正的解耦</h4><p><strong>Multi-tool Coordinator Protocol (MCP)</strong> 正是为了解决 FC 带来的兼容性与扩展性问题而诞生的。</p><p><strong>MCP 的核心思想是：在模型和工具之间增加一个抽象的中间层。</strong></p><p>这个中间层通常由一个 <strong>MCP Server</strong> 组成，其工作流程如下：</p><ol><li><strong>工具注册：</strong> 所有外部工具都按照一套统一的 <strong>MCP 协议</strong> 接入并注册到 MCP Server。</li><li><strong>Agent (LLM) 接入：</strong> 所有的 Agent 只需要学习并支持这套统一的 <strong>MCP 协议</strong>。它们不再需要知道每个工具的具体描述细节，只需向 MCP Server 发送一个统一的请求，来获取可用的工具列表或执行工具调用。</li></ol><p><strong>为什么说 MCP 实现了“M+N”？</strong></p><ul><li><strong>M</strong> 个不同的模型（或 Agent）只需要对接 <strong>1</strong> 个统一的 MCP Server。</li><li><strong>N</strong> 个不同的工具也只需要对接 <strong>1</strong> 个统一的 MCP Server。</li></ul><p>通过这种方式，模型与工具之间不再是复杂的点对点连接，而是通过一个中心化的枢纽进行通信。这不仅解决了 FC 在不同平台间的兼容性问题，更是一种架构上的巨大优化，它让工具的管理、维护和扩展变得前所未有的简单。</p><p><strong>总结来说：</strong></p><ul><li><strong>Agent</strong> 是让 LLM 具备行动能力的 <strong>思想</strong>。</li><li><strong>Function Call</strong> 是将这种思想 <strong>内置到模型中</strong> 的一种能力。</li><li><strong>MCP</strong> 是在此基础上，进一步将 <strong>模型与工具解耦</strong> 的一种 <strong>架构设计</strong>。</li></ul><p>当然，MCP 还需要在调用端封装。在典型的 <strong>MCP (Multi-tool Coordinator Protocol)</strong> 实现中，会区分 <strong>AI Host (AI 宿主)</strong> 和 <strong>MCP Client (MCP 客户端)</strong> 这两个角色，它们通过一种清晰的协作模式来共同完成任务。</p><ol><li><p><strong>AI Host (AI 宿主)</strong></p><ul><li><strong>角色：</strong> AI Host 是整个系统的核心，通常是运行大语言模型（LLM）或 Agent 框架的那部分。它负责接收用户的指令，并进行高级别的决策与推理。</li><li><strong>职责：</strong><ul><li>接收用户输入。</li><li>与 LLM 交互，进行意图分析。</li><li>根据 LLM 的输出，决定是直接生成回复还是需要调用工具。</li><li><strong>注意：</strong> AI Host 不直接与具体的工具交互，它只知道 MCP 协议。</li></ul></li></ul></li><li><p><strong>MCP Client (MCP 客户端)</strong></p><ul><li><strong>角色：</strong> MCP Client 是一个独立的模块或库，作为 AI Host 与 MCP Server 之间的桥梁。它封装了所有与 MCP Server 通信的细节。</li><li><strong>职责：</strong><ul><li>将来自 AI Host 的工具调用请求，按照 <strong>MCP 协议</strong> 进行格式化，并发送给 MCP Server。</li><li>接收 MCP Server 返回的结果，并将其转换回 AI Host 可理解的格式。</li><li>管理与 MCP Server 的连接和会话。</li><li><strong>注意：</strong> MCP Client 也不关心具体的工具是如何实现的，它只负责协议层面的通信。</li></ul></li></ul></li></ol><p>整个 MCP 协作过程可以分解为以下几个步骤：</p><ol><li><strong>用户请求：</strong> 用户向 AI Host 发出指令，例如：“帮我查一下旧金山的天气。”</li><li><strong>AI Host 意图分析：</strong> AI Host 将用户指令发送给内置的 LLM。LLM 基于其 Function Call 或 ReAct 能力，判断出需要调用一个天气查询工具。它会生成一个结构化的调用请求，比如 <code>{&quot;tool_name&quot;: &quot;weather_api&quot;, &quot;parameters&quot;: {&quot;city&quot;: &quot;旧金山&quot;}}</code>。</li><li><strong>AI Host 委托：</strong> AI Host 拿到 LLM 生成的调用请求后，并不会自己去执行，而是将其 <strong>委托</strong> 给 <strong>MCP Client</strong>。</li><li><strong>MCP Client 协议转换与发送：</strong> MCP Client 接收到这个请求，将其封装成符合 MCP 协议的格式（例如一个特定的 HTTP POST 请求或 gRPC 调用），然后发送给远端的 <strong>MCP Server</strong>。</li><li><strong>MCP Server 工具协调与执行：</strong> MCP Server 接收到请求后，根据 <code>tool_name</code> 找到对应的工具，将 <code>parameters</code> 传递给该工具并执行。</li><li><strong>结果返回：</strong> 工具执行完毕，结果返回给 MCP Server，MCP Server 再将结果通过 MCP 协议返回给 MCP Client。</li><li><strong>MCP Client 结果转换与回传：</strong> MCP Client 接收到 MCP Server 的响应，将其解析并转换为 AI Host 可理解的格式，然后返回给 AI Host。</li><li><strong>AI Host 回复生成：</strong> AI Host 拿到工具执行结果（例如：”旧金山今天多云，气温 15 摄氏度”），将其作为上下文的一部分再次输入给 LLM，最终由 LLM 生成完整的自然语言回复给用户。</li></ol><p>通过这种方式，AI Host 始终保持”干净”，只关心高级别的推理和决策，而具体的工具调用和协议通信的复杂性则完全由 MCP Client 和 MCP Server 这层抽象来处理，实现了 AI 能力与工具生态的彻底解耦。</p><p>最后，通过一个图说明 FC 和 MCP 工作模式：</p><p><img src="/images/posts/agent/fc_mcp.gif" alt="FC 和 MCP 工作模式"></p><h3 id="JSON-格式化输出"><a href="#JSON-格式化输出" class="headerlink" title="JSON 格式化输出"></a>JSON 格式化输出</h3><p>在 <strong>Function Call (FC)</strong> 模式下，要保证模型稳定输出 JSON 格式，主要依赖于 <strong>模型本身的训练和微调</strong>。</p><h4 id="如何保证模型稳定输出-JSON？"><a href="#如何保证模型稳定输出-JSON？" class="headerlink" title="如何保证模型稳定输出 JSON？"></a>如何保证模型稳定输出 JSON？</h4><p>这并非单纯通过 Prompt Engineering 就能完美解决的问题。核心在于：</p><ol><li><strong>大规模训练：</strong> 在模型的预训练和指令微调阶段，会使用大量的包含 JSON 格式的结构化数据作为训练样本。这些样本告诉模型，当它接收到特定类型的指令（例如，要求它调用某个工具）时，应该输出一个遵循特定 JSON Schema 的结果。</li><li><strong>特殊的解码策略：</strong> 一些模型在解码时会采用特殊的约束，比如 <strong>JSON Schema 约束解码</strong>。这意味着，模型在生成每一个 token 时，都会检查其是否符合预先定义的 JSON 格式规则。如果生成的 token 会导致 JSON 格式无效，模型会将其“回溯”并尝试生成另一个 token，直到生成完整且正确的 JSON。这种方法极大地提高了输出的稳定性和正确性。</li><li><strong>Prompt 工程辅助：</strong> 尽管核心能力来自模型本身，但好的 Prompt 仍然至关重要。例如，在 Prompt 中清晰地描述工具的函数签名，并明确要求模型“请以 JSON 格式输出调用结果”，可以进一步引导模型输出期望的格式。</li></ol><p>比如这个 FC 输出的格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;get_current_weather&quot;,</span><br><span class="line">  &quot;arguments&quot;: &#123;</span><br><span class="line">    &quot;location&quot;: &quot;旧金山&quot;,</span><br><span class="line">    &quot;unit&quot;: &quot;celsius&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的每个 key 和 value 都要非常精准的匹配到调用的工具才可以。</p><h4 id="MCP-是否也要求模型输出固定格式？"><a href="#MCP-是否也要求模型输出固定格式？" class="headerlink" title="MCP 是否也要求模型输出固定格式？"></a>MCP 是否也要求模型输出固定格式？</h4><p><strong>是的，但要求的是更简单、更统一的格式。</strong></p><p><strong>MCP</strong> 的核心理念是 <strong>解耦</strong>。它将工具的复杂性和多样性从模型端剥离，因此模型不需要了解每个工具具体的 JSON Schema。模型唯一需要知道的，是与 <strong>MCP Client</strong> 交互的 <strong>统一协议</strong>。</p><p>这个统一协议的格式通常非常简单，比如一个包含 <code>tool_name</code> 和 <code>parameters</code> 的 JSON 对象。模型需要做的，只是稳定输出这个简单的、所有工具都通用的 JSON 格式，然后由 <strong>MCP Client</strong> 去处理后续的协议转换和工具调用。</p><p>这正是 <strong>MCP</strong> 的优势所在：它降低了对模型的要求。模型不需要针对每一个新工具去学习其独特的调用格式，它只需要掌握一套通用的、简单的输出格式即可。这使得模型可以更专注于其核心的自然语言理解和意图识别，而将复杂的<br>工具协调任务交给 <strong>MCP Client</strong> 和 <strong>MCP Server</strong> 去完成。</p><p>下面是一个模型输出的 MCP 格式的调用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;tool_name&quot;: &quot;weather_api&quot;,</span><br><span class="line">  &quot;parameters&quot;: &#123;</span><br><span class="line">    &quot;city&quot;: &quot;旧金山&quot;,</span><br><span class="line">    &quot;temp_unit&quot;: &quot;celsius&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个看起来比较类似，但实际上模型并不知道 weather_api 到底是什么，也不知道对于一个 city，应该传入的参数是什么（city？location？），对于温度单位，也类似。</p><p>FC (Function Call) 模式下，模型的难度体现在 “量” 上。<strong>模型需要记忆并理解每一个工具独特的 JSON Schema，而且要非常精确</strong>。如果你的系统有 100 个不同的工具，每个工具的参数都不一样，那模型就需要稳定地输出 100 种不同结构的 JSON。这就像让一个人记住 100 个完全不同的表格格式，并根据指令填写。</p><p>而 MCP (Multi-tool Coordinator Protocol) 模式下，模型的难度只体现在 “质” 上。它只需要学会一种 统一且简单的 JSON 格式，即 {“tool_name”: “…”, “parameters”: {…}}。无论有多少个工具，这个输出格式始终不变。这就像让一个人永远只填写一种固定格式的表格，然后把表格交给一个“总机”去处理后续的细节。</p><p>MCP Client 拿到的模型输出，也就是通用且简单的 JSON 格式（例如：{“tool_name”: “…”, “parameters”: {…}}），负责将其解析、封装、调用 Server、接收响应、解析响应，回传给 AI Host。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近遇到了几个大模型模型算法应用的关键问题，作为记录。&lt;/p&gt;
    
    </summary>
    
      <category term="Agent" scheme="https://murphypei.github.io/categories/Agent/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="agent" scheme="https://murphypei.github.io/tags/agent/"/>
    
      <category term="rag" scheme="https://murphypei.github.io/tags/rag/"/>
    
      <category term="mcp" scheme="https://murphypei.github.io/tags/mcp/"/>
    
  </entry>
  
  <entry>
    <title>LLM 训练：ZeRO 技术详解</title>
    <link href="https://murphypei.github.io//blog/2025/07/llm-zero.html"/>
    <id>https://murphypei.github.io//blog/2025/07/llm-zero.html</id>
    <published>2025-07-22T22:08:12.000Z</published>
    <updated>2025-07-23T12:02:52.150Z</updated>
    
    <content type="html"><![CDATA[<p>在大语言模型（LLM）训练中，显存不足是一个普遍存在的问题。随着模型规模的不断增长，单个 GPU 的显存容量成为了训练大规模模型的主要瓶颈。DeepSpeed ZeRO（Zero Redundancy Optimizer）技术通过创新的数据分片策略，有效解决了这一问题，使得我们能够训练远超单卡显存上限的超大规模模型。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>随着大语言模型规模的快速增长，显存需求呈指数级增长。传统的分布式训练方法虽然能够利用多 GPU 进行训练，但每个 GPU 仍然需要存储完整的模型参数、梯度和优化器状态，这严重限制了可训练的模型规模。</p><p>ZeRO 技术通过<strong>数据并行</strong>与<strong>内存优化</strong>的结合，将模型训练中的大块数据（优化器状态、梯度和模型参数）分散到不同的 GPU 上，而非在每个 GPU 上都完整存储一份，从而显著降低了每个 GPU 的显存需求。</p><h2 id="ZeRO-技术原理"><a href="#ZeRO-技术原理" class="headerlink" title="ZeRO 技术原理"></a>ZeRO 技术原理</h2><h3 id="传统数据并行的问题"><a href="#传统数据并行的问题" class="headerlink" title="传统数据并行的问题"></a>传统数据并行的问题</h3><p>在传统的数据并行训练中，每个 GPU 都需要存储：</p><ol><li><strong>模型参数</strong>：完整的模型权重</li><li><strong>梯度</strong>：完整的梯度信息</li><li><strong>优化器状态</strong>：如 Adam 优化器的动量、方差等状态</li></ol><p>对于大规模模型，这些数据占用的显存非常庞大。例如，一个 175B 参数的模型使用 Adam 优化器时，仅优化器状态就需要约 700GB 显存（每个参数需要 4 个 float32 值）。</p><h3 id="ZeRO-的核心思想"><a href="#ZeRO-的核心思想" class="headerlink" title="ZeRO 的核心思想"></a>ZeRO 的核心思想</h3><p>ZeRO 的核心思想是<strong>消除冗余存储</strong>，通过分片技术将原本每个 GPU 都需要存储的完整数据分散到多个 GPU 上，实现显存的线性扩展。</p><p><strong>关键洞察</strong>：</p><ul><li>在数据并行中，不同 GPU 上的模型参数是相同的</li><li>梯度在反向传播后需要进行 All-Reduce 操作</li><li>优化器状态与参数一一对应</li></ul><p>基于这些观察，ZeRO 提出了分阶段的内存优化策略。</p><h2 id="ZeRO-的三个阶段"><a href="#ZeRO-的三个阶段" class="headerlink" title="ZeRO 的三个阶段"></a>ZeRO 的三个阶段</h2><h3 id="ZeRO-Stage-1：优化器状态分片（Optimizer-State-Sharding）"><a href="#ZeRO-Stage-1：优化器状态分片（Optimizer-State-Sharding）" class="headerlink" title="ZeRO-Stage 1：优化器状态分片（Optimizer State Sharding）"></a>ZeRO-Stage 1：优化器状态分片（Optimizer State Sharding）</h3><p><strong>原理</strong>：<br>将优化器状态分片存储在不同的 GPU 上，每个 GPU 只存储部分优化器状态。</p><p><strong>具体做法</strong>：</p><ul><li>假设有 $N$ 个 GPU，模型参数为 $P$</li><li>将优化器状态分成 $N$ 个分片，每个 GPU 存储 $P/N$ 个参数对应的优化器状态</li><li>在参数更新时，每个 GPU 只更新自己负责的那部分参数</li></ul><p><strong>内存节省</strong>：</p><ul><li>优化器状态内存减少 $N$ 倍</li><li>对于 Adam 优化器，每个参数需要 4 个 float32 值，节省效果显著</li></ul><h3 id="ZeRO-Stage-2：梯度分片（Gradient-Sharding）"><a href="#ZeRO-Stage-2：梯度分片（Gradient-Sharding）" class="headerlink" title="ZeRO-Stage 2：梯度分片（Gradient Sharding）"></a>ZeRO-Stage 2：梯度分片（Gradient Sharding）</h3><p><strong>原理</strong>：<br>在 Stage 1 的基础上，进一步将梯度分片存储。</p><p><strong>具体做法</strong>：</p><ul><li>每个 GPU 只计算和存储部分梯度</li><li>在反向传播结束时，通过 All-Reduce 操作收集完整的梯度</li><li>然后每个 GPU 只更新自己负责的参数部分</li></ul><p><strong>内存节省</strong>：</p><ul><li>梯度内存减少 $N$ 倍</li><li>与 Stage 1 结合，总内存节省更加显著</li></ul><h3 id="ZeRO-Stage-3：参数分片（Parameter-Sharding）"><a href="#ZeRO-Stage-3：参数分片（Parameter-Sharding）" class="headerlink" title="ZeRO-Stage 3：参数分片（Parameter Sharding）"></a>ZeRO-Stage 3：参数分片（Parameter Sharding）</h3><p><strong>原理</strong>：<br>在 Stage 1 和 Stage 2 的基础上，进一步将模型参数分片存储。</p><p><strong>具体做法</strong>：</p><ul><li>模型参数也被分片存储在不同的 GPU 上</li><li>在训练过程中，当需要某个层的所有参数时，通过 All-Gather 操作将所需参数动态地收集到当前 GPU</li><li>这意味着在任何给定时间点，每个 GPU 上只完整存在模型参数的一部分</li></ul><p><strong>内存节省</strong>：</p><ul><li>模型参数内存减少 $N$ 倍</li><li>实现了最大程度的内存优化</li></ul><h2 id="ZeRO-的具体实现"><a href="#ZeRO-的具体实现" class="headerlink" title="ZeRO 的具体实现"></a>ZeRO 的具体实现</h2><h3 id="通信模式"><a href="#通信模式" class="headerlink" title="通信模式"></a>通信模式</h3><p>ZeRO 使用了两种主要的通信模式：</p><ol><li><p><strong>All-Gather</strong>：用于参数收集</p><ul><li>当需要某个层的完整参数时，从所有 GPU 收集该层的参数分片</li><li>通信开销：$O(P)$，其中 $P$ 是参数数量</li></ul></li><li><p><strong>All-Reduce</strong>：用于梯度聚合</p><ul><li>在反向传播后，聚合所有 GPU 上的梯度分片</li><li>通信开销：$O(P)$</li></ul></li></ol><h3 id="内存管理策略"><a href="#内存管理策略" class="headerlink" title="内存管理策略"></a>内存管理策略</h3><p><strong>按需加载机制</strong>：</p><ul><li>参数只在需要时才加载到 GPU 显存</li><li>使用完毕后立即释放，避免长期占用显存</li></ul><p><strong>分片存储策略</strong>：</p><ul><li>优化器状态：静态分片，训练过程中保持不变</li><li>梯度：动态分片，每次反向传播后重新分配</li><li>参数：动态分片，根据计算需求动态加载</li></ul><h3 id="计算流程"><a href="#计算流程" class="headerlink" title="计算流程"></a>计算流程</h3><p><strong>前向传播</strong>：</p><ol><li>通过 All-Gather 收集当前层需要的参数</li><li>执行前向计算</li><li>释放不需要的参数</li></ol><p><strong>反向传播</strong>：</p><ol><li>通过 All-Gather 收集当前层需要的参数</li><li>计算梯度</li><li>将梯度分片存储</li><li>释放参数</li></ol><p><strong>参数更新</strong>：</p><ol><li>通过 All-Reduce 聚合所有梯度分片</li><li>每个 GPU 更新自己负责的参数部分</li><li>更新对应的优化器状态</li></ol><h2 id="ZeRO-的变体技术"><a href="#ZeRO-的变体技术" class="headerlink" title="ZeRO 的变体技术"></a>ZeRO 的变体技术</h2><h3 id="ZeRO-Offload"><a href="#ZeRO-Offload" class="headerlink" title="ZeRO-Offload"></a>ZeRO-Offload</h3><p><strong>原理</strong>：<br>对于模型训练中一些对性能不那么敏感，但内存占用大的部分（如优化器状态、甚至梯度），将其从 GPU 显存转移到 CPU 内存或硬盘（NVMe SSD）。</p><p><strong>具体做法</strong>：</p><ul><li>优化器状态存储在 CPU 内存中</li><li>梯度可以存储在 CPU 内存或 NVMe SSD 中</li><li>在需要时通过 PCIe 总线传输数据</li></ul><p><strong>优势</strong>：</p><ul><li>进一步减少 GPU 显存需求</li><li>能够训练更大的模型</li><li>成本相对较低</li></ul><p><strong>劣势</strong>：</p><ul><li>增加了 CPU-GPU 数据传输开销</li><li>训练速度可能有所下降</li></ul><h3 id="ZeRO-FSDP（Fully-Sharded-Data-Parallelism）"><a href="#ZeRO-FSDP（Fully-Sharded-Data-Parallelism）" class="headerlink" title="ZeRO-FSDP（Fully Sharded Data Parallelism）"></a>ZeRO-FSDP（Fully Sharded Data Parallelism）</h3><p><strong>原理</strong>：<br>ZeRO-FSDP 是 ZeRO-Stage 3 的完整实现，实现了优化器状态、梯度和模型参数的全面分片。</p><p><strong>特点</strong>：</p><ul><li>最大程度的内存优化</li><li>支持任意大小的模型训练</li><li>通信开销相对较高</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在大语言模型（LLM）训练中，显存不足是一个普遍存在的问题。随着模型规模的不断增长，单个 GPU 的显存容量成为了训练大规模模型的主要瓶颈。DeepSpeed ZeRO（Zero Redundancy Optimizer）技术通过创新的数据分片策略，有效解决了这一问题，使得我们能够训练远超单卡显存上限的超大规模模型。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="training" scheme="https://murphypei.github.io/tags/training/"/>
    
      <category term="zero" scheme="https://murphypei.github.io/tags/zero/"/>
    
      <category term="deepspeed" scheme="https://murphypei.github.io/tags/deepspeed/"/>
    
      <category term="memory" scheme="https://murphypei.github.io/tags/memory/"/>
    
  </entry>
  
  <entry>
    <title>LLM 训练：GRPO 算法详解</title>
    <link href="https://murphypei.github.io//blog/2025/07/llm-grpo.html"/>
    <id>https://murphypei.github.io//blog/2025/07/llm-grpo.html</id>
    <published>2025-07-22T17:43:43.000Z</published>
    <updated>2025-07-23T12:04:59.989Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇博客中，我们详细介绍了 PPO 和 DPO 算法。今天我们来深入探讨 GRPO（Group Relative Policy Optimization）算法，这是 PPO 的一个重要改进版本。GRPO 的核心创新在于改进了优势函数的计算方式，使得训练更加稳定和高效。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在 RLHF（Reinforcement Learning from Human Feedback）训练中，PPO 算法虽然表现良好，但在优势函数计算方面存在一些局限性。GRPO 算法通过引入分组相对优势计算，解决了 PPO 中的一些问题，特别是在处理长序列生成任务时表现更加稳定。</p><h2 id="PPO-计算过程详解"><a href="#PPO-计算过程详解" class="headerlink" title="PPO 计算过程详解"></a>PPO 计算过程详解</h2><p>在深入 GRPO 之前，我们先详细回顾 PPO 的计算过程，特别是优势函数的计算，这是理解 GRPO 改进的关键。</p><h3 id="PPO-的两阶段训练过程"><a href="#PPO-的两阶段训练过程" class="headerlink" title="PPO 的两阶段训练过程"></a>PPO 的两阶段训练过程</h3><p>PPO 的训练过程分为两个阶段：<strong>Rollout 阶段</strong>（经验收集）和<strong>Optimization 阶段</strong>（参数优化）。</p><h4 id="阶段一：Rollout（经验收集-前向传播阶段）"><a href="#阶段一：Rollout（经验收集-前向传播阶段）" class="headerlink" title="阶段一：Rollout（经验收集/前向传播阶段）"></a>阶段一：Rollout（经验收集/前向传播阶段）</h4><p>这个阶段的目标是让 Actor 模型与环境交互，生成完整的回答，并收集所有必要的数据。<strong>此阶段只有前向传播，没有反向传播。</strong></p><p>对于<strong>每一个 token</strong>的生成，都会进行以下计算：</p><ol><li><p><strong>Actor（策略模型）计算</strong>：</p><ul><li><strong>输入</strong>：<code>Prompt</code> + 已经生成的 <code>token_1, ..., token_{t-1}</code></li><li><strong>计算</strong>：Actor 模型进行一次前向传播，输出下一个 token 的概率分布（logits）</li><li><strong>动作</strong>：从这个分布中<strong>采样</strong>出一个 <code>token_t</code></li><li><strong>记录</strong>：保存此时的 <code>log_probs</code>（选择 <code>token_t</code> 的对数概率）和 Actor 的内部状态</li></ul></li><li><p><strong>Critic（价值模型）计算</strong>：</p><ul><li><strong>输入</strong>：与 Actor 相同的输入，即 <code>Prompt</code> + <code>token_1, ..., token_{t-1}</code></li><li><strong>计算</strong>：Critic 模型也进行一次前向传播</li><li><strong>输出</strong>：得到一个<strong>标量值 <code>V_t</code></strong>，这个值是 Critic 对当前状态未来能获得的总奖励的<strong>预测</strong></li><li><strong>记录</strong>：保存这个价值 <code>V_t</code></li></ul></li></ol><p>这个过程会循环往复，直到生成一个完整的回答（例如，遇到 <code>[EOS]</code> 标记或达到最大长度）。</p><p><strong>阶段一结束后，我们得到了一整条”轨迹”（Trajectory），包含以下信息：</strong></p><ul><li>完整的生成序列（<code>token_1, ..., token_n</code>）</li><li>每一步的对数概率（<code>log_probs_1, ..., log_probs_n</code>）</li><li>每一步的价值预测（<code>V_1, ..., V_n</code>）</li></ul><h4 id="阶段二：Optimization（优化-反向传播阶段）"><a href="#阶段二：Optimization（优化-反向传播阶段）" class="headerlink" title="阶段二：Optimization（优化/反向传播阶段）"></a>阶段二：Optimization（优化/反向传播阶段）</h4><p>当收集到一个或一个批次（Batch）的完整”轨迹”后，真正的计算和更新才开始。</p><ol><li><p><strong>计算最终奖励（Reward）</strong>：</p><ul><li>将完整的”<code>Prompt</code> + <code>回答</code>“序列输入到<strong>奖励模型（Reward Model）</strong>中，得到一个<strong>唯一的、总的奖励分数 <code>R</code></strong></li><li>这个 <code>R</code> 是对整个回答的评价</li></ul></li><li><p><strong>计算优势函数（Advantage）</strong>：</p><ul><li>这是最关键的一步。我们不能简单地把总奖励 <code>R</code> 归功于最后一个 token</li><li>我们需要为<strong>每一个 token</strong>的生成行为分配合理的”功劳”或”过失”</li><li>这通常使用<strong>通用优势估计（Generalized Advantage Estimation, GAE）</strong>技术来完成</li></ul></li></ol><h3 id="优势函数的详细计算"><a href="#优势函数的详细计算" class="headerlink" title="优势函数的详细计算"></a>优势函数的详细计算</h3><h4 id="GAE（通用优势估计）算法"><a href="#GAE（通用优势估计）算法" class="headerlink" title="GAE（通用优势估计）算法"></a>GAE（通用优势估计）算法</h4><p>GAE 是 PPO 中计算优势函数的核心技术。对于序列中的每个位置 <code>t</code>，优势函数定义为：</p><script type="math/tex; mode=display">A_t = \delta_t + \gamma \lambda \delta_{t+1} + \gamma^2 \lambda^2 \delta_{t+2} + ... + \gamma^{T-t} \lambda^{T-t} \delta_T</script><p>其中：</p><ul><li>$\delta_t = r_t + \gamma V_{t+1} - V_t$ 是时序差分误差</li><li>$\gamma$ 是折扣因子（通常设为 1）</li><li>$\lambda$ 是 GAE 参数（通常设为 0.95）</li><li>$T$ 是序列长度</li></ul><p><strong>GAE 的直观理解</strong>：</p><ul><li>如果 $\lambda = 0$，则 $A_t = \delta_t$，只考虑一步的时序差分</li><li>如果 $\lambda = 1$，则 $A_t = \sum_{k=t}^T \gamma^{k-t} r_k - V_t$，考虑所有未来奖励</li><li>$\lambda$ 在 0 和 1 之间，平衡了偏差和方差</li></ul><h4 id="优势函数的物理意义"><a href="#优势函数的物理意义" class="headerlink" title="优势函数的物理意义"></a>优势函数的物理意义</h4><p>优势函数 $A_t$ 的直观含义是：</p><ul><li>在 <code>t</code> 时刻选择 <code>token_t</code> 这个动作，比 Critic 模型平均预测的要好多少</li><li>如果 <code>A_t &gt; 0</code>，说明这是个”惊喜”的好动作</li><li>如果 <code>A_t &lt; 0</code>，说明这是个”糟糕”的动作</li></ul><h3 id="PPO-损失函数计算"><a href="#PPO-损失函数计算" class="headerlink" title="PPO 损失函数计算"></a>PPO 损失函数计算</h3><p>现在我们有了每一步的 <code>log_probs_t</code>、<code>V_t</code> 和 <code>A_t</code>，可以计算整个序列的总损失：</p><ol><li><p><strong>Actor Loss（策略损失）</strong>：</p><ul><li>目标：最大化优势值为正的动作的概率，同时最小化优势值为负的动作的概率</li><li>公式：$L^{CLIP}(\theta) = \mathbb{E}_t [\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]$</li><li>其中 $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 是概率比率</li></ul></li><li><p><strong>Critic Loss（价值损失）</strong>：</p><ul><li>目标：让 Critic 的预测越来越准确</li><li>计算 Critic 的预测值 <code>V_t</code> 和通过 GAE 计算出的”真实”回报之间的均方误差</li><li>公式：$L^{VF} = \mathbb{E}_t [(V_t - V_{target})^2]$</li></ul></li><li><p><strong>总损失</strong>：</p><ul><li>$L_{PPO} = L^{CLIP} - \alpha L^{KL} + \beta L^{VF}$</li><li>其中 $\alpha$ 和 $\beta$ 是权重参数</li></ul></li></ol><h2 id="GRPO-算法详解"><a href="#GRPO-算法详解" class="headerlink" title="GRPO 算法详解"></a>GRPO 算法详解</h2><h3 id="2-1-GRPO-算法背景与动机"><a href="#2-1-GRPO-算法背景与动机" class="headerlink" title="2.1 GRPO 算法背景与动机"></a>2.1 GRPO 算法背景与动机</h3><p>在大语言模型（LLM）的微调过程中，强化学习（RL）扮演着至关重要的角色。传统的近端策略优化（PPO）算法虽然被广泛应用于LLM的微调，但其在处理大规模模型时面临着巨大的计算和存储负担。</p><p><strong>PPO 的主要问题</strong>：</p><ul><li><strong>计算负担重</strong>：PPO 需要维护一个与策略模型大小相当的价值网络来估计优势函数，这在大模型场景下会导致显著的内存占用和计算代价</li><li><strong>训练不稳定</strong>：PPO 算法在更新策略时可能会导致策略分布发生剧烈变化，从而影响训练的稳定性</li><li><strong>扩展性差</strong>：在数十亿甚至千亿参数的语言模型上应用 PPO 时，价值网络的训练和更新会消耗大量的计算资源</li></ul><p>为了解决这些问题，<strong>DeepSeek 提出了一种新的强化学习算法——组相对策略优化（GRPO），旨在减少对价值网络的依赖，同时保持策略更新的稳定性和高效性。</strong></p><h3 id="2-2-GRPO-核心思想"><a href="#2-2-GRPO-核心思想" class="headerlink" title="2.2 GRPO 核心思想"></a>2.2 GRPO 核心思想</h3><p><strong>GRPO 的核心思想是通过组内相对奖励来优化策略模型，而不是依赖传统的批评模型（critic model）</strong>。具体来说，GRPO 会在每个状态下采样一组动作，然后根据这些动作的相对表现来调整策略，而不是依赖一个单独的价值网络来估计每个动作的价值。</p><p><strong>GRPO 的核心优势</strong>：</p><ol><li><strong>减少计算负担</strong>：通过避免维护一个与策略模型大小相当的价值网络，GRPO 显著降低了训练过程中的内存占用和计算代价</li><li><strong>提高训练稳定性</strong>：GRPO 通过组内比较来估计优势函数，减少了策略更新的方差，从而确保了更稳定的学习过程</li><li><strong>增强策略更新的可控性</strong>：GRPO 引入了 KL 散度约束，防止策略更新过于剧烈，从而保持了策略分布的稳定性</li></ol><h3 id="2-3-GRPO-算法流程"><a href="#2-3-GRPO-算法流程" class="headerlink" title="2.3 GRPO 算法流程"></a>2.3 GRPO 算法流程</h3><p>GRPO 算法的流程可以分为以下几个关键步骤：</p><h4 id="步骤一：采样动作组"><a href="#步骤一：采样动作组" class="headerlink" title="步骤一：采样动作组"></a>步骤一：采样动作组</h4><p>对于每个输入状态 $s$，GRPO 从当前策略 $\pi_\theta$ 中采样一组动作 $a_1, a_2, …, a_G$。这些动作的采样基于策略模型的概率分布，确保了多样性。</p><h4 id="步骤二：奖励评估"><a href="#步骤二：奖励评估" class="headerlink" title="步骤二：奖励评估"></a>步骤二：奖励评估</h4><p>每个采样动作 $a_i$ 都会通过奖励函数 $R$ 进行评估，得到对应的奖励值 $r_i$。奖励函数可以根据具体任务设计，例如在数学推理任务中，奖励函数可以基于答案的正确性。</p><h4 id="步骤三：计算相对优势"><a href="#步骤三：计算相对优势" class="headerlink" title="步骤三：计算相对优势"></a>步骤三：计算相对优势</h4><p>将每个动作的奖励值进行归一化处理，得到相对优势 $\tilde{A}_i$。具体来说，相对优势可以通过以下公式计算：</p><script type="math/tex; mode=display">\tilde{A}_i = \frac{r_i - \mu_r}{\sigma_r}</script><p>其中，$\mu_r$ 和 $\sigma_r$ 分别是奖励值的均值和标准差。</p><h4 id="步骤四：策略更新"><a href="#步骤四：策略更新" class="headerlink" title="步骤四：策略更新"></a>步骤四：策略更新</h4><p>根据计算得到的相对优势 $\tilde{A}_i$，更新策略模型参数。GRPO 的目标函数可以表示为：</p><script type="math/tex; mode=display">L_{GRPO}(\theta) = \mathbb{E}_{s,a_1,...,a_G} \left[ \frac{1}{G} \sum_{i=1}^{G} \min \left( r_i(\theta) \tilde{A}_i, \text{clip}(r_i(\theta), 1-\epsilon, 1+\epsilon) \tilde{A}_i \right) \right]</script><p>其中：</p><ul><li>$G$ 是采样动作的组大小</li><li>$r_i(\theta) = \frac{\pi_\theta(a_i|s)}{\pi_{\theta_{old}}(a_i|s)}$ 是概率比率</li><li>$\epsilon$ 是裁剪参数（通常设为 0.2）</li></ul><h3 id="2-4-GRPO-的数学原理"><a href="#2-4-GRPO-的数学原理" class="headerlink" title="2.4 GRPO 的数学原理"></a>2.4 GRPO 的数学原理</h3><p>从数学角度来看，GRPO 的目标是最大化预期累积奖励，同时保持策略更新的稳定性。其目标函数可以表示为：</p><script type="math/tex; mode=display">L_{GRPO} = \mathbb{E}_{s,a_1,...,a_G} \left[ \frac{1}{G} \sum_{i=1}^{G} \log \pi_\theta(a_i|s) \tilde{A}_i \right] - \alpha D_{KL}(\pi_\theta || \pi_{\theta_{old}})</script><p>其中：</p><ul><li><strong>第一项</strong>：策略梯度项，通过相对优势来指导策略更新</li><li><strong>第二项</strong>：KL 散度正则化项，防止策略更新过于剧烈</li><li>$\alpha$ 是正则化权重参数</li></ul><p><strong>相对优势的物理意义</strong>：</p><ul><li>$\tilde{A}_i &gt; 0$：表示动作 $a_i$ 在组内表现较好，应该增加其概率</li><li>$\tilde{A}_i &lt; 0$：表示动作 $a_i$ 在组内表现较差，应该减少其概率</li><li>$\tilde{A}_i = 0$：表示动作 $a_i$ 在组内表现平均，不需要调整</li></ul><h3 id="2-5-GRPO-vs-PPO-的对比"><a href="#2-5-GRPO-vs-PPO-的对比" class="headerlink" title="2.5 GRPO vs PPO 的对比"></a>2.5 GRPO vs PPO 的对比</h3><div class="table-container"><table><thead><tr><th>特性</th><th>PPO</th><th>GRPO</th></tr></thead><tbody><tr><td><strong>价值网络</strong></td><td>需要维护与策略模型大小相当的价值网络</td><td>不需要价值网络，减少计算负担</td></tr><tr><td><strong>优势计算</strong></td><td>基于 GAE 和时序差分误差</td><td>基于组内相对奖励比较</td></tr><tr><td><strong>训练稳定性</strong></td><td>可能因价值网络不准确而不稳定</td><td>通过组内比较提高稳定性</td></tr><tr><td><strong>计算效率</strong></td><td>需要训练两个网络（Actor + Critic）</td><td>只需要训练策略网络</td></tr><tr><td><strong>内存占用</strong></td><td>高（需要存储价值网络）</td><td>低（只需要存储策略网络）</td></tr><tr><td><strong>适用场景</strong></td><td>通用强化学习任务</td><td>特别适合大语言模型微调</td></tr></tbody></table></div><h3 id="2-6-GRPO-的实现细节"><a href="#2-6-GRPO-的实现细节" class="headerlink" title="2.6 GRPO 的实现细节"></a>2.6 GRPO 的实现细节</h3><h4 id="2-6-1-组大小选择"><a href="#2-6-1-组大小选择" class="headerlink" title="2.6.1 组大小选择"></a>2.6.1 组大小选择</h4><p>组大小 $G$ 是 GRPO 算法中的一个重要超参数：</p><ul><li><strong>较小的组</strong>（如 $G=4$）：计算效率高，但相对优势估计可能不够准确</li><li><strong>较大的组</strong>（如 $G=16$）：相对优势估计更准确，但计算成本更高</li><li><strong>推荐值</strong>：通常在 8-16 之间，根据具体任务和计算资源调整</li></ul><h4 id="2-6-2-奖励归一化"><a href="#2-6-2-奖励归一化" class="headerlink" title="2.6.2 奖励归一化"></a>2.6.2 奖励归一化</h4><p>为了确保相对优势计算的稳定性，GRPO 使用以下归一化策略：</p><script type="math/tex; mode=display">\tilde{A}_i = \frac{r_i - \mu_r}{\sigma_r + \epsilon}</script><p>其中 $\epsilon$ 是一个小的常数（如 $10^{-8}$），防止除零错误。</p><h4 id="2-6-3-KL-散度约束"><a href="#2-6-3-KL-散度约束" class="headerlink" title="2.6.3 KL 散度约束"></a>2.6.3 KL 散度约束</h4><p>为了防止策略更新过于剧烈，GRPO 引入了 KL 散度约束：</p><script type="math/tex; mode=display">D_{KL}(\pi_\theta || \pi_{\theta_{old}}) \leq \delta</script><p>其中 $\delta$ 是 KL 散度的目标值（通常设为 0.01）。</p><h3 id="2-7-GRPO-在数学推理任务中的表现"><a href="#2-7-GRPO-在数学推理任务中的表现" class="headerlink" title="2.7 GRPO 在数学推理任务中的表现"></a>2.7 GRPO 在数学推理任务中的表现</h3><p>根据 DeepSeek 的研究，GRPO 在数学推理任务中表现出了显著的优势：</p><ol><li><strong>GSM8K 数据集</strong>：GRPO 相比 PPO 在准确率上有明显提升</li><li><strong>MATH 数据集</strong>：在复杂数学问题上，GRPO 的推理能力更强</li><li><strong>训练稳定性</strong>：GRPO 的训练过程更加稳定，收敛速度更快</li><li><strong>计算效率</strong>：在相同硬件条件下，GRPO 的训练时间更短</li></ol><h3 id="2-8-GRPO-的局限性"><a href="#2-8-GRPO-的局限性" class="headerlink" title="2.8 GRPO 的局限性"></a>2.8 GRPO 的局限性</h3><p>尽管 GRPO 有很多优势，但也存在一些局限性：</p><ol><li><strong>组内比较的局限性</strong>：相对优势的计算依赖于组内其他动作的质量，如果组内动作质量都很差，相对优势可能不够准确</li><li><strong>超参数敏感性</strong>：组大小、KL 散度约束等超参数需要仔细调优</li><li><strong>任务依赖性</strong>：GRPO 的效果可能因具体任务而异，需要根据任务特点进行调整</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>GRPO 算法通过引入分组相对优势计算，成功解决了 PPO 在大语言模型微调中的计算负担和稳定性问题。其核心创新在于：</p><ol><li><strong>消除价值网络依赖</strong>：通过组内相对比较替代传统的价值网络估计</li><li><strong>提高训练稳定性</strong>：通过相对优势和 KL 散度约束确保策略更新的稳定性</li><li><strong>降低计算成本</strong>：减少了一半的网络参数和计算量</li></ol><p>GRPO 算法为大规模语言模型的强化学习微调提供了一个更加高效和稳定的解决方案，特别是在数学推理和代码生成等任务中表现出了显著的优势。随着大语言模型规模的不断增长，GRPO 这类轻量级强化学习算法的重要性将越来越突出。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇博客中，我们详细介绍了 PPO 和 DPO 算法。今天我们来深入探讨 GRPO（Group Relative Policy Optimization）算法，这是 PPO 的一个重要改进版本。GRPO 的核心创新在于改进了优势函数的计算方式，使得训练更加稳定和高效。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="rlhf" scheme="https://murphypei.github.io/tags/rlhf/"/>
    
      <category term="ppo" scheme="https://murphypei.github.io/tags/ppo/"/>
    
      <category term="grpo" scheme="https://murphypei.github.io/tags/grpo/"/>
    
      <category term="advantage" scheme="https://murphypei.github.io/tags/advantage/"/>
    
  </entry>
  
  <entry>
    <title>LLM 推理： KV Cache 原理与优化</title>
    <link href="https://murphypei.github.io//blog/2025/07/kv-cache.html"/>
    <id>https://murphypei.github.io//blog/2025/07/kv-cache.html</id>
    <published>2025-07-01T03:56:52.000Z</published>
    <updated>2025-07-21T11:44:22.914Z</updated>
    
    <content type="html"><![CDATA[<p>继续梳理 LLM 知识，这次写 KV Cache。KV Cache 是大语言模型推理过程中的重要优化技术，能够显著减少计算量，提高推理速度。本文将从 Attention 计算原理出发，详细推导 KV Cache 的数学等价性，并分析其优化效果。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在大语言模型的推理过程中，生成式推理（Generative Inference）是一个自回归过程，模型需要逐个生成token。在这个过程中，大量的计算被重复执行，特别是Attention机制中的Key和Value矩阵计算。KV Cache技术通过缓存这些中间结果，避免了重复计算，从而显著提高了推理效率。</p><p>本文将详细介绍KV Cache的工作原理，从Attention计算的数学原理出发，推导其等价性，并分析其在实际应用中的优化效果。</p><h2 id="Attention机制回顾"><a href="#Attention机制回顾" class="headerlink" title="Attention机制回顾"></a>Attention机制回顾</h2><h3 id="标准Attention计算"><a href="#标准Attention计算" class="headerlink" title="标准Attention计算"></a>标准Attention计算</h3><p>在Transformer的Attention机制中，对于输入序列 $X = [x_1, x_2, …, x_n]$，Attention的计算过程如下：</p><p><strong>1. 线性变换</strong></p><script type="math/tex; mode=display">Q = XW_Q, \quad K = XW_K, \quad V = XW_V</script><p>其中：</p><ul><li>$W_Q, W_K, W_V$ 是查询、键、值的权重矩阵</li><li>$Q, K, V$ 分别是查询、键、值的矩阵表示</li></ul><p><strong>2. Attention计算</strong></p><script type="math/tex; mode=display">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</script><p>其中 $d_k$ 是键向量的维度。</p><p><strong>3. 分步展开</strong><br>对于第 $i$ 个位置的输出，可以表示为：</p><script type="math/tex; mode=display">O_i = \sum_{j=1}^{n} \alpha_{ij} v_j</script><p>其中：</p><script type="math/tex; mode=display">\alpha_{ij} = \frac{\exp\left(\frac{q_i^T k_j}{\sqrt{d_k}}\right)}{\sum_{l=1}^{n} \exp\left(\frac{q_i^T k_l}{\sqrt{d_k}}\right)}</script><h3 id="自回归生成过程"><a href="#自回归生成过程" class="headerlink" title="自回归生成过程"></a>自回归生成过程</h3><p>在生成式推理中，模型逐个生成token。假设当前已经生成了 $t$ 个token，要生成第 $t+1$ 个token：</p><p><strong>输入序列</strong>：$X_{1:t} = [x_1, x_2, …, x_t]$</p><p><strong>计算过程</strong>：</p><ol><li>计算 $Q_{1:t}, K_{1:t}, V_{1:t}$</li><li>计算Attention输出</li><li>生成下一个token $x_{t+1}$</li><li>重复上述过程</li></ol><p><strong>问题</strong>：每次生成新token时，都需要重新计算整个序列的 $K$ 和 $V$ 矩阵，这导致了大量的重复计算。</p><h2 id="KV-Cache的核心思想"><a href="#KV-Cache的核心思想" class="headerlink" title="KV Cache的核心思想"></a>KV Cache的核心思想</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>KV Cache的核心思想是：<strong>缓存已经计算过的Key和Value矩阵，避免重复计算</strong>。</p><p><strong>缓存内容</strong>：</p><ul><li>$K_{cache} = [K_1, K_2, …, K_t]$：已生成token的Key矩阵</li><li>$V_{cache} = [V_1, V_2, …, V_t]$：已生成token的Value矩阵</li></ul><p><strong>增量更新</strong>：</p><ul><li>生成新token $x_{t+1}$ 时，只计算 $K_{t+1}$ 和 $V_{t+1}$</li><li>将新的Key和Value追加到缓存中</li><li>使用完整的缓存进行Attention计算</li></ul><h3 id="数学等价性推导"><a href="#数学等价性推导" class="headerlink" title="数学等价性推导"></a>数学等价性推导</h3><h4 id="1-标准计算的数学表示"><a href="#1-标准计算的数学表示" class="headerlink" title="1. 标准计算的数学表示"></a>1. 标准计算的数学表示</h4><p>在标准计算中，生成第 $t+1$ 个token时：</p><p><strong>输入</strong>：$X_{1:t+1} = [x_1, x_2, …, x_t, x_{t+1}]$</p><p><strong>计算过程</strong>：</p><script type="math/tex; mode=display">Q_{1:t+1} = X_{1:t+1}W_Q \\K_{1:t+1} = X_{1:t+1}W_K \\V_{1:t+1} = X_{1:t+1}W_V</script><p><strong>Attention输出</strong>：</p><script type="math/tex; mode=display">O_{t+1} = \sum_{j=1}^{t+1} \alpha_{(t+1)j} v_j</script><p>其中：</p><script type="math/tex; mode=display">\alpha_{(t+1)j} = \frac{\exp\left(\frac{q_{t+1}^T k_j}{\sqrt{d_k}}\right)}{\sum_{l=1}^{t+1} \exp\left(\frac{q_{t+1}^T k_l}{\sqrt{d_k}}\right)}</script><h4 id="2-KV-Cache的计算表示"><a href="#2-KV-Cache的计算表示" class="headerlink" title="2. KV Cache的计算表示"></a>2. KV Cache的计算表示</h4><p>在KV Cache中，生成第 $t+1$ 个token时：</p><p><strong>缓存状态</strong>：</p><ul><li>$K_{cache} = [K_1, K_2, …, K_t]$</li><li>$V_{cache} = [V_1, V_2, …, V_t]$</li></ul><p><strong>增量计算</strong>：</p><script type="math/tex; mode=display">q_{t+1} = x_{t+1}W_Q \\k_{t+1} = x_{t+1}W_K \\v_{t+1} = x_{t+1}W_V</script><p><strong>更新缓存</strong>：</p><script type="math/tex; mode=display">K_{cache}^{new} = [K_{cache}, k_{t+1}] = [K_1, K_2, ..., K_t, K_{t+1}] \\V_{cache}^{new} = [V_{cache}, v_{t+1}] = [V_1, V_2, ..., V_t, V_{t+1}]</script><p><strong>Attention计算</strong>：</p><script type="math/tex; mode=display">O_{t+1} = \sum_{j=1}^{t+1} \alpha_{(t+1)j} v_j</script><p>其中：</p><script type="math/tex; mode=display">\alpha_{(t+1)j} = \frac{\exp\left(\frac{q_{t+1}^T k_j}{\sqrt{d_k}}\right)}{\sum_{l=1}^{t+1} \exp\left(\frac{q_{t+1}^T k_l}{\sqrt{d_k}}\right)}</script><blockquote><p>这里注意重点，$O_{t+1}$，只和 $\alpha_{(t+1)j}$ 以及 $v_{i:t+1}$ 有关。而 $\alpha_{(t+1)j}$ 只和 $q_{t+1}$ 以及 $k_{i:t+1}$ 有关，这也是为何需要 KV  缓存，而不需要 Q 缓存的原因。这是 Attention 计算的核心，也是实现 KV cache 的关键。</p></blockquote><h4 id="3-等价性证明"><a href="#3-等价性证明" class="headerlink" title="3. 等价性证明"></a>3. 等价性证明</h4><p><strong>矩阵运算的线性性质</strong>：</p><p>对于线性变换 $K = XW_K$，由于矩阵乘法的线性性质：</p><script type="math/tex; mode=display">K_{1:t+1} = X_{1:t+1}W_K = [X_{1:t}, x_{t+1}]W_K = [X_{1:t}W_K, x_{t+1}W_K] = [K_{1:t}, K_{t+1}]</script><p>同理：</p><script type="math/tex; mode=display">V_{1:t+1} = [V_{1:t}, V_{t+1}]</script><p><strong>Attention计算的等价性</strong>：</p><p>在标准计算中：</p><script type="math/tex; mode=display">\text{Attention}(Q_{1:t+1}, K_{1:t+1}, V_{1:t+1}) = \text{softmax}\left(\frac{Q_{1:t+1}K_{1:t+1}^T}{\sqrt{d_k}}\right)V_{1:t+1}</script><p>在KV Cache中：</p><script type="math/tex; mode=display">\text{Attention}(q_{t+1}, [K_{cache}, k_{t+1}], [V_{cache}, v_{t+1}]) = \text{softmax}\left(\frac{q_{t+1}[K_{cache}, k_{t+1}]^T}{\sqrt{d_k}}\right)[V_{cache}, v_{t+1}]</script><p>由于：</p><ul><li>$[K_{cache}, k_{t+1}] = K_{1:t+1}$</li><li>$[V_{cache}, v_{t+1}] = V_{1:t+1}$</li><li>$q_{t+1}$ 是 $Q_{1:t+1}$ 的最后一行</li></ul><p>因此，两种计算方式在数学上完全等价。</p><h3 id="计算复杂度分析"><a href="#计算复杂度分析" class="headerlink" title="计算复杂度分析"></a>计算复杂度分析</h3><h4 id="1-标准计算复杂度"><a href="#1-标准计算复杂度" class="headerlink" title="1. 标准计算复杂度"></a>1. 标准计算复杂度</h4><p><strong>第 $t+1$ 步的计算量</strong>：</p><ul><li>线性变换：$O((t+1) \times d_{model} \times d_k)$</li><li>Attention计算：$O((t+1)^2 \times d_k)$</li><li>总复杂度：$O((t+1) \times d_{model} \times d_k + (t+1)^2 \times d_k)$</li></ul><p><strong>累积计算量</strong>（生成 $n$ 个token）：</p><script type="math/tex; mode=display">\sum_{t=1}^{n} O(t \times d_{model} \times d_k + t^2 \times d_k) = O(n^2 \times d_{model} \times d_k + n^3 \times d_k)</script><h4 id="2-KV-Cache计算复杂度"><a href="#2-KV-Cache计算复杂度" class="headerlink" title="2. KV Cache计算复杂度"></a>2. KV Cache计算复杂度</h4><p><strong>第 $t+1$ 步的计算量</strong>：</p><ul><li>线性变换：$O(d_{model} \times d_k)$（只计算新token）</li><li>Attention计算：$O((t+1)^2 \times d_k)$</li><li>总复杂度：$O(d_{model} \times d_k + (t+1)^2 \times d_k)$</li></ul><p><strong>累积计算量</strong>（生成 $n$ 个token）：</p><script type="math/tex; mode=display">\sum_{t=1}^{n} O(d_{model} \times d_k + t^2 \times d_k) = O(n \times d_{model} \times d_k + n^3 \times d_k)</script><h4 id="3-优化效果"><a href="#3-优化效果" class="headerlink" title="3. 优化效果"></a>3. 优化效果</h4><p><strong>计算量减少</strong>：</p><ul><li>线性变换部分：从 $O(n^2 \times d_{model} \times d_k)$ 减少到 $O(n \times d_{model} \times d_k)$</li><li>减少比例：$O(n)$ 倍</li></ul><p><strong>实际效果</strong>：</p><ul><li>对于长序列生成，计算量减少显著</li><li>特别是在生成较长文本时，优化效果明显</li></ul><h2 id="KV-Cache的实现细节"><a href="#KV-Cache的实现细节" class="headerlink" title="KV Cache的实现细节"></a>KV Cache的实现细节</h2><h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><h4 id="1-缓存结构"><a href="#1-缓存结构" class="headerlink" title="1. 缓存结构"></a>1. 缓存结构</h4><p><strong>缓存格式</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缓存结构示例</span></span><br><span class="line">kv_cache = &#123;</span><br><span class="line">    <span class="string">'key'</span>: torch.zeros(seq_len, num_layers, num_heads, head_dim),</span><br><span class="line">    <span class="string">'value'</span>: torch.zeros(seq_len, num_layers, num_heads, head_dim)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>内存布局</strong>：</p><ul><li>按层（layer）组织</li><li>每层包含多个注意力头（attention heads）</li><li>支持动态扩展</li></ul><h4 id="2-内存优化策略"><a href="#2-内存优化策略" class="headerlink" title="2. 内存优化策略"></a>2. 内存优化策略</h4><p><strong>预分配策略</strong>：</p><ul><li>根据最大序列长度预分配内存</li><li>避免频繁的内存重新分配</li></ul><p><strong>内存复用</strong>：</p><ul><li>在推理过程中复用缓存空间</li><li>减少内存碎片</li></ul><h3 id="增量更新机制"><a href="#增量更新机制" class="headerlink" title="增量更新机制"></a>增量更新机制</h3><h4 id="1-缓存更新"><a href="#1-缓存更新" class="headerlink" title="1. 缓存更新"></a>1. 缓存更新</h4><p><strong>更新流程</strong>：</p><ol><li>计算新token的Key和Value</li><li>将新的Key和Value追加到缓存</li><li>更新缓存的有效长度</li></ol><p><strong>代码示例</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_kv_cache</span><span class="params">(kv_cache, new_k, new_v, layer_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 追加新的Key和Value到缓存</span></span><br><span class="line">    kv_cache[<span class="string">'key'</span>][layer_idx] = torch.cat([kv_cache[<span class="string">'key'</span>][layer_idx], new_k], dim=<span class="number">0</span>)</span><br><span class="line">    kv_cache[<span class="string">'value'</span>][layer_idx] = torch.cat([kv_cache[<span class="string">'value'</span>][layer_idx], new_v], dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><h4 id="2-注意力计算"><a href="#2-注意力计算" class="headerlink" title="2. 注意力计算"></a>2. 注意力计算</h4><p><strong>使用缓存的Attention计算</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_with_cache</span><span class="params">(query, kv_cache, layer_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 获取缓存的Key和Value</span></span><br><span class="line">    cached_k = kv_cache[<span class="string">'key'</span>][layer_idx]</span><br><span class="line">    cached_v = kv_cache[<span class="string">'value'</span>][layer_idx]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算注意力分数</span></span><br><span class="line">    scores = torch.matmul(query, cached_k.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    attention_weights = torch.softmax(scores, dim=<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算输出</span></span><br><span class="line">    output = torch.matmul(attention_weights, cached_v)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></p><h3 id="多头注意力处理"><a href="#多头注意力处理" class="headerlink" title="多头注意力处理"></a>多头注意力处理</h3><h4 id="1-多头并行计算"><a href="#1-多头并行计算" class="headerlink" title="1. 多头并行计算"></a>1. 多头并行计算</h4><p><strong>缓存组织</strong>：</p><ul><li>每个注意力头独立缓存Key和Value</li><li>支持并行计算</li></ul><p><strong>计算优化</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_head_attention_with_cache</span><span class="params">(query, kv_cache, layer_idx)</span>:</span></span><br><span class="line">    batch_size, num_heads, seq_len, head_dim = query.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 并行计算所有注意力头</span></span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> head_idx <span class="keyword">in</span> range(num_heads):</span><br><span class="line">        head_query = query[:, head_idx, :, :]</span><br><span class="line">        head_k = kv_cache[<span class="string">'key'</span>][layer_idx][:, head_idx, :, :]</span><br><span class="line">        head_v = kv_cache[<span class="string">'value'</span>][layer_idx][:, head_idx, :, :]</span><br><span class="line">        </span><br><span class="line">        head_output = attention_with_cache(head_query, head_k, head_v)</span><br><span class="line">        outputs.append(head_output)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h4 id="2-内存布局优化"><a href="#2-内存布局优化" class="headerlink" title="2. 内存布局优化"></a>2. 内存布局优化</h4><p><strong>连续内存布局</strong>：</p><ul><li>将多头数据存储在连续内存中</li><li>提高缓存命中率</li></ul><p><strong>批处理优化</strong>：</p><ul><li>支持批量处理多个序列</li><li>减少内存访问开销</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;继续梳理 LLM 知识，这次写 KV Cache。KV Cache 是大语言模型推理过程中的重要优化技术，能够显著减少计算量，提高推理速度。本文将从 Attention 计算原理出发，详细推导 KV Cache 的数学等价性，并分析其优化效果。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="KV Cache" scheme="https://murphypei.github.io/tags/KV-Cache/"/>
    
      <category term="Attention" scheme="https://murphypei.github.io/tags/Attention/"/>
    
      <category term="推理优化" scheme="https://murphypei.github.io/tags/%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>LLM：RAG 中的文本检索技术</title>
    <link href="https://murphypei.github.io//blog/2025/06/text-retriveval.html"/>
    <id>https://murphypei.github.io//blog/2025/06/text-retriveval.html</id>
    <published>2025-06-29T19:19:02.000Z</published>
    <updated>2025-07-01T12:41:19.152Z</updated>
    
    <content type="html"><![CDATA[<p>继续准备 LLM 面试知识，这次写文本检索技术。文本检索是 RAG（检索增强生成）系统的核心组件，也是面试中经常被问到的问题。本文将详细介绍稠密向量检索、稀疏向量检索、BM25算法以及混合检索策略，帮助理解现代文本检索系统的技术原理。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在当今的信息检索领域，随着人工智能和自然语言处理技术的发展，文本检索技术已经从传统的基于关键词匹配的方法，发展到了基于深度学习的语义检索方法。文本检索是RAG（Retrieval-Augmented Generation）系统的核心组件，它负责从大规模文档集合中检索出与用户查询最相关的文档片段，为后续的生成模型提供上下文信息。</p><p>本文将详细介绍三种主要的文本检索方法：<strong>稠密向量检索（Dense Retrieval）</strong>、<strong>稀疏向量检索（Sparse Retrieval）</strong>和<strong>BM25算法</strong>，以及它们的混合使用策略。</p><h2 id="稠密向量检索（Dense-Retrieval）"><a href="#稠密向量检索（Dense-Retrieval）" class="headerlink" title="稠密向量检索（Dense Retrieval）"></a>稠密向量检索（Dense Retrieval）</h2><h3 id="稠密向量检索的基本原理"><a href="#稠密向量检索的基本原理" class="headerlink" title="稠密向量检索的基本原理"></a>稠密向量检索的基本原理</h3><p>稠密向量检索是一种基于深度学习的检索方法，它通过将文本转换为高维空间中的连续向量表示，然后使用向量相似度（如余弦相似度、欧氏距离）来检索相关文档。</p><p><strong>核心思想</strong>：</p><ul><li>将查询和文档都映射到同一个高维向量空间</li><li>通过计算向量间的相似度来衡量相关性</li><li>能够捕捉文本的深层语义信息</li></ul><h3 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h3><h4 id="1-文本编码"><a href="#1-文本编码" class="headerlink" title="1. 文本编码"></a>1. 文本编码</h4><p>稠密向量检索通常使用预训练的语言模型（如BERT、T5、Sentence-BERT等）对文本进行编码：</p><script type="math/tex; mode=display">\mathbf{q} = \text{Encoder}(query) \\\mathbf{d} = \text{Encoder}(document)</script><p>其中：</p><ul><li>$\mathbf{q}$ 是查询的向量表示</li><li>$\mathbf{d}$ 是文档的向量表示</li><li>$\text{Encoder}$ 是预训练的语言模型</li></ul><h4 id="2-相似度计算"><a href="#2-相似度计算" class="headerlink" title="2. 相似度计算"></a>2. 相似度计算</h4><p>常用的相似度计算方法包括：</p><p><strong>余弦相似度</strong>：</p><script type="math/tex; mode=display">\text{sim}(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q} \cdot \mathbf{d}}{|\mathbf{q}| \cdot |\mathbf{d}|}</script><p><strong>点积相似度</strong>：</p><script type="math/tex; mode=display">\text{sim}(\mathbf{q}, \mathbf{d}) = \mathbf{q} \cdot \mathbf{d}</script><p><strong>欧氏距离</strong>：</p><script type="math/tex; mode=display">\text{dist}(\mathbf{q}, \mathbf{d}) = \sqrt{\sum_{i=1}^{n} (q_i - d_i)^2}</script><h4 id="3-索引和检索"><a href="#3-索引和检索" class="headerlink" title="3. 索引和检索"></a>3. 索引和检索</h4><p><strong>向量索引</strong>：</p><ul><li>使用FAISS、Annoy、HNSW等向量索引库</li><li>支持高效的近似最近邻搜索</li><li>能够处理百万级别的向量检索</li></ul><p><strong>检索流程</strong>：</p><ol><li>将查询编码为向量</li><li>在向量索引中搜索最相似的文档向量</li><li>返回相似度最高的文档</li></ol><h3 id="稠密向量检索的优势和局限性"><a href="#稠密向量检索的优势和局限性" class="headerlink" title="稠密向量检索的优势和局限性"></a>稠密向量检索的优势和局限性</h3><p><strong>优势</strong>：</p><ol><li><strong>语义理解能力强</strong>：能够理解查询和文档的深层语义</li><li><strong>处理同义词和近义词</strong>：即使词汇不完全匹配，也能找到相关文档</li><li><strong>支持复杂查询</strong>：能够处理自然语言形式的查询</li></ol><p><strong>局限性</strong>：</p><ol><li><strong>计算成本高</strong>：需要深度学习模型进行编码</li><li><strong>索引规模限制</strong>：在大规模数据集上可能遇到性能瓶颈</li><li><strong>对训练数据敏感</strong>：检索效果依赖于编码模型的训练质量</li></ol><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li><strong>智能问答系统</strong>：从知识库中检索相关答案</li><li><strong>推荐系统</strong>：基于内容相似性推荐相关文档</li><li><strong>语义搜索</strong>：理解用户意图的搜索引擎</li></ul><h2 id="稀疏向量检索（Sparse-Retrieval）"><a href="#稀疏向量检索（Sparse-Retrieval）" class="headerlink" title="稀疏向量检索（Sparse Retrieval）"></a>稀疏向量检索（Sparse Retrieval）</h2><h3 id="稀疏向量检索的基本原理"><a href="#稀疏向量检索的基本原理" class="headerlink" title="稀疏向量检索的基本原理"></a>稀疏向量检索的基本原理</h3><p>稀疏向量检索是基于传统信息检索模型的方法，它使用词袋模型（Bag of Words）将文本表示为稀疏向量，并通过计算词频-逆文档频率（TF-IDF）来评估文档与查询的相关性。</p><p><strong>核心思想</strong>：</p><ul><li>将文本表示为高维稀疏向量</li><li>每个维度对应词汇表中的一个词</li><li>通过统计方法计算词的重要性</li></ul><h3 id="技术实现-1"><a href="#技术实现-1" class="headerlink" title="技术实现"></a>技术实现</h3><h4 id="1-TF-IDF计算"><a href="#1-TF-IDF计算" class="headerlink" title="1. TF-IDF计算"></a>1. TF-IDF计算</h4><p><strong>词频（Term Frequency, TF）</strong>：</p><script type="math/tex; mode=display">\text{TF}(t, d) = \frac{\text{词 } t \text{ 在文档 } d \text{ 中的出现次数}}{\text{文档 } d \text{ 的总词数}}</script><p><strong>逆文档频率（Inverse Document Frequency, IDF）</strong>：</p><script type="math/tex; mode=display">\text{IDF}(t) = \log \frac{\text{总文档数}}{\text{包含词 } t \text{ 的文档数}}</script><p><strong>TF-IDF权重</strong>：</p><script type="math/tex; mode=display">\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)</script><h4 id="2-向量构建"><a href="#2-向量构建" class="headerlink" title="2. 向量构建"></a>2. 向量构建</h4><p>文档向量 $\mathbf{d}$ 的构建：</p><script type="math/tex; mode=display">\mathbf{d} = [\text{TF-IDF}(t_1, d), \text{TF-IDF}(t_2, d), ..., \text{TF-IDF}(t_n, d)]</script><p>其中 $t_1, t_2, …, t_n$ 是词汇表中的所有词。</p><h4 id="3-相似度计算"><a href="#3-相似度计算" class="headerlink" title="3. 相似度计算"></a>3. 相似度计算</h4><p><strong>余弦相似度</strong>：</p><script type="math/tex; mode=display">\text{sim}(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q} \cdot \mathbf{d}}{|\mathbf{q}| \cdot |\mathbf{d}|}</script><p><strong>点积相似度</strong>：</p><script type="math/tex; mode=display">\text{sim}(\mathbf{q}, \mathbf{d}) = \mathbf{q} \cdot \mathbf{d}</script><h3 id="稀疏向量检索的优势和局限性"><a href="#稀疏向量检索的优势和局限性" class="headerlink" title="稀疏向量检索的优势和局限性"></a>稀疏向量检索的优势和局限性</h3><p><strong>优势</strong>：</p><ol><li><strong>计算效率高</strong>：基于统计方法，计算速度快</li><li><strong>可解释性强</strong>：能够明确知道哪些词贡献了相关性</li><li><strong>处理大规模数据</strong>：能够高效处理大规模文档集合</li></ol><p><strong>局限性</strong>：</p><ol><li><strong>语义理解能力弱</strong>：无法处理同义词和近义词</li><li><strong>词汇匹配限制</strong>：需要查询词在文档中实际出现</li><li><strong>无法处理语义相似性</strong>：无法理解词汇的深层含义</li></ol><h3 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li><strong>传统搜索引擎</strong>：基于关键词的网页搜索</li><li><strong>文档检索系统</strong>：从文档库中检索相关文档</li><li><strong>信息过滤</strong>：基于关键词的信息过滤</li></ul><h2 id="BM25算法"><a href="#BM25算法" class="headerlink" title="BM25算法"></a>BM25算法</h2><h3 id="BM25算法的基本原理"><a href="#BM25算法的基本原理" class="headerlink" title="BM25算法的基本原理"></a>BM25算法的基本原理</h3><p>BM25（Best Matching 25）是一种经典的信息检索算法，它是TF-IDF算法的改进版，通过引入词频（TF）和文档频率（DF）的函数来计算文档与查询的相关性得分。</p><p><strong>核心思想</strong>：</p><ul><li>在TF-IDF基础上引入文档长度归一化</li><li>使用词频饱和函数处理高频词</li><li>通过参数调整优化检索效果</li></ul><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><p>BM25算法的核心公式：</p><script type="math/tex; mode=display">\text{BM25}(Q, D) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}</script><p>其中：</p><ul><li>$Q$ 是查询，包含词 $q_1, q_2, …, q_n$</li><li>$D$ 是文档</li><li>$f(q_i, D)$ 是词 $q_i$ 在文档 $D$ 中的词频</li><li>$|D|$ 是文档 $D$ 的长度</li><li>$\text{avgdl}$ 是文档集合的平均长度</li><li>$k_1$ 和 $b$ 是调节参数</li></ul><p><strong>IDF计算</strong>：</p><script type="math/tex; mode=display">\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}</script><p>其中：</p><ul><li>$N$ 是文档集合的总文档数</li><li>$n(q_i)$ 是包含词 $q_i$ 的文档数</li></ul><h3 id="参数调节"><a href="#参数调节" class="headerlink" title="参数调节"></a>参数调节</h3><p><strong>$k_1$ 参数</strong>：</p><ul><li>控制词频饱和程度</li><li>通常设置为 1.2-2.0</li><li>值越大，词频的影响越线性</li></ul><p><strong>$b$ 参数</strong>：</p><ul><li>控制文档长度归一化程度</li><li>取值范围为 0-1</li><li>$b=0$ 表示不进行长度归一化</li><li>$b=1$ 表示完全归一化</li></ul><h3 id="BM25算法的优势"><a href="#BM25算法的优势" class="headerlink" title="BM25算法的优势"></a>BM25算法的优势</h3><ol><li><strong>理论基础扎实</strong>：基于概率检索模型</li><li><strong>参数可调节</strong>：能够适应不同的数据集和需求</li><li><strong>计算效率高</strong>：基于统计方法，计算速度快</li><li><strong>效果稳定</strong>：在许多基准测试中表现优异</li></ol><h3 id="应用场景-2"><a href="#应用场景-2" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li><strong>搜索引擎</strong>：Google、Bing等搜索引擎的核心算法</li><li><strong>文档检索</strong>：企业文档管理系统</li><li><strong>学术搜索</strong>：学术论文检索系统</li></ul><h2 id="混合检索策略"><a href="#混合检索策略" class="headerlink" title="混合检索策略"></a>混合检索策略</h2><h3 id="混合检索的基本原理"><a href="#混合检索的基本原理" class="headerlink" title="混合检索的基本原理"></a>混合检索的基本原理</h3><p>混合检索结合了稠密向量检索、稀疏向量检索和BM25算法的优势，通过多路召回和结果融合来提高检索系统的整体性能。</p><p><strong>核心思想</strong>：</p><ul><li>使用多种检索方法并行检索</li><li>通过融合算法合并检索结果</li><li>平衡准确性和召回率</li></ul><h3 id="混合检索的优势"><a href="#混合检索的优势" class="headerlink" title="混合检索的优势"></a>混合检索的优势</h3><ol><li><strong>互补性</strong>：不同方法各有优势，相互补充</li><li><strong>提高准确性</strong>：通过多路召回提高检索准确性</li><li><strong>提升召回率</strong>：增加检索结果的覆盖面</li><li><strong>适应性</strong>：能够适应不同的查询类型和场景</li></ol><h3 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h3><h4 id="1-多路召回"><a href="#1-多路召回" class="headerlink" title="1. 多路召回"></a>1. 多路召回</h4><p><strong>稠密向量检索</strong>：</p><ul><li>使用语义相似性进行检索</li><li>适合处理语义相关的查询</li></ul><p><strong>稀疏向量检索</strong>：</p><ul><li>使用关键词匹配进行检索</li><li>适合处理精确匹配的查询</li></ul><p><strong>BM25检索</strong>：</p><ul><li>使用传统信息检索方法</li><li>适合处理结构化查询</li></ul><h4 id="2-结果融合"><a href="#2-结果融合" class="headerlink" title="2. 结果融合"></a>2. 结果融合</h4><p><strong>RRF（Reciprocal Rank Fusion）</strong>：</p><script type="math/tex; mode=display">\text{RRF}(d) = \sum_{i=1}^{n} \frac{1}{k + \text{rank}_i(d)}</script><p>其中：</p><ul><li>$\text{rank}_i(d)$ 是文档 $d$ 在第 $i$ 个检索方法中的排名</li><li>$k$ 是调节参数，通常设置为 60</li></ul><p><strong>加权融合</strong>：</p><script type="math/tex; mode=display">\text{Score}(d) = \sum_{i=1}^{n} w_i \cdot \text{score}_i(d)</script><p>其中：</p><ul><li>$w_i$ 是第 $i$ 个检索方法的权重</li><li>$\text{score}_i(d)$ 是文档 $d$ 在第 $i$ 个检索方法中的得分</li></ul><h4 id="3-动态权重调整"><a href="#3-动态权重调整" class="headerlink" title="3. 动态权重调整"></a>3. 动态权重调整</h4><p>根据查询类型动态调整不同检索方法的权重：</p><ul><li><strong>语义查询</strong>：增加稠密向量检索的权重</li><li><strong>关键词查询</strong>：增加稀疏向量检索和BM25的权重</li><li><strong>混合查询</strong>：平衡各种方法的权重</li></ul><h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p><strong>查询处理层</strong>：</p><ul><li>查询解析和预处理</li><li>查询类型识别</li><li>参数选择</li></ul><p><strong>检索层</strong>：</p><ul><li>多路并行检索</li><li>结果初步排序</li><li>去重和合并</li></ul><p><strong>融合层</strong>：</p><ul><li>结果融合算法</li><li>最终排序</li><li>结果返回</li></ul><h2 id="性能评估指标"><a href="#性能评估指标" class="headerlink" title="性能评估指标"></a>性能评估指标</h2><h3 id="检索性能指标"><a href="#检索性能指标" class="headerlink" title="检索性能指标"></a>检索性能指标</h3><p><strong>准确率（Precision）</strong>：</p><script type="math/tex; mode=display">\text{Precision} = \frac{\text{相关文档数}}{\text{检索文档数}}</script><p><strong>召回率（Recall）</strong>：</p><script type="math/tex; mode=display">\text{Recall} = \frac{\text{相关文档数}}{\text{总相关文档数}}</script><p><strong>F1分数</strong>：</p><script type="math/tex; mode=display">\text{F1} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}</script><p><strong>平均精度（MAP）</strong>：</p><script type="math/tex; mode=display">\text{MAP} = \frac{1}{|Q|} \sum_{q \in Q} \text{AP}(q)</script><p>其中 $\text{AP}(q)$ 是查询 $q$ 的平均精度。</p><h3 id="效率指标"><a href="#效率指标" class="headerlink" title="效率指标"></a>效率指标</h3><p><strong>检索延迟</strong>：从查询到返回结果的时间<br><strong>吞吐量</strong>：单位时间内处理的查询数<br><strong>索引大小</strong>：索引占用的存储空间</p><h2 id="实际应用中的优化策略"><a href="#实际应用中的优化策略" class="headerlink" title="实际应用中的优化策略"></a>实际应用中的优化策略</h2><h3 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h3><p><strong>倒排索引</strong>：</p><ul><li>为每个词建立文档列表</li><li>支持快速的关键词查找</li><li>优化存储和查询效率</li></ul><p><strong>向量索引</strong>：</p><ul><li>使用HNSW、IVF等算法</li><li>支持高效的近似最近邻搜索</li><li>平衡精度和速度</li></ul><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p><strong>查询扩展</strong>：</p><ul><li>使用同义词扩展查询</li><li>基于用户反馈调整查询</li><li>利用查询日志优化</li></ul><p><strong>查询重写</strong>：</p><ul><li>将自然语言查询转换为结构化查询</li><li>使用查询模板提高效率</li><li>基于历史查询进行优化</li></ul><h3 id="缓存策略"><a href="#缓存策略" class="headerlink" title="缓存策略"></a>缓存策略</h3><p><strong>结果缓存</strong>：</p><ul><li>缓存热门查询的结果</li><li>使用LRU等策略管理缓存</li><li>提高响应速度</li></ul><p><strong>索引缓存</strong>：</p><ul><li>将常用索引加载到内存</li><li>使用分层缓存策略</li><li>优化内存使用</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;继续准备 LLM 面试知识，这次写文本检索技术。文本检索是 RAG（检索增强生成）系统的核心组件，也是面试中经常被问到的问题。本文将详细介绍稠密向量检索、稀疏向量检索、BM25算法以及混合检索策略，帮助理解现代文本检索系统的技术原理。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="RAG" scheme="https://murphypei.github.io/tags/RAG/"/>
    
      <category term="文本检索" scheme="https://murphypei.github.io/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E7%B4%A2/"/>
    
      <category term="向量检索" scheme="https://murphypei.github.io/tags/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/"/>
    
      <category term="BM25" scheme="https://murphypei.github.io/tags/BM25/"/>
    
  </entry>
  
  <entry>
    <title>LLM 幻觉与重复问题</title>
    <link href="https://murphypei.github.io//blog/2025/06/llm-hallucination-repetition.html"/>
    <id>https://murphypei.github.io//blog/2025/06/llm-hallucination-repetition.html</id>
    <published>2025-06-27T00:31:30.000Z</published>
    <updated>2025-07-01T12:40:54.832Z</updated>
    
    <content type="html"><![CDATA[<p>LLM 的幻觉和重复问题是 LLM 应用中的核心挑战，也是面试中经常被问到的问题。本文将从底层机理出发，深入分析这两个问题的成因，并探讨有效的解决方案。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>大语言模型（LLM）在近年来取得了巨大的成功，但同时也面临着两个关键问题：<strong>幻觉（Hallucination）</strong>和<strong>重复（Repetition）</strong>。这些问题不仅影响了模型的实用性，也阻碍了其在关键领域的应用。本文将从底层机理出发，深入分析这两个问题的成因，并探讨有效的解决方案。</p><h2 id="幻觉问题（Hallucination）"><a href="#幻觉问题（Hallucination）" class="headerlink" title="幻觉问题（Hallucination）"></a>幻觉问题（Hallucination）</h2><h3 id="幻觉的基本概念"><a href="#幻觉的基本概念" class="headerlink" title="幻觉的基本概念"></a>幻觉的基本概念</h3><p>幻觉是指LLM生成的内容与事实不符，包括：</p><ul><li><strong>事实性幻觉</strong>：生成错误的事实信息</li><li><strong>逻辑性幻觉</strong>：推理过程存在逻辑错误</li><li><strong>引用性幻觉</strong>：虚构不存在的引用或来源</li></ul><h3 id="幻觉产生的底层机理"><a href="#幻觉产生的底层机理" class="headerlink" title="幻觉产生的底层机理"></a>幻觉产生的底层机理</h3><h4 id="1-训练数据质量问题"><a href="#1-训练数据质量问题" class="headerlink" title="1. 训练数据质量问题"></a>1. 训练数据质量问题</h4><p><strong>数据噪声与错误</strong></p><ul><li>训练数据中本身就包含错误信息</li><li>网络爬取的数据质量参差不齐</li><li>标注错误导致模型学习到错误的知识</li></ul><p><strong>数据分布偏差</strong></p><ul><li>某些领域的数据过于稀少</li><li>时间性信息过时（如2023年之前的数据）</li><li>地域性偏见导致知识覆盖不均</li></ul><h4 id="2-Attention机制的局限性"><a href="#2-Attention机制的局限性" class="headerlink" title="2. Attention机制的局限性"></a>2. Attention机制的局限性</h4><p>根据<a href="https://arxiv.org/html/2504.04600v1" target="_blank" rel="noopener">Attention理论</a>，Attention机制本质上是一个2体相互作用系统：</p><script type="math/tex; mode=display">\mathcal{P}(\mathbf{x}) = \mathbf{N}^{(0)}\mathsf{W}_V\mathbf{x}^{\mathrm{T}}</script><p>其中：</p><ul><li>$\mathbf{N}^{(0)}$ 是上下文向量</li><li>$\mathsf{W}_V$ 是Value投影矩阵</li><li>$\mathbf{x}$ 是词汇表中的token</li></ul><p><strong>Attention机制的固有问题</strong>：</p><ol><li><strong>局部性限制</strong>：Attention主要关注局部相关性，难以捕捉全局一致性</li><li><strong>缺乏事实验证</strong>：模型无法验证生成内容的真实性</li><li><strong>过度依赖训练数据</strong>：当遇到训练数据中未覆盖的情况时，容易产生幻觉</li></ol><p><strong>物理机制解释</strong>：</p><p>从物理学的角度来看，Attention机制类似于一个自旋浴系统：</p><ul><li><strong>自旋状态</strong>：每个token对应一个自旋向量 $\mathbf{S}_i$</li><li><strong>相互作用</strong>：通过2体相互作用计算注意力权重</li><li><strong>相位分离</strong>：在特定条件下，系统会出现”好”与”坏”内容的相位分离</li></ul><p>这种物理机制解释了为什么模型在某些情况下会倾向于生成不准确的内容。</p><h4 id="3-训练目标与事实性不匹配"><a href="#3-训练目标与事实性不匹配" class="headerlink" title="3. 训练目标与事实性不匹配"></a>3. 训练目标与事实性不匹配</h4><p><strong>最大似然估计的局限性</strong></p><ul><li>训练目标是最小化预测下一个token的损失</li><li>这个目标并不直接优化事实准确性</li><li>模型可能为了流畅性而牺牲准确性</li></ul><p><strong>缺乏事实性监督</strong></p><ul><li>训练过程中没有明确的事实性约束</li><li>模型无法区分事实性内容和创造性内容</li></ul><h3 id="缓解和消除幻觉的方法"><a href="#缓解和消除幻觉的方法" class="headerlink" title="缓解和消除幻觉的方法"></a>缓解和消除幻觉的方法</h3><h4 id="1-数据层面的改进"><a href="#1-数据层面的改进" class="headerlink" title="1. 数据层面的改进"></a>1. 数据层面的改进</h4><p><strong>高质量数据收集</strong></p><ul><li>结合多个高质量数据源</li><li>使用事实性强的数据（如维基百科、学术论文）</li><li>建立数据质量评估体系</li></ul><p><strong>数据清洗与验证</strong></p><ul><li>自动检测和移除错误数据</li><li>使用外部知识库验证数据准确性</li><li>建立数据版本控制机制</li></ul><p><strong>知识注入技术</strong></p><ul><li>将结构化知识（如知识图谱）注入训练数据</li><li>使用检索增强生成（RAG）技术</li><li>结合外部知识库进行训练</li></ul><h4 id="2-模型架构的改进"><a href="#2-模型架构的改进" class="headerlink" title="2. 模型架构的改进"></a>2. 模型架构的改进</h4><p><strong>改进的Attention机制</strong></p><ul><li>引入多步推理机制</li><li>使用思维链（Chain-of-Thought）提示</li><li>实现推理过程的显式建模</li></ul><p><strong>事实性约束</strong></p><ul><li>在Attention中加入事实性约束</li><li>使用外部知识库指导注意力分配</li><li>实现事实性验证的端到端训练</li></ul><p><strong>检索增强生成（RAG）</strong></p><ul><li>在生成过程中实时检索相关信息</li><li>使用向量数据库存储知识</li><li>实现检索与生成的联合优化</li></ul><h4 id="3-训练策略的改进"><a href="#3-训练策略的改进" class="headerlink" title="3. 训练策略的改进"></a>3. 训练策略的改进</h4><p><strong>事实性监督</strong></p><ul><li>设计专门的事实性损失函数</li><li>使用外部知识库计算事实性得分</li><li>在训练中平衡流畅性和事实性</li></ul><p><strong>对比学习</strong></p><ul><li>使用对比学习区分事实性和非事实性内容</li><li>训练模型识别和避免幻觉</li></ul><p><strong>强化学习优化</strong></p><ul><li>设计基于事实准确性的奖励函数</li><li>使用PPO等算法优化事实性</li><li>实现事实性与流畅性的平衡</li></ul><h4 id="4-推理阶段的改进"><a href="#4-推理阶段的改进" class="headerlink" title="4. 推理阶段的改进"></a>4. 推理阶段的改进</h4><p><strong>后处理验证</strong></p><ul><li>使用外部工具验证生成内容的真实性</li><li>实现自动的事实性评分</li><li>对低置信度的内容进行标记</li></ul><p><strong>多模型验证</strong></p><ul><li>使用多个模型交叉验证</li><li>实现模型集成提高准确性</li></ul><p><strong>不确定性量化</strong></p><ul><li>为生成内容提供置信度分数</li><li>实现不确定性量化</li><li>帮助用户判断内容的可靠性</li></ul><h2 id="重复问题（Repetition）"><a href="#重复问题（Repetition）" class="headerlink" title="重复问题（Repetition）"></a>重复问题（Repetition）</h2><h3 id="重复问题的基本概念"><a href="#重复问题的基本概念" class="headerlink" title="重复问题的基本概念"></a>重复问题的基本概念</h3><p>重复问题表现为：</p><ul><li><strong>词汇重复</strong>：同一个词或短语反复出现</li><li><strong>结构重复</strong>：相似的句子结构重复使用</li><li><strong>内容重复</strong>：相同的信息多次表达</li></ul><h3 id="重复产生的底层机理"><a href="#重复产生的底层机理" class="headerlink" title="重复产生的底层机理"></a>重复产生的底层机理</h3><h4 id="1-训练数据的重复模式"><a href="#1-训练数据的重复模式" class="headerlink" title="1. 训练数据的重复模式"></a>1. 训练数据的重复模式</h4><p><strong>数据中的重复模式</strong></p><ul><li>训练数据中存在大量重复内容</li><li>某些表达方式在数据中频繁出现</li><li>模型学习到了这些重复模式</li></ul><p><strong>注意力机制的偏好</strong></p><ul><li>模型倾向于关注高频出现的模式</li><li>重复内容往往具有较高的注意力权重</li></ul><h4 id="2-生成策略的影响"><a href="#2-生成策略的影响" class="headerlink" title="2. 生成策略的影响"></a>2. 生成策略的影响</h4><p><strong>贪婪解码的局限性</strong></p><ul><li>每次都选择概率最高的token</li><li>容易陷入局部最优，导致重复</li></ul><p><strong>缺乏多样性约束</strong></p><ul><li>没有明确的多样性目标</li><li>模型倾向于选择”安全”的重复模式</li></ul><h4 id="3-上下文窗口的限制"><a href="#3-上下文窗口的限制" class="headerlink" title="3. 上下文窗口的限制"></a>3. 上下文窗口的限制</h4><p><strong>长距离依赖问题</strong></p><ul><li>模型难以记住之前生成的内容</li><li>在生成长文本时容易重复</li></ul><p><strong>注意力衰减</strong></p><ul><li>随着序列长度增加，注意力权重衰减</li><li>导致模型”忘记”之前的内容</li></ul><h3 id="缓解和消除重复的方法"><a href="#缓解和消除重复的方法" class="headerlink" title="缓解和消除重复的方法"></a>缓解和消除重复的方法</h3><h4 id="1-解码策略的改进"><a href="#1-解码策略的改进" class="headerlink" title="1. 解码策略的改进"></a>1. 解码策略的改进</h4><p><strong>多样性解码</strong></p><p><strong>核采样（Nucleus Sampling）</strong></p><ul><li>只从累积概率达到阈值的token中采样</li><li>避免选择过于保守的token</li><li>在保持质量的同时增加多样性</li></ul><p><strong>温度调节</strong></p><ul><li>使用温度参数控制采样的随机性</li><li>在生成过程中动态调整温度</li><li>平衡创造性和一致性</li></ul><p><strong>重复惩罚</strong></p><ul><li>对重复出现的token进行惩罚</li><li>使用n-gram级别的重复检测</li><li>实现自适应的重复惩罚机制</li></ul><p><strong>长度惩罚</strong></p><ul><li>对过长的重复序列进行惩罚</li><li>鼓励模型生成更简洁的内容</li></ul><h4 id="2-模型架构的改进-1"><a href="#2-模型架构的改进-1" class="headerlink" title="2. 模型架构的改进"></a>2. 模型架构的改进</h4><p><strong>改进的注意力机制</strong></p><p><strong>相对位置编码</strong></p><ul><li>使用相对位置编码代替绝对位置编码</li><li>更好地处理长序列</li><li>减少位置相关的重复</li></ul><p><strong>稀疏注意力</strong></p><ul><li>使用稀疏注意力减少计算复杂度</li><li>提高长文本的处理能力</li><li>减少注意力衰减问题</li></ul><p><strong>记忆机制</strong></p><ul><li>使用外部记忆存储重要信息</li><li>实现长期依赖的建模</li><li>减少重复生成相同内容</li></ul><p><strong>分层记忆</strong></p><ul><li>实现短期和长期记忆的分离</li><li>使用不同的记忆机制处理不同时间尺度的信息</li></ul><h4 id="3-训练策略的改进-1"><a href="#3-训练策略的改进-1" class="headerlink" title="3. 训练策略的改进"></a>3. 训练策略的改进</h4><p><strong>多样性训练</strong></p><p><strong>多样性损失</strong></p><ul><li>在训练中加入多样性损失</li><li>鼓励模型生成多样化的内容</li><li>平衡一致性和创造性</li></ul><p><strong>对抗训练</strong></p><ul><li>使用对抗训练提高多样性</li><li>训练判别器识别重复内容</li><li>实现生成器和判别器的博弈</li></ul><p><strong>课程学习</strong></p><ul><li>从简单任务开始，逐步增加复杂度</li><li>在训练过程中引入多样性约束</li><li>实现更好的泛化能力</li></ul><h4 id="4-推理阶段的改进-1"><a href="#4-推理阶段的改进-1" class="headerlink" title="4. 推理阶段的改进"></a>4. 推理阶段的改进</h4><p><strong>动态调整</strong></p><p><strong>自适应解码</strong></p><ul><li>根据上下文动态调整解码策略</li><li>实现智能的重复检测和避免</li><li>使用机器学习优化解码参数</li></ul><p><strong>多候选生成</strong></p><ul><li>生成多个候选序列</li><li>使用多样性指标选择最佳序列</li><li>实现更好的内容质量</li></ul><p><strong>后处理优化</strong></p><ul><li>使用规则或机器学习方法检测重复</li><li>自动移除或改写重复内容</li><li>实现智能的内容优化</li></ul><p><strong>风格一致性</strong></p><ul><li>保持生成内容的风格一致性</li><li>避免风格上的重复</li><li>实现更自然的文本生成</li></ul><h2 id="幻觉与重复问题的关系"><a href="#幻觉与重复问题的关系" class="headerlink" title="幻觉与重复问题的关系"></a>幻觉与重复问题的关系</h2><h3 id="共同根源"><a href="#共同根源" class="headerlink" title="共同根源"></a>共同根源</h3><p><strong>训练数据问题</strong></p><ul><li>数据质量差是幻觉和重复的共同原因</li><li>数据分布不均匀导致模型学习到错误的模式</li></ul><p><strong>Attention机制的局限性</strong></p><ul><li>2体相互作用的限制</li><li>难以处理复杂的全局关系</li></ul><p><strong>训练目标的不完善</strong></p><ul><li>缺乏对事实性和多样性的直接优化</li><li>过度依赖局部最优</li></ul><h3 id="相互影响"><a href="#相互影响" class="headerlink" title="相互影响"></a>相互影响</h3><p><strong>幻觉导致重复</strong></p><ul><li>当模型不确定时，倾向于重复”安全”的内容</li><li>幻觉内容可能被模型认为是正确的，从而重复生成</li></ul><p><strong>重复加剧幻觉</strong></p><ul><li>重复生成错误内容会强化幻觉</li><li>缺乏多样性限制了模型的探索能力</li></ul><h3 id="联合解决方案"><a href="#联合解决方案" class="headerlink" title="联合解决方案"></a>联合解决方案</h3><p><strong>统一的数据策略</strong></p><ul><li>同时提高数据的准确性和多样性</li><li>建立综合的数据质量评估体系</li></ul><p><strong>改进的模型架构</strong></p><ul><li>设计同时解决幻觉和重复的架构</li><li>引入全局一致性和多样性约束</li></ul><p><strong>综合的训练目标</strong></p><ul><li>平衡事实性、流畅性和多样性</li><li>使用多目标优化方法</li></ul><h2 id="未来发展方向"><a href="#未来发展方向" class="headerlink" title="未来发展方向"></a>未来发展方向</h2><h3 id="理论突破"><a href="#理论突破" class="headerlink" title="理论突破"></a>理论突破</h3><p><strong>3体Attention机制</strong><br>根据物理学理论，当前的Attention是2体相互作用，未来可能发展出3体Attention机制，能够更好地处理复杂的关系和依赖。</p><p><strong>量子计算的应用</strong><br>量子计算可能为Attention机制提供新的计算范式，实现更高效的注意力计算。</p><h3 id="技术融合"><a href="#技术融合" class="headerlink" title="技术融合"></a>技术融合</h3><p><strong>多模态融合</strong><br>结合视觉、听觉等多种模态信息，提高模型的理解能力和生成质量。</p><p><strong>知识图谱集成</strong><br>深度集成知识图谱，实现更准确的事实性生成。</p><h3 id="评估体系"><a href="#评估体系" class="headerlink" title="评估体系"></a>评估体系</h3><p><strong>标准化评估</strong><br>建立标准化的幻觉和重复评估体系，为模型改进提供客观指标。</p><p><strong>实时监控</strong><br>实现生成过程的实时监控，及时发现和纠正问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LLM的幻觉和重复问题是当前AI发展面临的重要挑战。通过深入理解其底层机理，我们可以从数据、模型架构、训练策略和推理优化等多个层面来缓解这些问题。随着技术的不断进步，我们有理由相信这些问题将得到更好的解决，推动LLM技术向更高水平发展。</p><p><strong>关键要点</strong>：</p><ol><li><strong>幻觉问题</strong>：主要由训练数据质量、Attention机制局限性和训练目标不匹配导致</li><li><strong>重复问题</strong>：主要由训练数据重复模式、生成策略局限性和上下文窗口限制导致</li><li><strong>解决方案</strong>：需要从数据、架构、训练和推理多个层面综合改进</li><li><strong>未来方向</strong>：3体Attention、多模态融合、标准化评估体系</li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://arxiv.org/html/2504.04600v1" target="_blank" rel="noopener">Capturing AI’s Attention: Physics of Repetition, Hallucination, Bias and Beyond</a></li><li><a href="https://zhuanlan.zhihu.com/p/677935286" target="_blank" rel="noopener">大语言模型幻觉问题研究综述</a></li><li><a href="https://www.secrss.com/articles/73856" target="_blank" rel="noopener">LLM幻觉问题的深度分析</a></li><li><a href="https://zhuanlan.zhihu.com/p/682647518" target="_blank" rel="noopener">大语言模型重复问题解决方案</a></li><li><a href="https://zhuanlan.zhihu.com/p/1897569693658744522" target="_blank" rel="noopener">Attention机制的物理基础</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LLM 的幻觉和重复问题是 LLM 应用中的核心挑战，也是面试中经常被问到的问题。本文将从底层机理出发，深入分析这两个问题的成因，并探讨有效的解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="幻觉" scheme="https://murphypei.github.io/tags/%E5%B9%BB%E8%A7%89/"/>
    
      <category term="重复" scheme="https://murphypei.github.io/tags/%E9%87%8D%E5%A4%8D/"/>
    
      <category term="Attention机制" scheme="https://murphypei.github.io/tags/Attention%E6%9C%BA%E5%88%B6/"/>
    
      <category term="模型训练" scheme="https://murphypei.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>LLM 训练：PPO 和 DPO</title>
    <link href="https://murphypei.github.io//blog/2025/06/llm-ppo-dpo.html"/>
    <id>https://murphypei.github.io//blog/2025/06/llm-ppo-dpo.html</id>
    <published>2025-06-24T12:44:51.000Z</published>
    <updated>2025-07-23T09:37:09.810Z</updated>
    
    <content type="html"><![CDATA[<p>已经接近 3 年没有更新博客了。今天立下一个 flag，开始准备 LLM 面试知识，主要是八股文为主，想到哪写到哪。第一篇没想到写啥，觉得对 PPO 和 DPO 比较了解，就先直接写这个吧。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在大语言模型（LLM）的训练过程中，RLHF（Reinforcement Learning from Human Feedback）是一个重要的技术，它通过人类反馈来优化模型的行为。在 RLHF 中，PPO（Proximal Policy Optimization）和 DPO（Direct Preference Optimization）是两种主流的算法。本文将详细介绍这两种算法的工作原理、数学推导以及它们之间的区别。</p><h2 id="PPO（Proximal-Policy-Optimization）"><a href="#PPO（Proximal-Policy-Optimization）" class="headerlink" title="PPO（Proximal Policy Optimization）"></a>PPO（Proximal Policy Optimization）</h2><h3 id="PPO-基本原理"><a href="#PPO-基本原理" class="headerlink" title="PPO 基本原理"></a>PPO 基本原理</h3><p>PPO 是一种基于策略梯度的强化学习算法，它的核心思想是通过限制策略更新的步长来保证训练的稳定性。在 RLHF 中，PPO 被用来优化语言模型，使其生成更符合人类偏好的回答。</p><h3 id="PPO-的数学推导"><a href="#PPO-的数学推导" class="headerlink" title="PPO 的数学推导"></a>PPO 的数学推导</h3><h4 id="1-策略梯度定理"><a href="#1-策略梯度定理" class="headerlink" title="1. 策略梯度定理"></a>1. 策略梯度定理</h4><p>首先，我们回顾一下策略梯度定理。对于策略 $\pi_\theta$，目标函数为：</p><script type="math/tex; mode=display">J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]</script><p>其中 $\tau$ 是轨迹，$R(\tau)$ 是轨迹的奖励。</p><p><strong>策略梯度定理</strong>是强化学习中的一个核心定理，它告诉我们如何直接优化策略参数 $\theta$ 来最大化期望奖励。这个定理的重要性在于：</p><ol><li><strong>直接优化策略</strong>：不像价值函数方法需要先学习价值函数再推导策略，策略梯度方法直接优化策略参数</li><li><strong>理论基础</strong>：为所有基于策略的强化学习算法提供了数学基础</li><li><strong>适用性广</strong>：适用于连续动作空间和离散动作空间</li></ol><p>策略梯度定理告诉我们：</p><script type="math/tex; mode=display">\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(\tau) R(\tau)]</script><p>这个公式的含义是：</p><ul><li><strong>左侧</strong>：目标函数 $J(\theta)$ 关于参数 $\theta$ 的梯度</li><li><strong>右侧</strong>：策略对数概率的梯度与奖励的乘积的期望</li></ul><p><strong>直观理解</strong>：</p><ul><li>如果某个轨迹 $\tau$ 的奖励 $R(\tau)$ 很高，我们就增加这个轨迹的概率</li><li>如果某个轨迹 $\tau$ 的奖励 $R(\tau)$ 很低，我们就减少这个轨迹的概率</li><li>$\nabla_\theta \log \pi_\theta(\tau)$ 告诉我们在参数空间中应该朝哪个方向移动</li></ul><p><strong>实际应用中的问题</strong>：</p><ol><li><strong>高方差</strong>：直接使用这个公式会导致训练不稳定</li><li><strong>样本效率低</strong>：需要大量样本来估计期望</li><li><strong>更新步长难以控制</strong>：可能导致策略更新过大或过小</li></ol><p>这就是为什么需要 PPO 等改进算法的原因。</p><h4 id="2-PPO-的目标函数"><a href="#2-PPO-的目标函数" class="headerlink" title="2. PPO 的目标函数"></a>2. PPO 的目标函数</h4><p>PPO 通过引入一个比率项来限制策略更新的幅度：</p><script type="math/tex; mode=display">r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}</script><p><strong>比率项的含义</strong>：</p><ul><li>$\pi_{\theta_{old}}(a_t|s_t)$ 是旧策略（更新前）在状态 $s_t$ 下选择动作 $a_t$ 的概率</li><li>$\pi_\theta(a_t|s_t)$ 是新策略（更新后）在状态 $s_t$ 下选择动作 $a_t$ 的概率</li><li>比率 $r_t(\theta)$ 衡量了新策略相对于旧策略的变化程度</li></ul><p><strong>为什么需要限制策略更新幅度</strong>：</p><ol><li><strong>防止策略崩溃</strong>：如果策略更新过大，可能导致某些动作的概率变为 0，失去探索能力</li><li><strong>保证训练稳定性</strong>：过大的更新步长会导致训练不稳定，甚至发散</li><li><strong>避免灾难性遗忘</strong>：防止新策略完全偏离旧策略，丢失之前学到的有用知识</li></ol><p>PPO 的目标函数为：</p><script type="math/tex; mode=display">L^{CLIP}(\theta) = \mathbb{E}_t [\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]</script><p>这个公式的核心思想是：</p><ul><li>如果 $A_t &gt; 0$（好的动作），我们希望增加这个动作的概率，但最多只能增加到 $(1+\epsilon)$ 倍</li><li>如果 $A_t &lt; 0$（坏的动作），我们希望减少这个动作的概率，但最多只能减少到 $(1-\epsilon)$ 倍</li><li>$\epsilon$ 通常设置为 0.2，意味着策略更新幅度被限制在 ±20% 以内</li></ul><p>其中：</p><ul><li>$A_t$ 是优势函数（Advantage function）</li><li>$\epsilon$ 是裁剪参数，通常设置为 0.2</li><li>$\text{clip}(x, a, b)$ 函数将 $x$ 限制在 $[a, b]$ 范围内</li></ul><h4 id="3-优势函数"><a href="#3-优势函数" class="headerlink" title="3. 优势函数"></a>3. 优势函数</h4><p>优势函数衡量了某个动作相对于平均水平的优势：</p><script type="math/tex; mode=display">A_t = Q(s_t, a_t) - V(s_t)</script><p>其中 $Q(s_t, a_t)$ 是动作价值函数，$V(s_t)$ 是状态价值函数。</p><p><strong>价值函数的基本概念</strong>：</p><p>在强化学习中，价值函数用于评估状态或状态-动作对的价值，帮助我们做出更好的决策。</p><p><strong>状态价值函数 $V(s_t)$</strong>：</p><ul><li><strong>定义</strong>：在状态 $s_t$ 下，遵循策略 $\pi$ 的期望累积奖励</li><li><strong>数学表达式</strong>：<script type="math/tex; mode=display">V^\pi(s_t) = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k} | S_t = s_t]</script></li><li><strong>含义</strong>：表示从状态 $s_t$ 开始，按照策略 $\pi$ 行动，能够获得的期望总奖励</li><li><strong>特点</strong>：只依赖于状态，不依赖于具体动作</li></ul><p><strong>动作价值函数 $Q(s_t, a_t)$</strong>：</p><ul><li><strong>定义</strong>：在状态 $s_t$ 下采取动作 $a_t$，然后遵循策略 $\pi$ 的期望累积奖励</li><li><strong>数学表达式</strong>：<script type="math/tex; mode=display">Q^\pi(s_t, a_t) = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k} | S_t = s_t, A_t = a_t]</script></li><li><strong>含义</strong>：表示在状态 $s_t$ 下采取动作 $a_t$，然后按照策略 $\pi$ 行动，能够获得的期望总奖励</li><li><strong>特点</strong>：依赖于状态和动作的组合</li></ul><p><strong>优势函数 $A_t$ 的作用</strong>：</p><ul><li><strong>相对评估</strong>：优势函数衡量了某个动作相对于该状态下所有动作平均水平的优势</li><li><strong>决策指导</strong>：<ul><li>如果 $A_t &gt; 0$，说明动作 $a_t$ 比平均水平好，应该增加其概率</li><li>如果 $A_t &lt; 0$，说明动作 $a_t$ 比平均水平差，应该减少其概率</li><li>如果 $A_t = 0$，说明动作 $a_t$ 处于平均水平</li></ul></li></ul><p><strong>在 PPO 中的重要性</strong>：</p><ol><li><strong>减少方差</strong>：相比直接使用奖励，优势函数提供了更稳定的学习信号</li><li><strong>基线作用</strong>：状态价值函数作为基线，减少了策略梯度的方差</li><li><strong>相对比较</strong>：通过相对比较而不是绝对奖励，使得训练更加稳定</li></ol><p><strong>实际计算中的挑战</strong>：</p><ul><li>真实的价值函数通常是未知的，需要通过神经网络来估计</li><li>这就是为什么 PPO 需要 Critic 模型来估计状态价值函数</li><li>优势函数通常通过时序差分（TD）方法或其他技术来估计</li></ul><h3 id="PPO-在-RLHF-中的应用"><a href="#PPO-在-RLHF-中的应用" class="headerlink" title="PPO 在 RLHF 中的应用"></a>PPO 在 RLHF 中的应用</h3><p>在 RLHF 中，PPO 需要四个模型：</p><ol><li><strong>Actor Model</strong>：被训练的策略模型</li><li><strong>Critic Model</strong>：价值函数模型，用于估计状态价值</li><li><strong>Reward Model</strong>：奖励模型，用于计算即时奖励</li><li><strong>Reference Model</strong>：参考模型，用于防止策略偏离太远</li></ol><p>关于这 4 个模型，可以参考我之前的文章：<a href="https://murphypei.github.io/blog/2024/07/llm-rlhf-ppo.html">大模型 RLHF 训练中的 PPO 算法细节</a></p><p><strong>四个模型的作用和特点</strong>：</p><p><strong>Actor Model（策略模型）</strong>：</p><ul><li>这是我们要训练的主要模型，最终用于实际应用</li><li>接收 prompt，生成 response</li><li>在训练过程中，其参数会不断更新以优化策略</li></ul><p><strong>Critic Model（价值函数模型）</strong>：</p><ul><li>用于估计状态价值函数 $V(s_t)$</li><li>通常用 Reward Model 初始化，架构与 Actor 相似</li><li>在最后一层增加 Value Head，输出单一的价值估计</li><li>需要更新参数，因为价值估计能力需要不断提升</li></ul><p><strong>Reward Model（奖励模型）</strong>：</p><ul><li>计算即时奖励 $R_t$，评估当前 response 的好坏</li><li>参数固定不更新，作为客观的评估标准</li><li>只关心当前 response 的质量，不考虑长期影响</li></ul><p><strong>Reference Model（参考模型）</strong>：</p><ul><li>通常用 SFT 模型初始化，参数冻结</li><li>主要作用是防止 Actor”训歪”，避免过拟合到高分但无意义的回答</li><li>通过 KL 散度约束，确保新策略与参考策略的输出分布相似</li></ul><p><strong>为什么需要四个模型</strong>：</p><ol><li><strong>Actor</strong>：学习生成符合人类偏好的回答</li><li><strong>Critic</strong>：评估整体价值，减少训练方差</li><li><strong>Reward</strong>：提供客观的即时评估标准</li><li><strong>Reference</strong>：防止策略偏离太远，保持语言能力</li></ol><p>PPO 的损失函数包括三个部分：</p><script type="math/tex; mode=display">L_{PPO} = L^{CLIP} - \alpha L^{KL} + \beta L^{VF}</script><p>其中：</p><ul><li>$L^{CLIP}$ 是 PPO 的主要损失，通过裁剪机制限制策略更新</li><li>$L^{KL}$ 是 KL 散度损失，用于限制与参考模型的差异</li><li>$L^{VF}$ 是价值函数损失，用于训练 Critic 模型</li><li>$\alpha$ 和 $\beta$ 是权重参数，平衡不同损失项的重要性</li></ul><p><strong>训练流程</strong>：</p><ol><li>Actor 接收 prompt，生成 response</li><li>Reward Model 计算即时奖励</li><li>Critic Model 估计状态价值</li><li>计算优势函数 $A_t = R_t - V_t$</li><li>使用 PPO 损失函数更新 Actor 和 Critic 参数</li><li>通过 KL 散度约束确保与 Reference Model 的相似性</li></ol><h2 id="DPO（Direct-Preference-Optimization）"><a href="#DPO（Direct-Preference-Optimization）" class="headerlink" title="DPO（Direct Preference Optimization）"></a>DPO（Direct Preference Optimization）</h2><h3 id="DPO-基本原理"><a href="#DPO-基本原理" class="headerlink" title="DPO 基本原理"></a>DPO 基本原理</h3><p>DPO 是一种更直接的方法，它不需要显式的奖励模型，而是直接通过人类偏好数据来优化策略。DPO 的核心思想是将偏好学习问题转化为一个分类问题。</p><p><strong>DPO 的核心创新</strong>：</p><ol><li><strong>消除奖励模型</strong>：不需要单独训练奖励模型，简化了训练流程</li><li><strong>直接偏好学习</strong>：直接从人类偏好数据中学习，避免了奖励建模的误差</li><li><strong>理论等价性</strong>：证明了 DPO 与基于奖励模型的 RLHF 在理论上是等价的</li></ol><h3 id="DPO-的数学推导"><a href="#DPO-的数学推导" class="headerlink" title="DPO 的数学推导"></a>DPO 的数学推导</h3><h4 id="1-偏好学习问题"><a href="#1-偏好学习问题" class="headerlink" title="1. 偏好学习问题"></a>1. 偏好学习问题</h4><p>给定一个提示 $x$ 和两个回答 $y_w$（获胜）和 $y_l$（失败），我们的目标是学习一个策略 $\pi_\theta$，使得：</p><script type="math/tex; mode=display">P(y_w \succ y_l | x) > P(y_l \succ y_w | x)</script><p><strong>偏好数据的含义</strong>：</p><ul><li>$y_w \succ y_l$ 表示在提示 $x$ 下，回答 $y_w$ 比 $y_l$ 更受人类偏好</li><li>这种偏好关系反映了人类的价值判断，是 RLHF 的核心数据</li></ul><h4 id="2-Bradley-Terry-模型"><a href="#2-Bradley-Terry-模型" class="headerlink" title="2. Bradley-Terry 模型"></a>2. Bradley-Terry 模型</h4><p>DPO 使用 Bradley-Terry 模型来建模偏好。这个模型假设偏好概率与奖励函数之间存在以下关系：</p><script type="math/tex; mode=display">P(y_w \succ y_l | x) = \frac{\exp(r_\theta(x, y_w))}{\exp(r_\theta(x, y_w)) + \exp(r_\theta(x, y_l))}</script><p>其中 $r_\theta(x, y)$ 是奖励函数。</p><p><strong>Bradley-Terry 模型的特点</strong>：</p><ul><li><strong>单调性</strong>：奖励越高，被偏好的概率越大</li><li><strong>对称性</strong>：$P(y_w \succ y_l | x) + P(y_l \succ y_w | x) = 1$</li><li><strong>温度控制</strong>：可以通过调整指数函数的温度参数来控制偏好强度</li></ul><h4 id="3-从奖励函数到策略的映射"><a href="#3-从奖励函数到策略的映射" class="headerlink" title="3. 从奖励函数到策略的映射"></a>3. 从奖励函数到策略的映射</h4><p>DPO 的关键洞察是：我们可以将奖励函数表示为策略与参考策略的比值：</p><script type="math/tex; mode=display">r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}</script><p>其中：</p><ul><li>$\pi_{ref}$ 是参考策略（通常是 SFT 模型）</li><li>$\beta$ 是温度参数，控制奖励的强度</li></ul><p><strong>这个映射的物理意义</strong>：</p><ul><li>如果 $\pi_\theta(y|x) &gt; \pi_{ref}(y|x)$，说明新策略更倾向于生成回答 $y$，奖励为正</li><li>如果 $\pi_\theta(y|x) &lt; \pi_{ref}(y|x)$，说明新策略不太倾向于生成回答 $y$，奖励为负</li><li>$\beta$ 控制奖励的敏感度，值越大，策略差异对奖励的影响越明显</li></ul><h4 id="4-DPO-的目标函数"><a href="#4-DPO-的目标函数" class="headerlink" title="4. DPO 的目标函数"></a>4. DPO 的目标函数</h4><p>将奖励函数代入 Bradley-Terry 模型，得到 DPO 的目标函数：</p><script type="math/tex; mode=display">L_{DPO} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]</script><p>其中：</p><ul><li>$\sigma$ 是 sigmoid 函数：$\sigma(x) = \frac{1}{1 + e^{-x}}$</li><li>$\beta$ 是温度参数，通常设置为 0.1-0.5</li><li>$\pi_{ref}$ 是参考策略（通常是 SFT 模型）</li></ul><p><strong>目标函数的直观理解</strong>：</p><ul><li>我们希望最大化偏好数据的对数似然</li><li>对于偏好对 $(y_w, y_l)$，我们希望 $P(y_w \succ y_l | x)$ 尽可能大</li><li>这等价于让 $\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}$ 尽可能大</li><li>即让获胜回答相对于参考策略的提升幅度大于失败回答</li></ul><h4 id="5-奖励函数的推导"><a href="#5-奖励函数的推导" class="headerlink" title="5. 奖励函数的推导"></a>5. 奖励函数的推导</h4><p>通过 DPO 的训练，我们可以推导出隐含的奖励函数：</p><script type="math/tex; mode=display">r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}</script><p>这个公式表明，DPO 实际上是在学习一个相对于参考策略的奖励函数。</p><p><strong>奖励函数的性质</strong>：</p><ol><li><strong>相对性</strong>：奖励是相对于参考策略定义的，不是绝对奖励</li><li><strong>可解释性</strong>：奖励直接反映了策略相对于参考策略的偏好程度</li><li><strong>一致性</strong>：与人类偏好数据保持一致</li></ol><h3 id="DPO-的训练过程"><a href="#DPO-的训练过程" class="headerlink" title="DPO 的训练过程"></a>DPO 的训练过程</h3><p><strong>训练步骤</strong>：</p><ol><li><strong>数据准备</strong>：收集人类偏好数据 $(x, y_w, y_l)$</li><li><strong>策略初始化</strong>：用 SFT 模型初始化 $\pi_\theta$ 和 $\pi_{ref}$</li><li><strong>前向传播</strong>：计算 $\pi_\theta(y_w|x)$ 和 $\pi_\theta(y_l|x)$</li><li><strong>损失计算</strong>：使用 DPO 损失函数计算梯度</li><li><strong>参数更新</strong>：只更新 $\pi_\theta$，保持 $\pi_{ref}$ 固定</li></ol><p><strong>关键超参数</strong>：</p><ul><li><strong>$\beta$（温度参数）</strong>：控制策略更新的强度，值越大更新越激进</li><li><strong>学习率</strong>：控制参数更新的步长</li><li><strong>批次大小</strong>：影响训练的稳定性和效率</li></ul><h3 id="DPO-损失计算的具体步骤"><a href="#DPO-损失计算的具体步骤" class="headerlink" title="DPO 损失计算的具体步骤"></a>DPO 损失计算的具体步骤</h3><p>为了更好地理解 DPO 的训练过程，我们来详细拆解针对一个具体样本的损失计算步骤。整个过程的核心思想是：<strong>直接利用偏好数据（哪个回答更好，哪个更差）来调整模型，让模型生成”更好”回答的概率变高，生成”更差”回答的概率变低。</strong></p><p>为了实现这个目标，DPO 的训练过程涉及两个模型：</p><ol><li><strong>策略模型 ($\pi_{\theta}$)</strong>：我们正在训练和优化的模型。它的参数在训练中会不断更新。</li><li><strong>参考模型 ($\pi_{ref}$)</strong>：一个固定的、不参与训练的模型。通常是策略模型在 DPO 训练开始前的初始版本（比如，经过 SFT 监督微调后的模型）。它的作用是作为一把”尺子”，防止策略模型在学习偏好的过程中偏离太远，忘掉其原有的语言能力。</li></ol><p>假设我们有以下一个训练样本：</p><ul><li><strong>Prompt (x)</strong>: “请介绍一下长城”</li><li><strong>Chosen (y_w)</strong>: “长城是古代中国为抵御侵略而修筑的军事工程。” (被标注为更好的回答)</li><li><strong>Rejected (y_l)</strong>: “长城是个墙，在中国。” (被标注为更差的回答)</li></ul><h4 id="第一步：计算两个回答在两个模型下的概率"><a href="#第一步：计算两个回答在两个模型下的概率" class="headerlink" title="第一步：计算两个回答在两个模型下的概率"></a>第一步：计算两个回答在两个模型下的概率</h4><p>模型处理的是 token 序列，而不是文字。假设经过分词后，两个回答的 token 序列如下：</p><ul><li><strong>y_w</strong>: <code>[&quot;长城&quot;, &quot;是&quot;, &quot;古代&quot;, &quot;中国&quot;, &quot;为&quot;, ...]</code></li><li><strong>y_l</strong>: <code>[&quot;长城&quot;, &quot;是&quot;, &quot;个&quot;, &quot;墙&quot;, &quot;，&quot;, ...]</code></li></ul><p>对于一个自回归语言模型来说，一个完整序列的概率是该序列中每个 token 的条件概率的乘积。在实际计算中，为了数值稳定性，我们通常使用对数概率（log probabilities）的加和。</p><p><strong>计算过程：</strong></p><ol><li><p><strong>对于 “Chosen” 回答 (y_w):</strong></p><ul><li>将 <code>Prompt (x)</code> 和 <code>Chosen (y_w)</code> 拼接起来，输入给<strong>策略模型 ($\pi_{\theta}$)</strong>。</li><li>模型会为 <code>y_w</code> 中的每一个 token 计算其生成的对数概率。例如，计算 P(“是” | “长城”)，P(“古代” | “长城是”)，以此类推。</li><li>将 <code>y_w</code> 序列中所有 token 的对数概率相加，得到<strong>策略模型</strong>认为生成 <code>y_w</code> 的总对数概率：$logP_{\pi_{\theta}}(y_w|x)$。</li><li>用同样的方法，将 <code>Prompt (x)</code> 和 <code>Chosen (y_w)</code> 输入给<strong>参考模型 ($\pi_{ref}$)</strong>，计算出<strong>参考模型</strong>认为生成 <code>y_w</code> 的总对数概率：$logP_{\pi_{ref}}(y_w|x)$。</li></ul></li><li><p><strong>对于 “Rejected” 回答 (y_l):</strong></p><ul><li>同样地，将 <code>Prompt (x)</code> 和 <code>Rejected (y_l)</code> 拼接后，分别输入给<strong>策略模型 ($\pi_{\theta}$)</strong> 和<strong>参考模型 ($\pi_{ref}$)</strong>。</li><li>计算出策略模型生成 <code>y_l</code> 的总对数概率：$logP_{\pi_{\theta}}(y_l|x)$。</li><li>计算出参考模型生成 <code>y_l</code> 的总对数概率：$logP_{\pi_{ref}}(y_l|x)$。</li></ul></li></ol><p>经过这一步，我们就得到了四个核心的对数概率值。</p><h4 id="第二步：计算隐式奖励-Implicit-Reward-或偏好度"><a href="#第二步：计算隐式奖励-Implicit-Reward-或偏好度" class="headerlink" title="第二步：计算隐式奖励 (Implicit Reward) 或偏好度"></a>第二步：计算隐式奖励 (Implicit Reward) 或偏好度</h4><p>DPO 的精髓在于，它证明了模型的偏好程度可以被一个简单的公式表示，这个公式衡量了策略模型相对于参考模型的改进程度。</p><ol><li><p><strong>计算 “Chosen” 回答的偏好度：</strong><br>这个值反映了策略模型相比于参考模型，有多”倾向于”生成那个更好的回答。</p><script type="math/tex; mode=display">r_w = \beta \cdot (logP_{\pi_{\theta}}(y_w|x) - logP_{\pi_{ref}}(y_w|x))</script><p>其中 $\beta$ 是一个超参数（通常设为 0.1），用来控制策略模型与参考模型之间的差异程度。</p></li><li><p><strong>计算 “Rejected” 回答的偏好度：</strong><br>同理，这个值反映了策略模型相比于参考模型，有多”倾向于”生成那个更差的回答。</p><script type="math/tex; mode=display">r_l = \beta \cdot (logP_{\pi_{\theta}}(y_l|x) - logP_{\pi_{ref}}(y_l|x))</script></li></ol><h4 id="第三步：计算最终的-DPO-损失"><a href="#第三步：计算最终的-DPO-损失" class="headerlink" title="第三步：计算最终的 DPO 损失"></a>第三步：计算最终的 DPO 损失</h4><p>DPO 的损失函数目标是最大化”Chosen”回答的偏好度与”Rejected”回答的偏好度之间的差距。</p><ol><li><p><strong>计算偏好度差异：</strong></p><script type="math/tex; mode=display">\text{diff} = r_w - r_l</script><p>将第二步的公式代入，得到：</p><script type="math/tex; mode=display">\text{diff} = \beta \cdot [(logP_{\pi_{\theta}}(y_w|x) - logP_{\pi_{ref}}(y_w|x)) - (logP_{\pi_{\theta}}(y_l|x) - logP_{\pi_{ref}}(y_l|x))]</script></li><li><p><strong>应用 Sigmoid 函数和负对数：</strong><br>DPO 将这个差异值传入一个 <code>log-sigmoid</code> 函数中来构造最终的损失。</p><script type="math/tex; mode=display">\text{Loss} = -log(\sigma(\text{diff}))</script><p>其中 $\sigma$ 是 Sigmoid 函数。</p></li></ol><p><strong>这个损失函数的直观理解是：</strong></p><ul><li>如果偏好度差异 <code>diff</code> 很大（即策略模型非常明确地更喜欢 <code>y_w</code> 而不是 <code>y_l</code>），那么 $\sigma(\text{diff})$ 的值会趋近于 1，$log(\sigma(\text{diff}))$ 会趋近于 0，最终的损失值 <code>Loss</code> 也就很小。这表示模型已经学习得很好了，不需要太多调整。</li><li>如果偏好度差异 <code>diff</code> 很小甚至是负数（即策略模型对 <code>y_w</code> 和 <code>y_l</code> 的偏好不明显，甚至搞反了），那么 $\sigma(\text{diff})$ 的值会小于 1，$log(\sigma(\text{diff}))$ 会是一个负数，最终的损失值 <code>Loss</code> 就会是一个较大的正数。这个较大的损失会通过反向传播来更新<strong>策略模型</strong>的参数。</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>通过这个损失函数进行梯度下降，模型参数的更新会朝着以下目标进行：</p><ul><li><strong>提高</strong> $logP_{\pi_{\theta}}(y_w|x)$ (增加生成 Chosen 回答的概率)</li><li><strong>降低</strong> $logP_{\pi_{\theta}}(y_l|x)$ (降低生成 Rejected 回答的概率)</li></ul><p>同时，由于参考模型 $logP_{\pi_{ref}}$ 的存在，这个过程又被施加了一个约束，确保策略模型不会为了迎合偏好而产生乱七八糟、不合语法的回答，从而保证了训练的稳定性。这就是 DPO 针对一个 token 序列（样本）计算输出和损失的全过程。</p><h3 id="DPO-的优势和局限性"><a href="#DPO-的优势和局限性" class="headerlink" title="DPO 的优势和局限性"></a>DPO 的优势和局限性</h3><p><strong>优势</strong>：</p><ol><li><strong>训练简单</strong>：只需要两个模型，训练流程简单</li><li><strong>数据效率高</strong>：直接使用偏好数据，避免了奖励建模的误差</li><li><strong>理论保证</strong>：在理论上与基于奖励的 RLHF 等价</li><li><strong>计算效率高</strong>：单轮训练，不需要复杂的优势估计</li></ol><p><strong>局限性</strong>：</p><ol><li><strong>依赖参考策略</strong>：奖励函数是相对于参考策略定义的</li><li><strong>偏好数据质量</strong>：对偏好数据的质量要求较高</li><li><strong>探索能力有限</strong>：可能无法探索到远离参考策略的新策略</li><li><strong>温度参数敏感</strong>：$\beta$ 的选择对性能影响较大</li></ol><h3 id="DPO-与-PPO-的理论联系"><a href="#DPO-与-PPO-的理论联系" class="headerlink" title="DPO 与 PPO 的理论联系"></a>DPO 与 PPO 的理论联系</h3><p><strong>等价性的核心前提</strong>：</p><p>DPO 与 PPO 等价的核心前提是：<strong>奖励函数必须满足特定的形式</strong>。具体来说，奖励函数必须能够表示为：</p><script type="math/tex; mode=display">r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}</script><p>这个前提条件意味着：</p><ol><li><strong>奖励函数与策略的耦合</strong>：奖励函数不能是任意的，必须与当前策略 $\pi_\theta$ 和参考策略 $\pi_{ref}$ 相关</li><li><strong>相对性</strong>：奖励是相对于参考策略定义的，不是绝对奖励</li><li><strong>策略依赖性</strong>：奖励函数会随着策略的更新而变化</li></ol><p><strong>为什么这个前提很重要</strong>：</p><p>在实际的 RLHF 中，奖励函数通常是独立训练的，其形式为 $r(x, y) = f_\phi(x, y)$，其中 $f_\phi$ 是一个独立的神经网络。这种形式的奖励函数与 DPO 假设的形式完全不同。</p><p><strong>等价性的假设前提</strong>：</p><p>DPO 与 PPO 的等价性是在以下关键假设下成立的：</p><ol><li><strong>奖励函数假设</strong>：奖励函数必须满足 $r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$ 的形式</li><li><strong>优势函数简化</strong>：忽略价值函数，直接使用奖励作为优势函数，即 $A_t = r_t$</li><li><strong>策略约束方式</strong>：使用 KL 散度约束而不是 PPO 的裁剪约束</li><li><strong>单步优化</strong>：假设每次更新都是单步的，不考虑多步交互</li></ol><p><strong>理论背景</strong>：</p><p>这个等价性来自于<strong>奖励函数与策略之间的对偶关系</strong>。在强化学习中，存在一个重要的理论结果：对于任何奖励函数 $r(x, y)$，都存在一个最优策略 $\pi^*(y|x)$，使得：</p><script type="math/tex; mode=display">\pi^*(y|x) \propto \pi_{ref}(y|x) \exp(\frac{r(x, y)}{\beta})</script><p>这个关系表明，奖励函数和策略之间存在一一对应的关系。DPO 的关键洞察是：如果我们直接学习策略，就可以隐式地学习到对应的奖励函数。</p><p><strong>等价性证明的局限性</strong>：</p><p>需要注意的是，这种等价性有以下局限性：</p><ol><li><strong>奖励函数形式限制</strong>：只有在特定形式的奖励函数下才成立</li><li><strong>忽略价值函数</strong>：实际 PPO 中价值函数的作用被简化了</li><li><strong>约束方式不同</strong>：PPO 的裁剪约束和 DPO 的 KL 散度约束在理论上不等价</li><li><strong>训练稳定性</strong>：虽然理论等价，但实际训练中的稳定性可能不同</li></ol><p><strong>实际应用中的差异</strong>：</p><p>尽管在理论上存在等价性，但在实际应用中：</p><ol><li><strong>PPO</strong>：通过显式奖励模型提供更直接的监督信号</li><li><strong>DPO</strong>：通过偏好数据提供相对比较信号</li><li><strong>训练稳定性</strong>：PPO 的裁剪机制可能提供更好的训练稳定性</li><li><strong>探索能力</strong>：PPO 可能具有更好的探索能力</li></ol><p><strong>等价性证明</strong>：<br>DPO 可以看作是 PPO 在特定条件下的简化版本。当 PPO 中的：</p><ul><li>奖励模型 $r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$</li><li>优势函数 $A_t = r_t$（忽略价值函数）</li><li>策略约束通过 KL 散度实现</li></ul><p>此时，PPO 的损失函数就退化为 DPO 的形式。</p><p><strong>主要区别</strong>：</p><ol><li><strong>奖励建模</strong>：PPO 需要显式奖励模型，DPO 隐含在策略中</li><li><strong>优势估计</strong>：PPO 需要复杂的优势估计，DPO 直接使用奖励</li><li><strong>约束方式</strong>：PPO 使用裁剪约束，DPO 使用 KL 散度约束</li></ol><h2 id="PPO-和-DPO-的区别"><a href="#PPO-和-DPO-的区别" class="headerlink" title="PPO 和 DPO 的区别"></a>PPO 和 DPO 的区别</h2><h3 id="1-训练复杂度"><a href="#1-训练复杂度" class="headerlink" title="1. 训练复杂度"></a>1. 训练复杂度</h3><ul><li><strong>PPO</strong>：需要四个模型（Actor、Critic、Reward、Reference），训练过程复杂</li><li><strong>DPO</strong>：只需要两个模型（策略模型和参考模型），训练过程简单</li></ul><h3 id="2-数据需求"><a href="#2-数据需求" class="headerlink" title="2. 数据需求"></a>2. 数据需求</h3><ul><li><strong>PPO</strong>：需要显式的奖励信号或奖励模型</li><li><strong>DPO</strong>：只需要偏好数据（哪个更好），不需要显式奖励</li></ul><h3 id="3-计算效率"><a href="#3-计算效率" class="headerlink" title="3. 计算效率"></a>3. 计算效率</h3><ul><li><strong>PPO</strong>：需要多轮交互和复杂的优势估计</li><li><strong>DPO</strong>：单轮训练，计算效率更高</li></ul><h3 id="4-稳定性"><a href="#4-稳定性" class="headerlink" title="4. 稳定性"></a>4. 稳定性</h3><ul><li><strong>PPO</strong>：通过裁剪机制保证训练稳定性</li><li><strong>DPO</strong>：通过 KL 散度约束保证稳定性</li></ul><h3 id="5-适用场景"><a href="#5-适用场景" class="headerlink" title="5. 适用场景"></a>5. 适用场景</h3><ul><li><strong>PPO</strong>：适用于有明确奖励信号或可以训练奖励模型的场景</li><li><strong>DPO</strong>：适用于只有人类偏好数据的场景</li></ul><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>PPO 和 DPO 都是 RLHF 中的重要算法，它们各有优缺点：</p><ul><li><strong>PPO</strong> 更加成熟和稳定，但训练复杂度高</li><li><strong>DPO</strong> 更加简单和高效，但可能在某些场景下效果不如 PPO</li></ul><p>选择哪种算法主要取决于具体的应用场景和可用的数据。在实际应用中，可以根据需求选择合适的算法，或者将两种算法结合使用。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li>Schulman, J., et al. “Proximal policy optimization algorithms.” arXiv preprint arXiv:1707.06347 (2017).</li><li>Rafailov, R., et al. “Direct preference optimization: Your language model is secretly a reward model.” arXiv preprint arXiv:2305.18290 (2023).</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已经接近 3 年没有更新博客了。今天立下一个 flag，开始准备 LLM 面试知识，主要是八股文为主，想到哪写到哪。第一篇没想到写啥，觉得对 PPO 和 DPO 比较了解，就先直接写这个吧。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="rlhf" scheme="https://murphypei.github.io/tags/rlhf/"/>
    
      <category term="ppo" scheme="https://murphypei.github.io/tags/ppo/"/>
    
      <category term="dpo" scheme="https://murphypei.github.io/tags/dpo/"/>
    
  </entry>
  
  <entry>
    <title>图像生成基础：DDPM</title>
    <link href="https://murphypei.github.io//blog/2024/07/aigc-ddpm.html"/>
    <id>https://murphypei.github.io//blog/2024/07/aigc-ddpm.html</id>
    <published>2024-07-31T09:44:51.000Z</published>
    <updated>2025-07-21T11:29:15.401Z</updated>
    
    <content type="html"><![CDATA[<p>目前所采用的扩散模型大都是来自于 2020 年的工作 DDPM。DDPM 对之前的扩散模型进行了简化，并通过变分推断（variational inference）来进行建模，这主要是因为扩散模型也是一个隐变量模型（latent variable model），相比 VAE 这样的隐变量模型，扩散模型的隐变量是和原始数据是同维度的，而且推理过程（即扩散过程）往往是固定的。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>扩散模型包括两个过程：前向过程（forward process）和反向过程（reverse process），其中前向过程又称为扩散过程（diffusion process），如下图所示。无论是前向过程还是反向过程都是一个参数化的马尔可夫链（Markov chain），其中反向过程可以用来生成数据，这里我们将通过变分推断来进行建模和求解。</p><p><img src="/images/posts/aigc/ddpm/1.webp" alt></p><h3 id="前向扩散过程（Forward-Diffusion-Process）"><a href="#前向扩散过程（Forward-Diffusion-Process）" class="headerlink" title="前向扩散过程（Forward Diffusion Process）"></a>前向扩散过程（Forward Diffusion Process）</h3><p>解释扩散之前先介绍一个基本的数学表示：</p><script type="math/tex; mode=display">\mathcal{N}(x_t; \mu, \Sigma)</script><p>一个正态分布，其中 $\mu$ 是均值，$\Sigma$ 是协方差矩阵。在这个过程中，$x_t$ 服从一个 $\mu$ 为均值、$\Sigma$ 为协方差矩阵的正态分布。</p><p>扩散就是对图像数据进行加噪声的过程，<strong>最核心的数学公式</strong>表示如下：</p><script type="math/tex; mode=display">q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t \mathbf{I})</script><p>$x_0$ 是原始数据，$x_t$ 是在 $t$ 时刻的样本，$\mathcal{N}$ 表示正态分布，$\beta_t$ 表示在第 $t$ 步的方差（噪音量），它是一个介于 0 和 1 之间的值，$\sqrt{1 - \beta_t}$ 表示输入数据的缩放系数，$\beta_t \mathbf{I}$ 表示加的噪音的方差。</p><p>这个公式表示，给定 $x_{t-1}$ 的情况下，$x_t$ 是以 $\sqrt{1 - \beta_t} x_{t-1}$ 为均值、$\beta_t \mathbf{I}$ 为协方差矩阵的正态分布。可以简单理解为，$x_t$ 是 $x_{t-1}$ 加上高斯噪音后的结果。</p><p>前向过程就是这么简单。当我们逐渐加大 $\beta_t$ 时，$x_t$ 逐渐变得模糊，最终变成一个高斯噪声图像。</p><script type="math/tex; mode=display">q(x_T|x_0) \approx \mathcal{N}(0, I)</script><p>这里也有一个推导，就是通过 $x_0$，可以直接表示 $x_T$，因为高斯分布可以直接相加。</p><h3 id="逆向生成过程（Reverse-Generation-Process）"><a href="#逆向生成过程（Reverse-Generation-Process）" class="headerlink" title="逆向生成过程（Reverse Generation Process）"></a>逆向生成过程（Reverse Generation Process）</h3><p>训练过程中，DDPM 学习从噪声生成数据的逆向过程。我们<strong>假设逆向过程也是一个高斯过程</strong>，但参数未知：</p><script type="math/tex; mode=display">p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_\theta^2(x_t, t) I)</script><p>这里，模型的任务是学习 $\mu_\theta$ 和 $\sigma_\theta$ 的参数化形式，使得可以从噪声生成逼真的数据样本。</p><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><p>训练的目标是最小化前向过程和逆向过程之间的差异。具体来说，训练目标可以表示为以下 KL 散度的和：</p><script type="math/tex; mode=display">L = \sum_{t=1}^{T} D_{KL}\left(q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t)\right)</script><p>每一个 KL 项衡量在第 $t$ 个时间步长上真实分布和模型估计分布之间的差异。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>为了简化训练过程，我们可以<strong>重参数化</strong>损失函数为一个去噪过程的预测任务。目标变为预测加入噪声的程度（噪声项）的均值和方差。</p><blockquote><p>这里重参数的推导很长，可以网上找一下。</p></blockquote><script type="math/tex; mode=display">L = \mathbb{E}_{q(x_0, \epsilon)} \left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]</script><p>其中 $\epsilon$ 是在前向过程加入的数据噪声，$\epsilon_\theta$ 是通过神经网络预测的噪声。所以神经网络的任务就是抽取一个 $t$（1~$T$ 之间），通过 $x_0$ 和加噪过程，计算得到 $x_t$，然后神经网络预测噪声，计算预测的噪声和实际噪声的分布差异。</p><h3 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h3><ol><li><strong>采样数据 $x_0$</strong> 从真实数据分布中。</li><li><strong>采样噪声 $\epsilon$</strong> 从标准正态分布中。</li><li><strong>计算 $x_t$</strong> 通过前向扩散过程，将噪声加入数据。</li><li><strong>计算预测的噪声 $\epsilon_\theta(x_t, t)$</strong> 使用神经网络。</li><li><strong>计算损失 $L$</strong> 并通过反向传播更新模型参数。</li></ol><h3 id="生成步骤"><a href="#生成步骤" class="headerlink" title="生成步骤"></a>生成步骤</h3><p>生成数据时，从标准正态分布中采样 $x_T$，然后逐步通过逆向生成过程去噪，生成数据 $x_0$。</p><p>在 DDPM 中，会将原始图像的像素值从 [0, 255] 范围归一化到 [-1, 1]，像素值属于离散化值。</p><h3 id="背后原理"><a href="#背后原理" class="headerlink" title="背后原理"></a>背后原理</h3><p>DDPM 通过一个称为”马尔科夫链”的过程，逐步将噪声转化为数据。其核心思想是分阶段进行去噪，每个阶段只去除一小部分噪声，使得每一步的去噪过程更为简单和稳定。</p><p>总的来说，DDPM 在生成任务中表现出色，特别是生成图像和其他复杂结构的数据类型。这是因为它通过多步生成过程有效地捕捉了数据的复杂结构和细节。</p><p>DDPM 的推导过程中，最重要的就是重参数技巧，这个技巧在很多生成模型中都有应用，比如 VAE、GAN 等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前所采用的扩散模型大都是来自于 2020 年的工作 DDPM。DDPM 对之前的扩散模型进行了简化，并通过变分推断（variational inference）来进行建模，这主要是因为扩散模型也是一个隐变量模型（latent variable model），相比 VAE 这样的隐变量模型，扩散模型的隐变量是和原始数据是同维度的，而且推理过程（即扩散过程）往往是固定的。&lt;/p&gt;
    
    </summary>
    
      <category term="AIGC" scheme="https://murphypei.github.io/categories/AIGC/"/>
    
    
      <category term="AIGC" scheme="https://murphypei.github.io/tags/AIGC/"/>
    
      <category term="DDPM" scheme="https://murphypei.github.io/tags/DDPM/"/>
    
      <category term="图像生成" scheme="https://murphypei.github.io/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>大模型 RLHF 训练中的 PPO 算法细节</title>
    <link href="https://murphypei.github.io//blog/2024/07/llm-rlhf-ppo.html"/>
    <id>https://murphypei.github.io//blog/2024/07/llm-rlhf-ppo.html</id>
    <published>2024-07-25T09:15:51.000Z</published>
    <updated>2025-07-21T11:41:47.156Z</updated>
    
    <content type="html"><![CDATA[<p>虽然了解大模型训练中的 RLHF 训练，但是都是有点不够深刻，特别是 PPO 算法的细节。</p><a id="more"></a><p>看到一篇好文章，转载并重新编辑，加入个人理解，以便日后查阅。有兴趣可以参考<a href="https://zhuanlan.zhihu.com/p/677607581" target="_blank" rel="noopener">原文</a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>强化学习属于机器学习的一个分支，区别于有监督学习。关键点在于：</p><ol><li>无监督，没有标签，通过试错和奖励来优化行为和策略。</li><li>与环境有交互。（抽象概念，不要深究）</li><li>没有明确的反馈（无标签），反馈是通过奖励信号传递的，可以是延迟的，需要考虑长期回报。</li></ol><p>强化学习的简化图：<br><img src="/images/posts/llm/ppo/rl.webp" alt></p><p>强化学习的两个实体：<strong>智能体（Agent）</strong>与<strong>环境（Environment）</strong>。强化学习中两个实体的交互：</p><ul><li><strong>状态空间 S</strong>：S 即为 State，指环境中所有可能状态的集合</li><li><strong>动作空间 A</strong>：A 即为 Action，指智能体所有可能动作的集合</li><li><strong>奖励 R</strong>：R 即为 Reward，指智能体在环境的某一状态下所获得的奖励。</li></ul><p>一个交互过程可以表示为：</p><ol><li>在 $t$ 时刻，智能体处于状态 $S_t$，在该状态下，得到的奖励为 $R_t$；</li><li>根据 $S_t$、$R_t$ 以及策略智能体选择动作 $A_t$；</li><li>执行动作 $A_t$ 后环境转移到状态 $S_{t+1}$，智能体获得奖励 $R_{t+1}$。</li></ol><p>智能体在这个过程中学习，它的最终目标是：<strong>找到一个策略，这个策略根据当前观测到的环境状态和奖励反馈，来选择最佳的动作</strong>。</p><h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>奖励 $R$ 是一个标量，但是在实际问题中，一个动作，既有即时奖励，也要考虑<strong>长期回报</strong>。为了解决这个问题，引入了<strong>价值函数</strong>的概念。</p><script type="math/tex; mode=display">V_t = R_t + \gamma V_{t+1}</script><p>其中：</p><ul><li>$V_t$：表示在 $t$ 时刻的状态 $S_t$ 下的价值（包含了即时和未来的奖励）。</li><li>$R_t$：$t$ 时刻的即时收益。</li><li>$\gamma$：是折扣因子，用于平衡当前奖励和未来奖励。</li></ul><p>这里最需要注意的是：$V_{t+1}$ 同样包含了现在和未来的奖励，但是对于 $V_{t}$ 来说，它就相当于未来潜在收益。</p><h2 id="NLP-和强化学习"><a href="#NLP-和强化学习" class="headerlink" title="NLP 和强化学习"></a>NLP 和强化学习</h2><p><img src="/images/posts/llm/ppo/nlp-rl.webp" alt></p><p>这里的 NLP 是特质生成模型。</p><p>生成模型的推理执行过程：给模型一个 prompt，让模型能生成符合人类喜好的 response。再回想一下 GPT 模型做推理的过程：每个时刻 $t$ 只产生一个 token，即 token 是一个一个蹦出来的，先有上一个 token，再有下一个 token。</p><p>结合上面的图，分解一下这个过程：</p><ol><li>智能体就是生成模型。</li><li>在 $t$ 时刻，有上下文 context（$S_t$），模型产出一个 token，对应 RL 中的动作，记为 $A_t$。动作空间就是词表。</li><li>在 $t$ 时刻，有了 $A_t$ 动作，<strong>即时</strong>收益为 $R_t$，总收益为 $V_t$（注意二者不一样）。对于生成模型，收益是什么？人类喜好。</li><li>状态变化，$S_{t+1}$ 变为 $S_t$ 和新生成的 token。</li><li><strong>忽略图中的下表，主要理解过程和对应的东西</strong>。</li></ol><p>$A_t$ 是产出一个新 token，$S_t$ 是词表空间，$R_t$ 和 $V_t$ 是什么？答案是通过模型产生的分数，这里不要在意命名，你叫评价模型，叫奖励模型都行，只不过是两个打分模型而已。</p><p>记住，到此已经有了<strong>3 个模型</strong>了啊，$A_t$ 模型表示智能体的动作，$R_t$ 和 $V_t$ 是两个打分模型，分别表示即时奖励和未来长期奖励。</p><p>还有一个重要的点：不是生成一个 token，也就是有一个动作，我们就要计算奖励、打分，可以等生成模型回答完毕（也就是 EOS token）再打分。</p><h2 id="RLHF-中的-4-个模型"><a href="#RLHF-中的-4-个模型" class="headerlink" title="RLHF 中的 4 个模型"></a>RLHF 中的 4 个模型</h2><p>OpenAI 的示意图：</p><p><img src="/images/posts/llm/ppo/ppo.png" alt></p><p>RLHF 中使用的模型示意图：</p><p><img src="/images/posts/llm/ppo/rlhf.webp" alt></p><p>现在大家都知道 PPO 有 4 个模型，上面我们说了 3 个，还有 1 个，这里将 4 个模型都列出来：</p><ul><li><strong>Actor Model</strong>：PPO 训练的模型，也是我们最终要用于应用的模型。</li><li><strong>Critic Model</strong>：即时收益 $V_t$，反映的是当前输出的潜在价值。</li><li><strong>Reward Model</strong>：整体收益的打分模型，也是上面的 $R_t$。</li><li><strong>Reference Model</strong>：这个模型是额外增加的，主要是在 RLHF 阶段给语言模型增加一些”约束”，防止语言模型训歪（朝不受控制的方向更新，效果可能越来越差）。这个看 Loss 就能明白了。</li></ul><h3 id="哪些模型需要更新参数？"><a href="#哪些模型需要更新参数？" class="headerlink" title="哪些模型需要更新参数？"></a>哪些模型需要更新参数？</h3><p>Actor Model 和 Critic Model。Actor 肯定是很好理解的，所以不多说了，Critic Model 为什么也要更新？主要是这里存在一个难点：怎么评估总体收益呢？我们自己随口一说评估总体收益，但是这个是很难的，因为没有真的标签（有监督）。所以我们需要通过一个模型来判断，而且更重要的是，这个判断模型的能力，<strong>要不断提升能力</strong>，才能做好这件事。</p><h3 id="Actor-Model"><a href="#Actor-Model" class="headerlink" title="Actor Model"></a>Actor Model</h3><p><img src="/images/posts/llm/ppo/actor.webp" alt></p><p>我们的最终目的是让 Actor 模型能产生符合人类喜好的 response。所以我们的策略是，先喂给 Actor 一条 prompt（这里假设 batch_size = 1，所以是 1 条 prompt），让它生成对应的 response。然后，我们再将”prompt + response”送入我们的”奖励-loss”计算体系中去算得最后的 loss，用于更新 actor。</p><h3 id="Reference-Model"><a href="#Reference-Model" class="headerlink" title="Reference Model"></a>Reference Model</h3><p>Reference Model 一般也用 SFT 阶段得到的 SFT 模型做初始化，在训练过程中，它的参数是冻结的。Ref 模型的主要作用是防止 Actor”训歪”，那么它具体是怎么做到这一点的呢？</p><p><img src="/images/posts/llm/ppo/ref.webp" alt></p><p>“防止模型训歪”换一个更详细的解释是：我们希望训练出来的 Actor 模型既能达到符合人类喜好的目的，又尽量让它和 SFT 模型不要差异太大。简言之，我们希望两个模型的输出分布尽量相似。那什么指标能用来衡量输出分布的相似度呢？我们自然而然想到了<strong>KL 散度</strong>。</p><blockquote><p>简单来说就是防止模型”高分低能”，过拟合到乱七八糟但是得分高的回答上。</p></blockquote><p>关于 KL 散度和 ref 模型的计算，这里不需要展开，网上资料特别多。</p><p>Reference Model 输入和 Actor Model 一致，输出是一个参考答案。</p><h3 id="Critic-Model"><a href="#Critic-Model" class="headerlink" title="Critic Model"></a>Critic Model</h3><p><img src="/images/posts/llm/ppo/critic.webp" alt></p><p>前面已经讲了这个模型的作用以及为什么要更新参数。简单讲一下这个模型怎么训练的：一般都是采用了 Reward 模型作为它的初始化，所以这里我们也按 Reward 模型的架构来简单画画它。你可以简单理解成，Reward/Critic 模型和 Actor 模型的架构是很相似的（毕竟输入都一样），同时，它在最后一层增加了一个 Value Head 层，该层是个简单的线形层，用于将原始输出结果映射成单一的 $V_t$ 值。</p><p><strong>特别要注意</strong>：</p><ul><li>价值函数是一个计算 Actor Model 在生成过程中每个 token 的价值（一个标量）。那么它的输入就是当前的问题 + Actor Model 当前的输出（并非完整的答案）</li><li>价值标量是怎么得到的呢？将最后一个 token（认为包含了整个序列 token 的注意力）的隐藏层（4096 维）输入到一个 1 维的 FC 层，就输出一个标量。</li></ul><h3 id="Reward-Model"><a href="#Reward-Model" class="headerlink" title="Reward Model"></a>Reward Model</h3><p><img src="/images/posts/llm/ppo/reward.webp" alt></p><p>计算整体奖励，也没啥好讲的，提前训练好的（RLHF 第二阶段做的事情），这里重点讲一下为啥 reward 模型不需要更新参数呢？</p><p>其实我觉得不要深入去纠结，我感觉 PPO 这么做的原因就是为了引入一个客观的、绝对的标准。这个模型最重要的区别在于，<strong>它只关心当前这个 response 的好坏</strong>。Critic 隐含了综合考虑所有 response 的好坏的含义（需要才需要更新参数）。</p><p><strong>特别要注意</strong>：</p><ul><li>奖励函数是一个计算 Actor Model 在生成的完整的答案的奖励，所以输入是当前的问题 + 完整的答案，得到一个标量奖励。</li><li>奖励的标量是怎么得到的呢？将输入的最后一个 token（认为包含了整个序列 token 的注意力）的隐藏层（4096 维）输入到一个 1 维的 FC 层，就输出一个标量。</li></ul><h2 id="RLHF-的-Loss-计算"><a href="#RLHF-的-Loss-计算" class="headerlink" title="RLHF 的 Loss 计算"></a>RLHF 的 Loss 计算</h2><p>我已经看过太多次了，所以不想重新写这个了，直接看原文或者网上搜一下就知道了。注意每一项对应 ref、critic、reward 模型，结合前面讲解的各个模型的作用，应该能很好地理解这个 Loss 的含义。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然了解大模型训练中的 RLHF 训练，但是都是有点不够深刻，特别是 PPO 算法的细节。&lt;/p&gt;
    
    </summary>
    
      <category term="LLM" scheme="https://murphypei.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://murphypei.github.io/tags/LLM/"/>
    
      <category term="RLHF" scheme="https://murphypei.github.io/tags/RLHF/"/>
    
      <category term="PPO" scheme="https://murphypei.github.io/tags/PPO/"/>
    
      <category term="大模型" scheme="https://murphypei.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>STL 旋转序列算法 rotate</title>
    <link href="https://murphypei.github.io//blog/2022/12/stl-rotate.html"/>
    <id>https://murphypei.github.io//blog/2022/12/stl-rotate.html</id>
    <published>2022-12-07T03:35:51.000Z</published>
    <updated>2025-06-25T02:00:04.078Z</updated>
    
    <content type="html"><![CDATA[<p>最近开发需要不管刷新缓冲区，发现了一个有用的 STL 算法。</p><a id="more"></a><p>先说明应用场景：我有一块缓冲区 vector，不断接收数据和消费数据（生产消费模型），接收数据就放在末尾，消费头部数据，消费完删除。之前用 realloc 和 memmove 来操作，改为 vector 之后如果每次搬移数据就很麻烦了，查了一下发现 <a href="https://en.cppreference.com/w/cpp/algorithm/rotate" target="_blank" rel="noopener">rotate</a> 配合 resize 可以搞定。</p><p>std::rotate() 的第一个参数是这个序列的开始迭代器；第二个参数是指向新的第一个元素的迭代器，<strong>它必定在序列之内</strong>。第三个参数是这个序列的结束迭代器。意思是将第二个参数的元素旋转到第一个参数的位置，旋转的序列是第一个参数到第三个参数的范围。</p><p>你可以想象第一个参数 ~ 第三个参数之间的元素序列组成一个圆盘，左转就是逆时针旋转，直到第二个参数转到第一个参数的位置，旋转结束。</p><p>可以参数<a href="http://c.biancheng.net/view/609.html" target="_blank" rel="noopener">图解</a></p><p>旋转完成后，头部就变到 vector 末尾了，用 resize 可以标记删除掉这些元素。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开发需要不管刷新缓冲区，发现了一个有用的 STL 算法。&lt;/p&gt;
    
    </summary>
    
      <category term="C/C++" scheme="https://murphypei.github.io/categories/C-C/"/>
    
    
      <category term="C++" scheme="https://murphypei.github.io/tags/C/"/>
    
      <category term="STL" scheme="https://murphypei.github.io/tags/STL/"/>
    
      <category term="stl" scheme="https://murphypei.github.io/tags/stl/"/>
    
      <category term="rotate" scheme="https://murphypei.github.io/tags/rotate/"/>
    
  </entry>
  
  <entry>
    <title>vscode C++ 开发之使用 clangd、C/C++、clang-format</title>
    <link href="https://murphypei.github.io//blog/2022/12/vscode-clang-format.html"/>
    <id>https://murphypei.github.io//blog/2022/12/vscode-clang-format.html</id>
    <published>2022-12-07T03:25:55.000Z</published>
    <updated>2025-06-25T02:00:04.082Z</updated>
    
    <content type="html"><![CDATA[<p>最近比较忙，废话少说，vscode 开发 C/C++ 需要很繁琐的配置，之前也说过 launch 和 tasks 的配置。这篇文章主要结合自身使用经历讲讲 C++ 相关插件。</p><a id="more"></a><p>vscode 最常用的几个 C++ 插件（不包含 cmake）就是微软的 C/C++、LLVM 的 clangd，以前我也使用 C/C++，但是智能补全和提示、include 路径都太差劲了，转投 clangd 了，确实好用。所以不废话，直接推荐使用 clangd，不过 C/C++ 也在用，为了二者不冲突，需要配置如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">"C_Cpp.autocomplete": "Disabled",</span><br><span class="line">"C_Cpp.clang_format_fallbackStyle": "Visual Studio",</span><br><span class="line">"C_Cpp.clang_format_sortIncludes": true,</span><br><span class="line">"C_Cpp.clang_format_style": "file",</span><br><span class="line">"C_Cpp.default.compilerPath": "/usr/bin/g++",</span><br><span class="line">"C_Cpp.default.configurationProvider": "ms-vscode.cmake-tools",</span><br><span class="line">"C_Cpp.default.cppStandard": "c++11",</span><br><span class="line">"C_Cpp.default.cStandard": "c99",</span><br><span class="line">"C_Cpp.default.intelliSenseMode": "gcc-x64",</span><br><span class="line">"C_Cpp.errorSquiggles": "Disabled",</span><br><span class="line">"C_Cpp.intelliSenseEngine": "Disabled",</span><br><span class="line">"clangd.arguments": [</span><br><span class="line">// 在后台自动分析文件（基于complie_commands)</span><br><span class="line">"--background-index",</span><br><span class="line">"--compile-commands-dir=$&#123;workspaceFolder&#125;/build",</span><br><span class="line">"-j=8",</span><br><span class="line">// 支持 .clangd 配置</span><br><span class="line">"--enable-config",</span><br><span class="line">"--clang-tidy",</span><br><span class="line">"--clang-tidy-checks=performance-*,bugprone-*",</span><br><span class="line">"--log=verbose",</span><br><span class="line">"--pretty",</span><br><span class="line">// 全局补全（会自动补充头文件）</span><br><span class="line">"--all-scopes-completion",</span><br><span class="line">// 更详细的补全内容</span><br><span class="line">"--completion-style=detailed",</span><br><span class="line">// 补充头文件的形式</span><br><span class="line">"--header-insertion=iwyu",</span><br><span class="line">// pch优化的位置</span><br><span class="line">"--pch-storage=memory",</span><br><span class="line">"--function-arg-placeholders",</span><br><span class="line">],</span><br></pre></td></tr></table></figure><p>clangd 的 include 可以通过如下配置：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">"clangd.fallbackFlags": [</span><br><span class="line">    "-std=c++11",</span><br><span class="line">    "-I/usr/include/c++/9",</span><br><span class="line">    "-I/usr/include/opencv4",</span><br><span class="line">    "-I$&#123;workspaceFolder&#125;/src/",</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>clangd 虽然很香，但是有个明显的缺点，就是它一定要使用自身的 clang-format 来格式化，而且无法配置使用 .clang-format 文件。为此，需要安装另一个插件 xaver clang-format。安装完成后配置格式化的程序：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"[cpp]": &#123;</span><br><span class="line">// "editor.defaultFormatter": "llvm-vs-code-extensions.vscode-clangd"</span><br><span class="line">"editor.defaultFormatter": "xaver.clang-format"</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>这个插件可以直接调用项目根目录下的 .clang-format 文件来格式化。</p><p>最后，有条件的推荐使用 clion 来开发和调试 C++。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近比较忙，废话少说，vscode 开发 C/C++ 需要很繁琐的配置，之前也说过 launch 和 tasks 的配置。这篇文章主要结合自身使用经历讲讲 C++ 相关插件。&lt;/p&gt;
    
    </summary>
    
      <category term="C/C++" scheme="https://murphypei.github.io/categories/C-C/"/>
    
    
      <category term="C++" scheme="https://murphypei.github.io/tags/C/"/>
    
      <category term="vscode" scheme="https://murphypei.github.io/tags/vscode/"/>
    
      <category term="clangd" scheme="https://murphypei.github.io/tags/clangd/"/>
    
      <category term="clang-format" scheme="https://murphypei.github.io/tags/clang-format/"/>
    
  </entry>
  
  <entry>
    <title>golang select 机制和超时</title>
    <link href="https://murphypei.github.io//blog/2022/06/go-select-timeout.html"/>
    <id>https://murphypei.github.io//blog/2022/06/go-select-timeout.html</id>
    <published>2022-06-25T06:24:59.000Z</published>
    <updated>2025-06-25T02:00:04.078Z</updated>
    
    <content type="html"><![CDATA[<p>golang 中的协程使用非常方便，但是协程什么时候结束是一个控制问题，可以用 select 配合使用。</p><a id="more"></a><p>首先声明，golang 使用并不熟悉，本文仅仅是记录使用过程中遇到的一些坑。</p><p>子协程和父协程的通信通常用 context 或者 chan。我遇到一个通常的使用场景，在子协程中尝试多次处理，父协程等待一段时间超时，我选择用 chan 实现。我以为 select 和 C++ 中 switch 类似，所以最开始代码类似如下：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">            <span class="comment">// process ctx done</span></span><br><span class="line">        <span class="keyword">case</span> &lt;-time.After(time.Second * <span class="number">3</span>):</span><br><span class="line">            <span class="comment">// process after</span></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="comment">// process code</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试发现无法实现 timeout，又仔细查看文档，才发现 golang 中 select 另有玄机。废话少说，直接总结要点：</p><ul><li>select 中的 case 必须是进行 chan 的手法操作，也就是只能在 case 中操作 chan，并且是<strong>非阻塞接收</strong>。</li><li>select 中的 case 是同时监听的，多个 case 同时操作，并未 switch 中一个个顺序判断。如果多个 case 满足要求，随机执行一个，如果一个没有则阻塞当前的协程（没有 default 情况下）。<strong>很类似 Linux 文件符操作的 select 语义</strong>。</li><li>上面说的阻塞是没有 default 的情况下，如果有 default，则执行 default，然后退出 select，也就是不会阻塞当前协程。</li></ul><p>回到上述代码，我这个 select 会一直不断的执行 default，<code>time.After</code> 生成的 chan 并不会被阻塞判断，所以根本无法完成我想要的效果。理解了之后重新修改代码：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">done := <span class="built_in">make</span>(char <span class="keyword">int</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(c <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="comment">// process code</span></span><br><span class="line">        <span class="keyword">if</span> &#123;</span><br><span class="line">            c &lt;- <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    c &lt;- <span class="number">0</span></span><br><span class="line">&#125;(done)</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">        <span class="comment">// process ctx done</span></span><br><span class="line">    <span class="keyword">case</span> &lt;-time.After(time.Second * <span class="number">3</span>):</span><br><span class="line">        <span class="comment">// process after</span></span><br><span class="line">    <span class="keyword">case</span> &lt;-done:</span><br><span class="line">        <span class="comment">// process code</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>开一个新的协程去不断尝试，在外的三个 case 有一个满足，则会执行。但是这里有一个问题非常需要注意：<strong>子协程什么时候退出？</strong>。</p><p>因为 gorountine 不能被强制 kill，所以在上述超时的情况下，select 语句执行 <code>case time.After</code> 之后退出，<code>done</code> 这个 chan 已经没有接受方了，因此既没有接受者，又没有缓冲区，结合 chan 的特性，则子协程会一直阻塞无法退出，所以本质上这个实现会导致子协程累积下去，也就是<strong>协程泄露</strong>，可能会使资源耗尽。</p><p>如何避免上述问题呢？一个很简单的想法就是提供缓冲区，<code>done := make(char int, 1)</code>，这样即使没有接收方，子协程也能完成发送，不会被阻塞。</p><p>还要一种办法，上面说了，select 操作 chan，并且可以指定 default，那是不是有思路了呢？</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> done &lt;- <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们尝试往 chan 中发送，如果发不出去，则就退出，也实现了目的。</p><p>最后总结一下，goroutine 泄露的防范条例：</p><ul><li>创建 goroutine 时就要想好该 goroutine 该如何结束。</li><li>使用 chan 时，要考虑到 chan 阻塞时协程可能的行为。</li><li>实现循环语句时注意循环的退出条件，避免死循环。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;golang 中的协程使用非常方便，但是协程什么时候结束是一个控制问题，可以用 select 配合使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Golang" scheme="https://murphypei.github.io/categories/Golang/"/>
    
    
      <category term="select" scheme="https://murphypei.github.io/tags/select/"/>
    
      <category term="golang" scheme="https://murphypei.github.io/tags/golang/"/>
    
      <category term="case" scheme="https://murphypei.github.io/tags/case/"/>
    
      <category term="timeout" scheme="https://murphypei.github.io/tags/timeout/"/>
    
      <category term="after" scheme="https://murphypei.github.io/tags/after/"/>
    
      <category term="chan" scheme="https://murphypei.github.io/tags/chan/"/>
    
  </entry>
  
  <entry>
    <title>C++ 链接一个不需要的库(--no-as-needed)</title>
    <link href="https://murphypei.github.io//blog/2022/04/link-noneed-lib.html"/>
    <id>https://murphypei.github.io//blog/2022/04/link-noneed-lib.html</id>
    <published>2022-04-18T09:16:49.000Z</published>
    <updated>2025-06-25T02:00:04.074Z</updated>
    
    <content type="html"><![CDATA[<p>使用 libtorch 的 C++ 动态链接库遇到了一个非常诡异的问题…</p><a id="more"></a><p>我使用 libtorch 的库编译了一个语音识别程序，使用 CPU 推理，能够完美运行，然后在 go 中对这个程序封装了一层 GRPC，也都 OK。</p><p>但是当我想用 GPU 推理的时候，我直接下载了 libtorch 的 <a href="https://download.pytorch.org/libtorch/cu113/libtorch-cxx11-abi-shared-with-deps-1.11.0%2Bcu113.zip" target="_blank" rel="noopener">GPU 库</a>，然后直接编译语音程序（需要修改 <code>torch::Device</code>），可以直接跑在 GPU 上了，很开心。</p><p>但是我用第二次编译出来的库放到 go 程序中，则出现了诡异的错误，运行加载模型的时候，<code>model-&gt;to_device</code>，而且 <code>device_count</code> 为 0，很明显，程序没找到 GPU。</p><p>利用 ldd 查看 go 编译出来的可执行文件，发现没有链接到 <code>torch_cuda_*</code> 这些库，怎么会这么奇怪呢？我明明把这些库放到编译的 flags 中了。为此我反复调整了链接的 flag，包括库的顺序，库的路径等等，但是都无济于事。</p><p>几经辗转，终于找到一个和我类似的错误了。<a href="https://github.com/pytorch/pytorch/issues/72396" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/72396</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">Could not run &apos;aten::empty_strided&apos; with arguments from the &apos;CUDA&apos; backend. This could be because the operator doesn&apos;t exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. &apos;aten::empty_strided&apos; is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].</span><br><span class="line"></span><br><span class="line">CPU: registered at aten\src\ATen\RegisterCPU.cpp:18433 [kernel]</span><br><span class="line">Meta: registered at aten\src\ATen\RegisterMeta.cpp:12703 [kernel]</span><br><span class="line">BackendSelect: registered at aten\src\ATen\RegisterBackendSelect.cpp:665 [kernel]</span><br><span class="line">Python: registered at ..\..\aten\src\ATen\core\PythonFallbackKernel.cpp:47 [backend fallback]</span><br><span class="line">Named: registered at ..\..\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]</span><br><span class="line">Conjugate: fallthrough registered at ..\..\aten\src\ATen\ConjugateFallback.cpp:22 [kernel]</span><br><span class="line">Negative: fallthrough registered at ..\..\aten\src\ATen\native\NegateFallback.cpp:22 [kernel]</span><br><span class="line">ADInplaceOrView: fallthrough registered at ..\..\aten\src\ATen\core\VariableFallbackKernel.cpp:64 [backend fallback]</span><br><span class="line">AutogradOther: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradCPU: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradCUDA: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradXLA: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradLazy: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradXPU: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradMLC: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradHPU: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradNestedTensor: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradPrivateUse1: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradPrivateUse2: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">AutogradPrivateUse3: registered at ..\..\torch\csrc\autograd\generated\VariableType_2.cpp:10483 [autograd kernel]</span><br><span class="line">Tracer: registered at ..\..\torch\csrc\autograd\generated\TraceType_2.cpp:11423 [kernel]</span><br><span class="line">UNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ..\..\aten\src\ATen\autocast_mode.cpp:466 [backend fallback]</span><br><span class="line">Autocast: fallthrough registered at ..\..\aten\src\ATen\autocast_mode.cpp:305 [backend fallback]</span><br><span class="line">Batched: registered at ..\..\aten\src\ATen\BatchingRegistrations.cpp:1016 [backend fallback]</span><br><span class="line">VmapMode: fallthrough registered at ..\..\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]</span><br><span class="line"></span><br><span class="line">Exception raised from reportError at ..\..\aten\src\ATen\core\dispatch\OperatorEntry.cpp:431 (most recent call first):</span><br><span class="line">00007FFEE7CAA29200007FFEE7CAA230 c10.dll!c10::Error::Error [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFEE7C843C500007FFEE7C84350 c10.dll!c10::NotImplementedError::NotImplementedError [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F015C7100007FFD5F015AA0 torch_cpu.dll!c10::impl::OperatorEntry::reportError [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F6C6AF000007FFD5F66DBB0 torch_cpu.dll!at::_ops::xlogy_Tensor::redispatch [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F8E73F100007FFD5F8CF610 torch_cpu.dll!at::_ops::zeros_out::redispatch [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F8E3F7400007FFD5F8CF610 torch_cpu.dll!at::_ops::zeros_out::redispatch [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F6EB6E800007FFD5F6EB520 torch_cpu.dll!at::_ops::empty_strided::call [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5EF259CB00007FFD5EF258D0 torch_cpu.dll!at::empty_strided [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F2C24D100007FFD5F2C2130 torch_cpu.dll!at::native::_to_copy [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5FA7C3D600007FFD5FA7BF10 torch_cpu.dll!at::compositeexplicitautograd::xlogy_ [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5FA5A8FB00007FFD5FA3F310 torch_cpu.dll!at::compositeexplicitautograd::bitwise_xor_outf [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F4EB5AD00007FFD5F45B290 torch_cpu.dll!at::TensorMaker::make_tensor [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F8DED7700007FFD5F8CF610 torch_cpu.dll!at::_ops::zeros_out::redispatch [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F8E36EB00007FFD5F8CF610 torch_cpu.dll!at::_ops::zeros_out::redispatch [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F4EB5AD00007FFD5F45B290 torch_cpu.dll!at::TensorMaker::make_tensor [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F56326800007FFD5F563190 torch_cpu.dll!at::_ops::_to_copy::redispatch [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD60A27F0000007FFD60A27A30 torch_cpu.dll!at::redispatch::_thnn_fused_lstm_cell_backward [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD60A4031D00007FFD60A34930 torch_cpu.dll!torch::jit::Node::c_ [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F50C12B00007FFD5F50BF70 torch_cpu.dll!at::_ops::_to_copy::call [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F2C2E7900007FFD5F2C2BD0 torch_cpu.dll!at::native::to_dense_backward [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F2C2B0C00007FFD5F2C29E0 torch_cpu.dll!at::native::to [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5FB6A66800007FFD5FB63F10 torch_cpu.dll!at::compositeimplicitautograd::where [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5FB4DB5D00007FFD5FB1BE50 torch_cpu.dll!at::compositeimplicitautograd::broadcast_to [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5F7E6F4600007FFD5F7E6D70 torch_cpu.dll!at::_ops::to_dtype_layout::call [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5EF4AA8800007FFD5EF4A970 torch_cpu.dll!at::Tensor::to [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFD5EF9EAE900007FFD5EF9E9F0 torch_cpu.dll!at::tensor [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FF7714295A200007FF7714294B0 SplinterlandsSimulator.exe!main [C:\Users\xargo\source\repos\SplinterlandsSimulator\SplinterlandsSimulator\SplinterlandsSimulator.cpp @ 390]</span><br><span class="line">00007FF77144164C00007FF771441540 SplinterlandsSimulator.exe!__scrt_common_main_seh [d:\a01\_work\20\s\src\vctools\crt\vcstartup\src\startup\exe_common.inl @ 288]</span><br><span class="line">00007FFF47C554E000007FFF47C554D0 KERNEL32.DLL!BaseThreadInitThunk [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br><span class="line">00007FFF48DA485B00007FFF48DA4830 ntdll.dll!RtlUserThreadStart [&lt;unknown file&gt; @ &lt;unknown line number&gt;]</span><br></pre></td></tr></table></figure><p>上述报错跟我的很像，而且从下面的回复来看，也是没能链接到 cuda 相应的库。下面的回复给我了启发：<strong>如果我的 go 程序没用到 libtorch 的 cuda 接口，是不是不会主动链接到 libtorch 相应的 cuda 的库</strong>？</p><p>前面说了，ldd 查看的确实没有，那怎么让编译器强制链接到 libtorch 的 cuda 相应的库呢？显然是的，编译器默认使用了 <code>--as-needed</code> 编译参数，这也是合理的，我们没必要链接所有的动态库，动态库本来就是按需链接，但是在我们的这个使用场景中，会遇到这种特殊情况，使用 <code>--no-as-needed</code> 强制链接到 libtorch cuda 的相应库，结果就没有问题了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--as-needed</span><br><span class="line">--no-as-needed</span><br><span class="line">This option affects ELF DT_NEEDED tags for dynamic libraries mentioned on the command line after the --as-needed option. Normally the linker will add a DT_NEEDED tag for each dynamic library mentioned on the command line, regardless of whether the library is actually needed or not. --as-needed causes a DT_NEEDED tag to only be emitted for a library that satisfies an undefined symbol reference from a regular object file or, if the library is not found in the DT_NEEDED lists of other libraries linked up to that point, an undefined symbol reference from another dynamic library. --no-as-needed restores the default behaviour.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 libtorch 的 C++ 动态链接库遇到了一个非常诡异的问题…&lt;/p&gt;
    
    </summary>
    
      <category term="C/C++" scheme="https://murphypei.github.io/categories/C-C/"/>
    
    
      <category term="C++" scheme="https://murphypei.github.io/tags/C/"/>
    
      <category term="link" scheme="https://murphypei.github.io/tags/link/"/>
    
      <category term="no-as-needed" scheme="https://murphypei.github.io/tags/no-as-needed/"/>
    
      <category term="undef" scheme="https://murphypei.github.io/tags/undef/"/>
    
  </entry>
  
  <entry>
    <title>shared_ptr 和 unique_ptr 深入探秘</title>
    <link href="https://murphypei.github.io//blog/2022/03/shared-unique-ptr.html"/>
    <id>https://murphypei.github.io//blog/2022/03/shared-unique-ptr.html</id>
    <published>2022-03-24T02:52:47.000Z</published>
    <updated>2025-06-25T02:00:04.070Z</updated>
    
    <content type="html"><![CDATA[<p>C++ 中 <code>shared_ptr</code> 和 <code>unique_ptr</code> 是 C++11 之后被广泛使用的两个智能指针，但是其实他们在使用上还是有一些“秘密”的，我根据平时遇到的两个问题，总结记录一些知识。</p><a id="more"></a><h3 id="为什么-unique-ptr-需要明确知道类型的析构函数"><a href="#为什么-unique-ptr-需要明确知道类型的析构函数" class="headerlink" title="为什么 unique_ptr 需要明确知道类型的析构函数"></a>为什么 unique_ptr 需要明确知道类型的析构函数</h3><p>这个问题是我写 <code>unique_ptr</code> 调试接口的时候才注意到的，之前确实不知道。为什么会这样呢？首先我们必须要知道 <code>unique_ptr</code> 到底封装了什么？通常 <code>unique_ptr</code> 就是简单的对裸指针封装，并且禁用拷贝和赋值：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">T</span>,</span></span><br><span class="line"><span class="class">    <span class="title">class</span> <span class="title">Deleter</span> = <span class="title">std</span>:</span>:default_delete&lt;T&gt;</span><br><span class="line">&gt; <span class="class"><span class="keyword">class</span> <span class="title">unique_ptr</span>;</span></span><br></pre></td></tr></table></figure><p>可以看到，<code>Deleter</code> 的类型是 <code>unique_ptr</code> 类型的一部分。在 <code>unique_ptr</code> 内部会保存类型为 <code>T*</code> 和 <code>Deleter</code> 的成员 ，分别表示保存的裸指针和删除器。假设内部是这么实现的 (一般会运用空基类优化把 <code>Deleter</code> 的空间优化掉，<code>libstdc++</code> 里把他们放进了一个 tuple。这里是简化了)：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    T* p;</span><br><span class="line">    Deleter del;</span><br></pre></td></tr></table></figure><p>然后析构的时候就会这样：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~<span class="built_in">unique_ptr</span>()</span><br><span class="line">&#123;</span><br><span class="line">    del(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当 <code>Deleter</code> 是默认的 <code>std::default_delete</code> 时，<code>del(p)</code> 就会 <code>delete p</code>，<code>delete</code> 会调用析构函数。而 <code>delete</code> 一个不完整类型的指针是 ub(undefined behavior)。在典型的实现中都会在 <code>delete</code> 前通过 <code>static_assert(sizeof(T) &gt; 0)</code> 做检查。 <code>sizeof</code> 对 incomplete type 求值会直接编译出错。</p><blockquote><p>incomplete type 是指当定义一个变量的时候，不知道应该分配多少内存。C++ 声明和定义最大的区别就是是否发生内存分配，当发生内存分配的时候，必须知道要分配多少内存，通常一个未定义的 struct，未指定长度的数组类型，都会引发 incomplete type 的问题。参考：<a href="https://docs.microsoft.com/en-us/cpp/c-language/incomplete-types?view=msvc-170" target="_blank" rel="noopener">https://docs.microsoft.com/en-us/cpp/c-language/incomplete-types?view=msvc-170</a></p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span>;</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(student) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>上述代码执行会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prog.cc:17:18: error: invalid application of &apos;sizeof&apos; to an incomplete type &apos;student&apos;</span><br><span class="line">    std::cout &lt;&lt; sizeof(student) &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><p>只声明了结构体 <code>student</code>，但是并没有定义，所以是一个 incomplete type，所以 <code>sizeof</code> 无法执行。</p><p>回到 <code>unique_ptr</code>，现在我们知道 <code>unique_ptr</code> 的报错链路是 <code>unique_ptr</code>-&gt;<code>delete</code>-&gt;<code>sizoef</code>，也就是 <code>sizeof</code> 才是罪魁祸首。所以当 <code>Deleter</code> 非默认时，就不一定需要知道类型的析构函数。比如下面这样：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A is incomplete type</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>;</span></span><br><span class="line"><span class="keyword">auto</span> Del = [] (A*) &#123; &#125;;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;A, <span class="keyword">decltype</span>(Del)&gt; ptr;</span><br></pre></td></tr></table></figure><p>因此可以对这个问题做定性：<strong>并不是 <code>unique_ptr</code> 需要知道析构函数，而是 <code>unique_ptr</code> 的默认删除器 <code>Deleter</code> 需要明确知道类型的析构函数</strong>。</p><p>继续深挖一下，这个问题会出现在 <code>shared_ptr</code> 吗？答案是<strong>不会</strong>。这又引入了另一个问题，shared_ptr 和 unique_ptr 的封装有什么不同？</p><h3 id="shared-ptr-的封装"><a href="#shared-ptr-的封装" class="headerlink" title="shared_ptr 的封装"></a>shared_ptr 的封装</h3><p>按理说 <code>shared_ptr.reset</code> 的时候需要 <code>delete</code> ptr 就需要 ptr 的类型（错了请指正），而 <code>shared_ptr</code> 的 template type 可以是 incomplete type（错误请指正）。cppreference 是这么描述的：</p><blockquote><p><code>std::shared_ptr</code> may be used with an incomplete typeT. However, the constructor from a raw pointer (template<class y> shared_ptr(Y<em>)) and the template<class y>void reset(Y</class></em>) member function may only be called with a pointer to a complete type (note that std::unique_ptr may be constructed from a raw pointer to an incomplete type).</class></p></blockquote><p><code>reset</code> 的时候需要类型完整。默认构造的时候允许是不完整类型。为什么会这样呢？<code>shared_ptr</code> 怎么处理 <code>Deleter</code> 呢？(还记得吧， Deleter 就是智能指针析构时候的删除操作)</p><p>在常见编译器的实现里，<code>shared_ptr</code> 把 <code>Deleter</code>（包括默认情况下的 operator delete）放进一个叫做 <strong>control block</strong> 的结构里，相当于做了一次 type erasure，把 <code>Deleter</code> 的类型从 <code>shared_ptr</code> 类型本身里面擦下去。<code>Deleter</code> 的类型在 control block 的具体类型上，<code>shared_ptr</code> 本身只<strong>持有一个 <code>control block</code> 基类的指针</strong>，通过虚函数来调用 <code>Deleter</code>。而因为 <code>shared_ptr</code> 构造的时候要求必须是 complete type，control block已经知道怎么析构了，<code>shared_ptr</code> 析构的时候就调用个虚函数，具体事情它不管的。</p><p>这下我们明白了，<code>unique_ptr</code> 的封装太简单了，没有 control block，<code>Deleter</code>（包括默认的std::default_delete）直接做在 <code>unique_ptr</code> 一起了，这就导致 <code>unique_ptr</code> 的析构函数需要亲手析构被管理的类型，因此析构函数必须看到 complete type。然而反过来，因为<strong>构建的时候只需要保存下指针，所以 <code>unique_ptr</code> 构造的时候不需要看到 complete type</strong>。这俩正好是反的。C++ 标准并没有规定这些实现细节，但是规定函数签名和特性的时候，是考虑着比较合理的实现方式来写标准的，到最后标准落下来之后也差不多只能这么实现了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><code>unique_ptr</code> 只保存了类型指针 ptr 和这个指针的析构方法，调用 delete ptr，就需要ptr的完整类型，为了防止这个问题出现，直接通过 assert sizeof 排除掉了这种风险。<strong><code>unique_ptr</code> 相当于在编译时绑定了删除器</strong>。</li><li><code>shared_ptr</code> 保存的是一个控制块的指针。控制块包含的就是一个引用计数和一个原来对象的裸指针。控制块中初始化的指针是 <code>nullptr</code>，在运行时为其赋值，也可以通过 <code>reset</code> 修改。类似于虚函数，<strong><code>shared_ptr</code> 相当于在运行时绑定了删除器</strong>。</li></ul><p>虽然只是一个小小的知识点，但是也帮助我深入理解了 <code>shared_ptr</code> 和 <code>unique_ptr</code> 在设计上的区别，对于不同使用场景下选择不同智能指针的体会也更加深刻。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C++ 中 &lt;code&gt;shared_ptr&lt;/code&gt; 和 &lt;code&gt;unique_ptr&lt;/code&gt; 是 C++11 之后被广泛使用的两个智能指针，但是其实他们在使用上还是有一些“秘密”的，我根据平时遇到的两个问题，总结记录一些知识。&lt;/p&gt;
    
    </summary>
    
      <category term="C/C++" scheme="https://murphypei.github.io/categories/C-C/"/>
    
    
      <category term="delete" scheme="https://murphypei.github.io/tags/delete/"/>
    
      <category term="c++" scheme="https://murphypei.github.io/tags/c/"/>
    
      <category term="shared_ptr" scheme="https://murphypei.github.io/tags/shared-ptr/"/>
    
      <category term="unique_ptr" scheme="https://murphypei.github.io/tags/unique-ptr/"/>
    
      <category term="template" scheme="https://murphypei.github.io/tags/template/"/>
    
  </entry>
  
  <entry>
    <title>使用 pyenv 搭建任意 python 环境</title>
    <link href="https://murphypei.github.io//blog/2022/01/pyenv-virtualenv.html"/>
    <id>https://murphypei.github.io//blog/2022/01/pyenv-virtualenv.html</id>
    <published>2022-01-13T02:47:20.000Z</published>
    <updated>2025-06-25T02:00:04.070Z</updated>
    
    <content type="html"><![CDATA[<p>开发和部署的过程中，常常遇到 python 版本和环境导致的冲突不兼容问题，pyenv 能够完美解决。</p><a id="more"></a><p>virtualenv 可以搭建虚拟且独立的 python 环境，可以使每个项目环境与其他项目独立开来，保持环境的干净，解决包冲突问题。但是这个依赖于已安装的 python 版本，相当于<strong>同一版本的不同环境</strong>。</p><p>pyenv 可以帮助你在一台开发机上建立多个版本的 python 环境，并提供方便的切换方法，可以搭配 virtualenv，完美解决 python 环境冲突，自由搭建任意版本的 python 环境。</p><h3 id="pyenv-安装"><a href="#pyenv-安装" class="headerlink" title="pyenv 安装"></a>pyenv 安装</h3><p><strong>安装 pyenv 之前建议卸载本机的 virtualenv 和 virtualenvwrapper 等相关虚拟环境</strong>，因为我从没用过 conda， 所以不清楚 conda 是否需要卸载。</p><ul><li><p>下载最新 pyenv </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/yyuu/pyenv.git ~/.pyenv</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 'export PYENV_ROOT="$HOME/.pyenv"' &gt;&gt; ~/.bashrc</span><br><span class="line">echo 'export PATH="$PYENV_ROOT/bin:$PATH"' &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><blockquote><p>用 zsh 的改为 ~/.zshrc，下同</p></blockquote><ul><li>添加 pyenv 初始化到你的 shell</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 'eval "$(pyenv init -)"' &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><ul><li>重新启动你的 shell 使更改生效</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exec $SHELL</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="安装某个版本的-python"><a href="#安装某个版本的-python" class="headerlink" title="安装某个版本的 python"></a>安装某个版本的 python</h3><p>首先我们可以查看一下有哪些版本的 python 可以安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv install --list</span><br></pre></td></tr></table></figure><p>一般情况下，几乎所有的 python 版本都可以安装，这也是 pyenv 强大之处。</p><ul><li>安装指定版本：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv install -v 3.9.9</span><br></pre></td></tr></table></figure><ul><li>安装完成后可以查看安装情况：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv versions</span><br></pre></td></tr></table></figure><p>一般输出如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* system (set by ~/.pyenv/version)</span><br><span class="line">3.9.9</span><br></pre></td></tr></table></figure><p>system 代表当前系统的 python 版本, 3.9.9 是我们用pyenv安装的, *表示当前的 python 版本， 可以看到，我们还在使用的是默认的 system 自带的 python 版本。</p><ul><li>切换 python 版本</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pyenv global 3.9.9</span><br><span class="line"><span class="meta">#</span><span class="bash"> pyenv <span class="built_in">local</span> 3.9.9</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pyenv shell 3.9.9</span></span><br></pre></td></tr></table></figure><p>上面三条命令都可以切换 python 版本，区别简单解释如下：</p><ul><li><code>pyenv global</code> 读写 <code>~/.python-version</code> 文件，基本来说你在当前 shell 和今后打开的 shell 中，默认都是用这个版本的 python。</li><li><code>pyenv local</code> 读写<strong>当前目录</strong>的 <code>.python-version</code> 文件，相当于覆盖了 <code>~/.python-version</code> 的版本。</li><li><code>pyenv shell</code> 指定当前 shell 使用的 python 版本，相当于覆盖了前面两个。</li></ul><p>此外设置 <code>PYENV_VERSION</code> 变量也可以修改 python 版本，看上去很杂很乱，但是多用几次就明白了。详细命令文档看这里：<a href="https://github.com/pyenv/pyenv/blob/master/COMMANDS.md" target="_blank" rel="noopener">pyenv commands</a></p><ul><li>卸载 python 版本</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv uninstall 3.9.9</span><br></pre></td></tr></table></figure><h3 id="pyenv-中使用-virtualenv"><a href="#pyenv-中使用-virtualenv" class="headerlink" title="pyenv 中使用 virtualenv"></a>pyenv 中使用 virtualenv</h3><p>pyenv virtualenv 是 pyenv 的插件，为 UNIX 系统提供 pyenv virtualenv 命令。</p><ul><li>安装 pyenv-virtualenv</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv</span><br><span class="line">echo 'eval "$(pyenv virtualenv-init -)"' &gt;&gt; ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><ul><li>创建虚拟环境</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv virtualenv 3.9.9 env399</span><br></pre></td></tr></table></figure><blockquote><p>创建虚拟环境的 python 版本需要提前装好</p></blockquote><ul><li>激活环境</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv activate env399</span><br></pre></td></tr></table></figure><p>切换后查看一下 python 版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  system</span><br><span class="line">  3.9.9</span><br><span class="line">  3.9.9/envs/env399</span><br><span class="line">* env399 (set by PYENV_VERSION environment variable)</span><br></pre></td></tr></table></figure><ul><li>退出虚拟环境</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv deactivate</span><br></pre></td></tr></table></figure><ul><li>删除虚拟环境</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf ~/.pyenv/versions/env399</span><br></pre></td></tr></table></figure><h3 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h3><ul><li>安装依赖</li></ul><p>自己谷歌查依赖的安装，我测试没遇到过。</p><ul><li>activate 激活不生效</li></ul><p>简单来说就是激活后 <code>pyenv versions</code> 显示生效了，<code>python version</code> 还是系统版本，暂时没找到具体原因，手动指定激活可以解决 <code>source ~/.pyenv/version/env399/bin/activate</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开发和部署的过程中，常常遇到 python 版本和环境导致的冲突不兼容问题，pyenv 能够完美解决。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://murphypei.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="https://murphypei.github.io/tags/python/"/>
    
      <category term="virtualenv" scheme="https://murphypei.github.io/tags/virtualenv/"/>
    
      <category term="pyenv" scheme="https://murphypei.github.io/tags/pyenv/"/>
    
      <category term="activate" scheme="https://murphypei.github.io/tags/activate/"/>
    
  </entry>
  
  <entry>
    <title>SSH 穿越多个跳板机的连接方法</title>
    <link href="https://murphypei.github.io//blog/2021/12/ssh-proxyjump.html"/>
    <id>https://murphypei.github.io//blog/2021/12/ssh-proxyjump.html</id>
    <published>2021-12-27T02:40:42.000Z</published>
    <updated>2025-06-25T02:00:04.066Z</updated>
    
    <content type="html"><![CDATA[<p>鉴于安全原因，工作需要使用跳板机登录；鉴于服务器环境老旧，我需要在服务器上使用 docker 来搞个开发环境，所以需要有一种方法穿越层层阻隔，让我的 vscode 直接连过去。</p><a id="more"></a><h2 id="SSH-公钥和私钥"><a href="#SSH-公钥和私钥" class="headerlink" title="SSH 公钥和私钥"></a>SSH 公钥和私钥</h2><ul><li>首先搞清楚一些基本关系，一般使用密钥登录，<code>ssh-keygen -t rsa</code> 运行此命令产生公钥私钥（id_rsa 和 id_rsa.pub），一路回车可以不设置保护密码，假设要登录的机器是 server，登录的终端是 client，那么将公钥 id_rsa.pub 的内容记录在 server 的 authorized_keys 中，然后 client 使用私钥 id_rsa 登录。</li><li>每一个被登录的机器都开启的 ssh 服务，并配置了 ssh 密钥登录功能。对于我的需求来说，公司的跳板机和服务器一定是已经配置的，否则无法登录服务器，因此我还需要在 docker 中配置 ssh 密钥登录服务。</li><li>client 设置登录的层层专跳（这是重点）</li></ul><blockquote><p>ssh 相关的文件如果没有特殊说明，都是在 <code>~/.ssh</code> 文件夹中，ssh 服务的配置文件在 <code>/etc/ssh/sshd_config</code> 中。</p></blockquote><h2 id="openssh-的-ProxyJump"><a href="#openssh-的-ProxyJump" class="headerlink" title="openssh 的 ProxyJump"></a>openssh 的 ProxyJump</h2><p>在 openssh7.5 之后（ubuntu18.04），支持 ProxyJump 语句，非常方便。windows 不支持。</p><p>假设我们登录路径是这样的：</p><p>client-&gt;jump_server-&gt;server-&gt;dev_docker</p><p>那么 client 的 <code>~/.ssh/config</code> 文件应该如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Host jump</span><br><span class="line">    HostName &lt;jump_server ip&gt;</span><br><span class="line">    Port &lt;jump_server port&gt;</span><br><span class="line">    User &lt;jump_server username&gt;</span><br><span class="line">    IdentityFile &lt;jump_server id_rsa&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Host server</span><br><span class="line">    HostName &lt;server ip&gt;</span><br><span class="line">    Port &lt;server port&gt;</span><br><span class="line">    User &lt;server username&gt;</span><br><span class="line">    IdentityFile &lt;server id_rsa&gt;</span><br><span class="line">    ProxyJump jump</span><br><span class="line"></span><br><span class="line">Host dev_docker</span><br><span class="line">    HostName &lt;dev_docker ip&gt;</span><br><span class="line">    Port &lt;dev_docker port&gt;</span><br><span class="line">    User &lt;dev_docker username&gt;</span><br><span class="line">    IdentityFile &lt;dev_docker id_rsa&gt;</span><br><span class="line">    ProxyJump server</span><br></pre></td></tr></table></figure><p>然后在 client 中，直接使用 <code>ssh dev_docker</code> 命令，ssh 就会一步步登录过去。使用 <code>-v</code> 可以看到每一步的登录过程。</p><p>vscode 会自动读取 config 文件，就可以直接打开 docker 中的文件夹了。真的很方便。</p><p>还有两个比较实用的配置，同样是配置在客户端：</p><ul><li><code>ServerAliveInterval 60</code>：每隔 60s 服务器发送一个包看客户端是否有响应。</li><li><code>ServerAliveCountMax 600</code>：服务器发出请求后客户端没有响应的次数达到一定值，就自动断开，正常情况下，客户端不会不响应。</li></ul><p>这两个配置组合就可以保持 ssh 的长连接了，不用一直手动连接。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;鉴于安全原因，工作需要使用跳板机登录；鉴于服务器环境老旧，我需要在服务器上使用 docker 来搞个开发环境，所以需要有一种方法穿越层层阻隔，让我的 vscode 直接连过去。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://murphypei.github.io/categories/Linux/"/>
    
    
      <category term="linux" scheme="https://murphypei.github.io/tags/linux/"/>
    
      <category term="ssh" scheme="https://murphypei.github.io/tags/ssh/"/>
    
      <category term="proxy" scheme="https://murphypei.github.io/tags/proxy/"/>
    
      <category term="proxyjump" scheme="https://murphypei.github.io/tags/proxyjump/"/>
    
      <category term="jump" scheme="https://murphypei.github.io/tags/jump/"/>
    
  </entry>
  
</feed>
